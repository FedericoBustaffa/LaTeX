\section{Risoluzione di sistemi lineari}
Vediamo ora alcuni metodi per la risoluzione di sistemi lineari e andiamo a studiare il loro condizionamento
in macchina. Il problema che vogliamo provare a risolvere ha in input la matrice $A \in \R^{n \times n}$ e il
vettore $b \in \R^n$ dai quali vogliamo ricavare il vettore $x \in \R^n$ tale che $A x = b$.

\subsection{Matrici diagonali}
La prima cosa che vogliamo osservare è che per particolari classi della matrice $A$ il sistema è facilmente
risolubile. Prendiamo ad esempio la classe delle matrici \textbf{diagonali} ossia matrici con tutti gli elementi
fuori dalla diagonale principale nulli (per $i \neq j$ vale $a_{i,j} = 0$). Per esempio sono matrici diagonali
\[
	\begin{bmatrix}
		1 & 0 & 0 \\
		0 & 3 & 0 \\
		0 & 0 & 2
	\end{bmatrix} \quad
	\begin{bmatrix}
		2 & 0 & 0 \\
		0 & 5 & 0 \\
		0 & 0 & 0
	\end{bmatrix} \quad
	\begin{bmatrix}
		0 & 0 & 0 \\
		0 & 0 & 0 \\
		0 & 0 & 0
	\end{bmatrix}
\]
Se proviamo a calcolare il determinante della matrice $A$ possiamo vedere che questo equivale al prodotto degli
elementi sulla diagonale principale, così come possiamo dimostrare che gli autovalori sono esattamente gli
elementi sulla diagonale principale.

Segue dalle considerazioni appena fatte che una matrice diagonale è invertibile se e solo se ha tutti gli elementi
sulla diagonale principale diversi da 0. Per risolvere un sistema diagonale possiamo scrivere ogni equazione del
sistema nella forma
\[ a_{i,i} x_i = b_i \]
ottenendo facilmente che
\[ x_i = \frac{b_i}{a_{i,i}} \]
che è ben definito se tutti gli $a_{i,i}$ sono diversi da 0. Un semplice algoritmo che risolve un sistema diagonale
$n \times n$ è il seguente:
\begin{lstlisting}[language=pseudo, style=pseudo-style]
for i = 1 to n
	x[i] = b[i] / a[i][i]
\end{lstlisting}
Dunque il costo per risoluzione di un sistema diagonale è di $O(n)$ operazioni.

\subsection{Matrici triangolari}
Prendiamo ora in esame il caso delle matrici triangolari.

\begin{definition}
	Una matrice si dice \textbf{triangolare} se ha tutti elementi nulli sopra la diagonale principale e, in
	tal caso, si dirà \textbf{inferiore} (per $i < j$ vale $a_{i,j} = 0$). Se invece ha tutti elementi nulli
	sotto la diagonale principale si dirà \textbf{superiore} (per $i > j$ vale $a_{i,j} = 0$).
\end{definition}

Non facciamo nessuna assunzione sugli altri elementi: la matrice nulla è sia triangolare superiore che inferiore.
Esempi di matrici triangolari inferiori e superiori sono
\[
	\begin{bmatrix}
		1 & 0 & 0 \\
		2 & 4 & 0 \\
		3 & 1 & 9
	\end{bmatrix} \quad
	\begin{bmatrix}
		1 & 3 & 5 \\
		0 & 2 & 7 \\
		0 & 0 & 4
	\end{bmatrix}
\]
Il determinante di una matrice triangolare, se calcolato ad esempio con lo sviluppo di Laplace secondo la prima
colonna, risulta ancora una volta equivalente al prodotto degli elementi sulla diagonale principale.

Il discorso fatto per gli autovalori di matrici diagonali vale anche qui, dato che la matrice $A - \lambda I$ è
ancora una matrice triangolare, abbiamo anche in questo caso che gli autovalori di una matrice triangolare sono
esattamente gli elementi sulla diagonale principale.

Prendiamo un generico sistema di equazioni $A x = b$ con $A$ triangolare superiore. Otterremo qualcosa di questo
tipo
\[
	\begin{cases}
		a_{1,1} x_1 + a_{1,2} x_2 + \dots + a_{1,n} x_n = b_1 \\
		a_{2,2} x_2 + \dots + a_{2,n} x_n = b_2               \\
		\dots                                                 \\
		a_{n,n} x_n = b_n
	\end{cases}
\]
Per risolvere il sistema si fa ricorso al metodo di \textbf{sostituzione all'indietro}: si ricava $x_n$
dall'ultima equazione, lo si sostituisce nella penultima trovando $x_{n-1}$ e così via fino a ricavare tutti gli
$x_i$.

Supponiamo di aver già ricavato $x_{k+1}, \dots, x_n$ e di voler determinare $x_k$. Dalla $k$-esima equazione si
ottiene
\[
	a_{k,k} x_k = \sum_{j=k+1}^n a_{k,j} x_j = b_k \quad \to \quad
	x_k = \frac{b_k - \displaystyle\sum_{j=k+1}^n a_{k,j} x_j}{a_{k,k}}
\]
In MatLab la \emph{sostituzione all'indietro} sarebbe così implementata:
\begin{lstlisting}[language=matlab]
function[x] = solve_tri(a, b)
	n = length(b);
	x = zeros(n, 1);
	x(n) = b(n) / a(n, n);
	for k = n-1 : -1 : 1
		s = 0;
		for j = k+1 : n
			s = s+ a(k, j) * x(j);
		end
		x(k) = (b(k) - s) / a(k, k);
	end
end
\end{lstlisting}
Il costo computazionale di questo algoritmo è di $O(n^2)$ moltiplicazioni:
\[ \sum_{k=1}^n k = \frac{n (n + 1)}{2} \]
Se $A \in \R^{n \times n}$ si può pensare, per la risoluzione del sistema lineare $A x = b$, di \emph{ridurre}
progressivamente $A$ in forma triangolare mediante una sequenza di trasformazioni del tipo
\begin{align*}
	A_0 = & A, \quad A_k \to A_{k+1}, \quad 0 \leq k \leq n - 2; \\
	b_0 = & b, \quad b_k \to b_{k+1}, \quad 0 \leq k \leq n - 2
\end{align*}

\subsection{Fattorizzazione LU}
Introduciamo quindi il concetto di \textbf{fattorizzazione}, ossia scrivere una matrice come prodotto di più
matrici.

Vogliamo quindi rappresentare la matrice $A \in \R^{n \times n}$ come prodotto di due matrici $L$ ed $U$, la
prima triangolare inferiore con elementi uguali a 1 sulla diagonale pricipale e la seconda triangolare superiore.
\[
	A = L \cdot U = \begin{bmatrix}
		1       & 0      & \dots     & 0      \\
		l_{2,1} & \ddots & \ddots    & \vdots \\
		\vdots  & \ddots & \ddots    & 0      \\
		l_{n,1} & \dots  & l_{n,n-1} & 1
	\end{bmatrix} \cdot
	\begin{bmatrix}
		u_{1,1} & \dots  & \dots  & u_{1,n} \\
		0       & \ddots &        & \vdots  \\
		\vdots  & \ddots & \ddots & \vdots  \\
		0       & \dots  & 0      & u_{n,n}
	\end{bmatrix}
\]

Se siamo in grado di fare questo allora diciamo che $A$ è \textbf{fattorizzabile} nella forma $LU$
(fattorizzazione di Gauss).

\begin{observation}
	Se $A$ può essere scritta in questa forma allora, per il teorema di Binet, possiamo dire che
	\[ \det(A) = \det(L) \cdot \det(U) \]
	Dato che $L$ ha tutti 1 sulla diagonale principale $\det(L) = 1$ quindi vale che
	\[ \det(A) = \det(U) \neq 0 \]
	Ne deduciamo quindi che, se tale fattorizzazione esiste, allora $L$ e $U$ sono invertibili. Vale quindi che
	\[ A x = b \quad \Leftrightarrow \quad LU x = b \]
	Imponendo $U x = y$ possiamo scrivere
	\[
		\begin{cases}
			L y = b \\
			U x = y
		\end{cases}
	\]
	Il sistema lineare è dunque risolvibile tramite la risoluzione di due sistemi triangolari.
\end{observation}

\begin{observation}
	In questo modo abbiamo anche un metodo veloce per il calcolo del determinante di $A$ in quanto questo equivale
	al determinante di $U$. E dato che $U$ è triangolare il suo determinante è equivalente al prodotto degli
	elementi sulla diagonale principale.
\end{observation}

Il problema è che, in generale, una decomposizione di questo tipo \textbf{non esiste} e se esiste
\textbf{non è univocamente} determinata.

\begin{example}
	Prendiamo la matrice
	\[
		A  = \begin{bmatrix}
			0 & 1 \\
			1 & 0
		\end{bmatrix}
	\]
	e proviamo a cercare una fattorizzazione per tale matrice in questo modo
	\[
		\begin{bmatrix}
			0 & 1 \\
			1 & 0
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0 \\
			x & 1
		\end{bmatrix} \cdot
		\begin{bmatrix}
			u_1 & u_2 \\
			0   & u_3
		\end{bmatrix}
	\]
	Vogliamo provare a trovare $x$, $u_1$, $u_2$ e $u_3$ che soddisfano questa relazione. Se facciamo prima riga
	per prima colonna notiamo subito che $u_1$ deve essere uguale a 0, ma a questo punto, se proviamo a fare
	seconda riga per prima colonna otteniamo come risultato 0 per qualunque valore di $x$, quando invece dovremmo
	ottenere come risultato 1.
\end{example}

In generale, possiamo dire che una fattorizzazione di quel tipo non esiste neanche se la matrice è invertibile.

\begin{example}
	Proviamo ora a trovare una fattorizzazione per la matrice nulla
	\[
		\begin{bmatrix}
			0 & 0 \\
			0 & 0
		\end{bmatrix} =
		\begin{bmatrix}
			1 & 0 \\
			x & 1
		\end{bmatrix} \cdot
		\begin{bmatrix}
			0 & 0 \\
			0 & 0
		\end{bmatrix}
	\]
	Come possiamo vedere, la fattorizzazione in questo caso esiste ma non è univocamente determinata. Questo
	perché, per qualsiasi valore di $x$, otteniamo una fattorizzazione $LU$.
\end{example}

Prima di proseguire introduciamo una notazione. Sia $A \in \R^{n \times n}$ la matrice ad elementi $(a_{i,j})$
chiamiamo $A_k = (a_{i,j}) \in \R^{k \times k}$ \textbf{sottomatrici principali di testa} di $A$. Si tratta delle
matrici formate dalle prime $k$ righe e prime $k$ colonne di $A$.

\begin{theorem}
	Sia $A \in \R^{n \times n}$, se $\det(A_k) \neq 0$ per $1 \leq k \leq n-1$ allora esiste ed è unica la
	fattorizzazione $LU$ di $A$.
	\begin{proof}
		Dimostriamo per induzione su $n$. Per $n = 1$ allora $n-1 = 0$ dunque il teorema ci dice che, presa
		una matrice $1 \times 1$ esiste ed è unica la sua fattorizzazione $LU$
		\[ (a) = (1) \cdot (a) \]
		Assumiamo che il teorema sia vero per una qualsiasi matrice di ordine fino a $n-1$ e consideriamo una
		matrice $A$ di ordine $n$ che scriviamo in questo modo
		\[
			A = \begin{bmatrix}
				A_{n-1} & z       \\
				w^T     & a_{n,n}
			\end{bmatrix} = L \cdot U =
			\begin{bmatrix}
				L_{n-1} & 0 \\
				x^T     & 1
			\end{bmatrix} \cdot
			\begin{bmatrix}
				U_{n-1} & y       \\
				0       & u_{n,n}
			\end{bmatrix}
		\]
		Vogliamo quindi trovare due matrici $L$ e $U$ tali che $L \cdot U = A$. Per riuscire a farlo cerchiamo
		gli elementi dei vari partizionamenti con i quali abbiamo organizzato le matrici tali che $L \cdot U = A$.

		Tenendo conto delle dimensioni è possibile fare il prodotto righe per colonne anche per interi blocchi di
		matrice. Otteniamo quindi
		\[
			\begin{cases}
				L_{n-1} \cdot U_{n-1} = A_{n-1} \\
				L_{n-1} \cdot y = z             \\
				x^T \cdot U_{n-1} = w^T         \\
				x^T \cdot y + u_{n,n} = a_{n,n}
			\end{cases}
		\]
		La prima equazione ci dice che $L_{n-1} \cdot U_{n-1}$ è una fattorizzazione della matrice $A_{n-1}$.
		Dato che $A_{n-1}$ è una matrice di ordine $n-1$ e le sue sottomatrici principali di testa fino all'ordine
		$n-2$ sono invertibili per l'ipotesi del teorema e per ipotesi induttiva il teorema è vero per tutte le
		matrici fino all'ordine $n-1$. Quindi tale fattorizzazione esiste ed è unica.

		Per trovare $y$ dobbiamo risolvere un sistema lineare e vale
		\[ y = L_{n-1}^{-1} \cdot z \]
		e sappiamo che $L_{n-1}$ è invertibile perché si tratta di una matrice triangolare con tutti 1 sulla
		diagonale principale.

		Anche la terza equazione è un sistema lineare e, dato che $U_{n-1}$ è invertibile per ipotesi del teorema
		la quale ci garantisce l'invertibilità fino all'ordine $n-2$ ma anche $A_{n-1}$ è invertibile e quindi,
		per il teorema di Binet vale che
		\[ \det(A_{n-1}) = \det(U_{n-1}) \]
		Possiamo quindi scrivere
		\[ x^T = w^T \cdot U_{n-1} \]
		Una volta ricavati $x^T$ e $y$ troviamo $u_{n,n}$ in modo univoco calcolando
		\[ u_{n,n} = a_{n,n} - x^T \cdot y \]
		Il sistema è quindi univocamente risolubile e quindi la fattorizzazione esiste ed è unica.
	\end{proof}
	Quando le ipotesi del teorema sono violate la fattorizzazione non esiste o non è univocamente determinata.
\end{theorem}

La dimostrazione del teorema appena enunciato ci dice che se siamo interessati a calcolare la fattorizzazione
$LU$ di una matrice di ordine $n$, è possibile farlo calcolando la fattorizzazione $LU$ della sua sottomatrice
di ordine $n-1$ per poi aggiornarla con tale costruzione.

Supponendo di avere la fattorizzazione di ordine $n-1$, il costo per trovare al fattorizzazione di ordine $n$
è dato dal costo per trovare la fattorizzazione di ordine $n-1$ sommato al costo dell'\emph{aggiornamento}.

Il costo dell'aggiornamento è pari al costo della risoluzione di due sistemi triangolari di ordine $n-1$ ossia
\[ c (n - 1)^2 \]
Il costo complessivo dell'algoritmo per la fattorizzazione è dunque
\[ F(n) = F(n-1) + c (n-1)^2 = c \cdot \sum_{i=1}^{n} (n - i)^2 \]
Il costo che ne ricaviamo è di $O(n^3)$ operazioni. Il problema di questo metodo è che, implementato in macchina,
riporta problemi di stabilità.

L'algoritmo può essere comunque interessanti in alcuni casi, ad esempio per alcune classi di matrici particolari.

\begin{example}
	Prendiamo la matrice $A \in \R^{n \times n}$ così definita
	\[
		A = \begin{bmatrix}
			1      & 0      & \dots  & 0      & x_1    \\
			0      & \ddots & \ddots & \vdots & \vdots \\
			\vdots & \ddots & \ddots & 0      & \vdots \\
			0      & \dots  & 0      & 1      & \vdots \\
			x_1    & \dots  & \dots  & \dots  & x_n
		\end{bmatrix}
	\]
	detta \textbf{matrice a freccia}. Il problema che ci poniamo è trovare per quali $x_i$ con $1 \leq i \leq n$
	la matrice ammette fattorizzazione $LU$ e in caso affermativo, trovare la fattorizzazione $LU$ per tali
	valori.

	\`E immediato notare che le sottomatrici principali di testa di $A$ sono tutte matrici identità e dunque vale
	\[ A_k = I_k \]
	quindi sono tutte invertibili. Per il teorema esiste ed è unica la sua fattorizzazione.

	Per trovare tale fattorizzazione dobbiamo trovare la fattorizzazione $LU$ della matrice di ordine $n-1$. Per
	farlo scriviamo
	\[
		A = \begin{bmatrix}
			L_{n-1} & 0 \\
			x^T     & 1
		\end{bmatrix} \cdot
		\begin{bmatrix}
			U_{n-1} & y       \\
			0       & u_{n,n}
		\end{bmatrix}
	\]
	Come abbiamo detto, abbiamo che
	\[ A_{n-1} = I_{n-1} \]
	L'identità di ordine $n-1$ si fattorizza come l'identità per se stessa. Possiamo quindi scrivere
	\[
		A = \begin{bmatrix}
			I_{n-1} & 0 \\
			x^T     & 1
		\end{bmatrix} \cdot
		\begin{bmatrix}
			I_{n-1} & y       \\
			0       & u_{n,n}
		\end{bmatrix}
	\]
	A questo punto ricaviamo che
	\begin{align*}
		y =       & \begin{bmatrix} x_1 \\ \vdots \\ x_{n-1} \end{bmatrix} \\
		x^T =     & \begin{bmatrix} x_1 & \dots & x_{n-1} \end{bmatrix}    \\
		u_{n,n} = & x_n - \sum_{i=1}^{n-1} x_i^2
	\end{align*}
	Quando $u_{n,n} \neq 0$ allora la matrice è invertibile, ossia quando
	\[ x_n - \sum_{i=1}^{n-1} x_i^2 \neq 0 \]
	Risolvere i sistemi lineari per $L$ e $U$ appena trovate con l'eliminazione gaussiana ha un costo di $O(n)$
	operazioni.
\end{example}

\begin{definition}
	La matrice $A$ si dice \textbf{tridiagonale} se, per $|i - j| > 1$, l'elemento $a_{i,j}$ è uguale a 0.
\end{definition}

\begin{definition}
	Sia $A$ una matrice di dimensione $n \times n$, $A$ si dice \textbf{predominante diagonale} se il valore
	assoluto di ogni elemento diagonale è maggiore alla somma dei valori assoluti degli elementi sulla stessa
	riga escluso l'elemento diagonale.
	\[ |a_{i,i}| > \sum_{j=1, j \neq i}^{n} |a_{i,j}| \quad i = 1 \dots n \]
	In questo caso la matrice si dice \emph{predominante diagonale per righe}. Se
	l'operazione viene fatta effettuando la somma degli elementi di ciascuna colonna si parlerà di
	\emph{predominanza diagonale per colonne}.
\end{definition}

\begin{proposition}
	Se una matrice è predominante diagonale allora è invertibile.
\end{proposition}

\begin{proposition}
	Se una matrice è predominante diagonale allora anche le sue sottomatrici principali di testa sono predominanti
	diagonali.
\end{proposition}

\begin{proposition}
	Se una matrice è predominante diagonale esiste ed è unica la sua fattorizzazione $LU$ in quanto le sue
	sottomatrici principali di testa sono anch'esse predominanti diagonali e quindi invertibili.
\end{proposition}

\begin{example}
	Prendiamo la seguente matrice \emph{tridiagonale} $A \in \R^{n \times n}$
	\[
		A = \begin{bmatrix}
			3      & -1     & 0      & \dots  & 0      \\
			-1     & \ddots & \ddots & \ddots & \vdots \\
			0      & \ddots & \ddots & \ddots & 0      \\
			\vdots & \ddots & \ddots & \ddots & -1     \\
			0      & \dots  & 0      & -1     & 3
		\end{bmatrix}
	\]
	Se volessimo applicare il teorema dobbiamo andare a verificare l'invertibilità di tutte le sottomatrici fino
	all'ordine $n-1$. Le sottomatrici sono tutte \emph{predominanti diagonali} in quanto, se $A$ è predominante
	diagonale anche le sue sottomatrici sono predominanti diagonali e dunque invertibili.
\end{example}

\subsection{Eliminazione gaussiana}
Parliamo ora di come sia possibile calcolare una fattorizzazione $LU$ per la risoluzione di sistemi lineari
tramite eliminazione gaussiana.

\subsubsection{Matrici elementari di Gauss}
Fino ad ora non abbiamo parlato di fattorizzazione $LU$ ma non abbiamo un vero e proprio algoritmo per riuscire a
calcolarla. A tal proposito dobbiamo introdurre le \textbf{matrici elementari di Gauss}, ossia matrici elementari
espresse come
\[ A = I + u \cdot v^T \]
Le matrici elementari di Gauss sono particolari matrici elementari nelle quali il vettore $v$ è un vettore della
base canonica e il vettore $u$ ha le prime $j$ componenti nulle.
\[ A = I + u \cdot e_j^T \]
con $u_1 = u_2 = \dots = u_j = 0$. Le matrici elementari di Gauss sono triangolari inferiori con tutti 1 sulla
diagonale principale. Per esempio per $j=1$ abbiamo
\[
	I + u \cdot e_1^T =
	\begin{bmatrix}
		1   & 0 & 0 \\
		u_2 & 1 & 0 \\
		u_3 & 0 & 1
	\end{bmatrix}
\]
\begin{theorem}
	L'inversa di una matrice elementare di Gauss è equivalente alla stessa matrice dove si cambia segno agli
	elementi del vettore $u$.
	\[ A^{-1} = I - u \cdot e_j^T \]
	\begin{proof}
		Per dimostrarlo è sufficiente verificare la seguente uguaglianza
		\[ (I + u \cdot e_j^T) \cdot (I - u \cdot e_j^T) = I \]
		Sviluppando il prodotto otteniamo
		\[
			(I + u \cdot e_j^T) \cdot (I - u \cdot e_j^T) =
			I + u \cdot e_j^T - u \cdot e_j^T - u \cdot e_j^T \cdot u \cdot e_j^T
		\]
		Dato che il prodotto riga per colonna tra matrici è associativo è possibile svolgere per primo il prodotto
		riga per colonna
		\[ e_j^T \cdot u = 0 \]
		A questo punto l'ultimo termine dell'equazione si annulla e otteniamo l'identità.
	\end{proof}
\end{theorem}

Le matrici elementari di Gauss godono di una proprietà importante per riuscire a calcolare la fattorizzazione
$LU$ di una matrice $A$. In particolare questa proprietà è necessaria per riuscire a mettere degli zeri dove
necessario in modo da ottenere una matrice triangolare.

\begin{theorem}\label{th: eliminazione_gauss}
	Sia $x \in \R^n$ con $x_k \neq 0$ allora esiste una matrice elementare di Gauss $E$ tale che
	\[
		E \cdot x = \begin{bmatrix}
			x_1 \\ \vdots \\ x_k \\ 0 \\ \vdots \\ 0
		\end{bmatrix}
	\]
	\begin{proof}
		Prendiamo
		\[ E = I + u e_k^T \]
		con $u_1 = \cdots = u_k = 0$. Calcoliamo quindi $E \cdot x$
		\[
			(I + u e_k^T) x = x + u x_k =
			\begin{bmatrix}
				x_1 \\ \vdots \\ x_k \\ x_{k+1} \\ \vdots \\ x_n
			\end{bmatrix} + x_k
			\begin{bmatrix}
				0 \\ \vdots \\ 0 \\ u_{k+1} \\ \vdots \\ u_n
			\end{bmatrix} =
			\begin{bmatrix}
				x_1 \\ \vdots \\ x_k \\ x_{k+1} + x_k u_{k+1} \\ \vdots \\ x_n + x_k u_n
			\end{bmatrix}
		\]
		Dato che vogliamo le componenti dalla $(k+1)$-esima alla $n$-esima uguali a 0 basta imporre
		\begin{align*}
			u_{k+1} = & -\frac{x_{k+1}}{x_k} \\
			\vdots    &                      \\
			u_n =     & -\frac{x_n}{x_k}
		\end{align*}
		Le quali sono quantità ben definite dato che $x_k \neq 0$.
	\end{proof}
\end{theorem}

\subsubsection{Algoritmo per l'eliminazione gaussiana}
Passiamo ora al calcolo della fattorizzazione $LU$ tramite matrici elementari di Gauss. Supponiamo di avere una
generica matrice $A$ così definita
\[
	A = A_0 = \begin{bmatrix}
		a_{1,1}^{(0)} & \dots & a_{1,n}^{(0)} \\
		\vdots        &       & \vdots        \\
		a_{n,1}^{(0)} & \dots & a_{n,n}^{(0)}
	\end{bmatrix}
\]
Per questo metodo supponiamo $a_{1,1}^{(0)} \neq 0$. Allora possiamo determinare $E_1$ tale che
\[
	E_1 \cdot \begin{bmatrix}
		a_{1,1}^{(0)} \\ \vdots \\ a_{n,1}^{(0)}
	\end{bmatrix} =
	\begin{bmatrix}
		a_{1,1}^{(0)} \\ 0 \\ \vdots \\ 0
	\end{bmatrix}
\]
Ricaviamo quindi che
\[ E_1 = I + u e_1^T \]
con
\[
	u = \begin{bmatrix}
		0 \\ -\frac{a_{2,1}^{(0)}}{a_{1,1}^{(0)}} \\ \vdots \\ -\frac{a_{n,1}^{(0)}}{a_{1,1}^{(0)}}
	\end{bmatrix}
\]
A questo punto, possiamo generare la matrice $A_1$ come segue
\[ A_1 = E_1 \cdot A_0 = E_1 \cdot A \]
La matrice $A_1$ ha la seguente forma
\[
	A_1 = \begin{bmatrix}
		a_{1,1}^{(1)} & \dots         & \dots & a_{1,n}^{(1)} \\
		0             & a_{2,2}^{(1)} & \dots & a_{2,n}^{(1)} \\
		\vdots        & \vdots        &       & \vdots        \\
		0             & a_{n,2}^{(1)} & \dots & a_{n,n}^{(1)}
	\end{bmatrix}
\]
dove
\[ a_{1,i}^{(1)} = a_{1,i}^{(0)} \]
e con
\[ a_{i,j}^{(1)} = a_{i,j}^{(0)} + \left( -\frac{a_{i,1}^{(0)}}{a_{1,1}^{(0)}} \right) \cdot a_{1,j}^{(0)} \]
Questo per ogni elemento $a_{i,j}^{(1)}$ con $2 \leq i,j \leq n$.

Gli elementi sulla prima colonna di $E_1$ eccetto l'1 sulla diagonale sono detti \textbf{moltiplicatori} di
Gauss. Per riuscire a calcolarli dobbiamo fare $n-1$ divisioni. Una volta ottenuti dobbiamo fare un prodotto
e un'addizione per $(n-1)^2$ elementi per aggiornare la matrice. Ne deduciamo che il passaggio da $A_0$ ad $A_1$
è costato $(n-1)^2$ operazioni moltiplicative.

Ora passiamo alla seconda colonna e supponiamo $a_{2,2}^{(1)} \neq 0$ allora possiamo determinare $E_2$ tale che
\[
	E_2 \cdot \begin{bmatrix}
		a_{1,2}^{(1)} \\ \vdots \\ a_{n,2}^{(1)}
	\end{bmatrix} =
	\begin{bmatrix}
		a_{1,2}^{(1)} \\ a_{2,2}^{(1)} \\ 0 \\ \vdots \\ 0
	\end{bmatrix}
\]
Per il teorema \ref{th: eliminazione_gauss} abbiamo che
\[ E_2 = I + u e_2^T \]
dove
\[
	u = \begin{bmatrix}
		0 \\ 0 \\ -\frac{a_{3,2}^{(1)}}{a_{2,2}^{(1)}} \\ \vdots \\ -\frac{a_{n,2}^{(1)}}{a_{2,2}^{(1)}}
	\end{bmatrix}
\]
Una volta ottenuto $u$ possiamo calcolare $A_2$ in questo modo
\[ A_2 = E_2 \cdot A_1 = E_2 \cdot E_1 \cdot A_0 \]
ottenendo una matrice con la seguente forma
\[
	A_2 = \begin{bmatrix}
		a_{1,1}^{(2)} & \dots         & \dots         & \dots & a_{1,n}^{(2)} \\
		0             & a_{2,2}^{(2)} & \dots         & \dots & a_{2,n}^{(2)} \\
		\vdots        & 0             & a_{3,3}^{(2)} & \dots & a_{3,n}^{(2)} \\
		\vdots        & \vdots        & \vdots        &       & \vdots        \\
		0             & 0             & a_{n,3}^{(2)} & \dots & a_{n,n}^{(2)}
	\end{bmatrix}
\]
A questo punto se $a_{3,3}^{(2)} \neq 0$ possiamo andare avanti fino a $a_{n,n}^{(n-1)}$ definendo così tutte
le matrici elementari di Gauss $E_i$ con $1 \leq i \leq n-1$ e ottenendo la matrice triangolare superiore $U$
in questo modo
\[ E_{n-1} \dots E_1 A_0 = U \]
Non ci rimane che calcolare $L$. Per farlo scriviamo
\[ A = A_0 = E_1^{-1} \dots E_{n-1}^{-1} U \]
da cui possiamo dedurre che
\[ L = E_1^{-1} \dots E_{n-1}^{-1} \]
Valutiamo ora il costo computazionale per il calcolo di una fattorizzazione $LU$ utilizzando il metodo appena
descritto. In generale, il passaggio da $A_i$ ad $A_j$ ha costo $O((n-j)^2)$ operazioni. Il costo complessivo
della computazione è quindi $O(n^3)$.

\begin{example}
	Calcoliamo la fattorizzazione $LU$ della matrice $A$ definita come segue
	\[ A = \begin{bmatrix}
			1 & 1 & 1 \\
			1 & 2 & 2 \\
			1 & 2 & 3
		\end{bmatrix}
	\]
	Prima di tutto vogliamo calcolarci $E_1$ tale che
	\[ E_1 \begin{bmatrix}
			1 \\ 1 \\ 1
		\end{bmatrix} = \begin{bmatrix}
			1 \\ 0 \\ 0
		\end{bmatrix}
	\]
	Per il teorema \ref{th: eliminazione_gauss} possiamo prendere $E_1$ tale che
	\[ E_1 = I + u e_1^T \]
	con
	\[ u = \begin{bmatrix} 0 \\ -1 \\ -1 \end{bmatrix} \]
	Ricaviamo quindi
	\[
		E_1 = \begin{bmatrix}
			1  & 0 & 0 \\
			-1 & 1 & 0 \\
			-1 & 0 & 1
		\end{bmatrix}
	\]
	Possiamo ora calcolare $A_1$
	\[
		A_1 = E_1 A = \begin{bmatrix}
			1 & 1 & 1 \\
			0 & 1 & 1 \\
			0 & 1 & 2
		\end{bmatrix}
	\]
	Dato che $a_{2,2}^{(1)} \neq 0$ allora procediamo con il calcolo di $E_2$ tale che
	\[
		E_2 = I + u e_2^T = \begin{bmatrix}
			1 & 0  & 0 \\
			0 & 1  & 0 \\
			0 & -1 & 1 \\
		\end{bmatrix}
	\]
	A questo punto possiamo calcolare $A_2$
	\[
		A_2 = E_2 A_1 = \begin{bmatrix}
			1 & 1 & 1 \\
			0 & 1 & 1 \\
			0 & 0 & 1
		\end{bmatrix} = U
	\]
	Non ci rimane che trovare $L$ in questo modo
	\[
		E_1^{-1} E_2^{-1} = \begin{bmatrix}
			1 & 0 & 0 \\
			1 & 1 & 0 \\
			1 & 0 & 1
		\end{bmatrix} \begin{bmatrix}
			1 & 0 & 0 \\
			0 & 1 & 0 \\
			0 & 1 & 1
		\end{bmatrix} = \begin{bmatrix}
			1 & 0 & 0 \\
			1 & 1 & 0 \\
			1 & 1 & 1
		\end{bmatrix} = L
	\]
	Facciamo il prodotto $L \cdot U$ per verificare che la fattorizzazione sia corretta.
\end{example}

\subsubsection{Eliminazione gaussiana con pivoting parziale}
Supponiamo di avere ora una matrice $A$ definta come segue
\[
	A = \begin{bmatrix}
		1 & 1 & 1 \\
		1 & 1 & 2 \\
		1 & 2 & 3
	\end{bmatrix}
\]
In questo caso se provassimo ad applicare l'algoritmo appena visto otteniamo dopo il primo passaggio
\[
	A_1 = \begin{bmatrix}
		1 & 1 & 1 \\
		0 & 0 & 1 \\
		0 & 1 & 2
	\end{bmatrix}
\]
La quale non ci permette di compiere un ulteriore passo. Ma come possiamo notare, scambiando le ultime due righe
si giunge ad una forma triangolare. Questa \textbf{non} è una fattorizzazione $LU$ di $A$, vale infatti
\[ P_2 E_1 A = U \]
dove $P_2$ è detta \textbf{matrice di permutazione}, la quale scambia le ultime due righe ed è definita come
segue
\[
	P_2 = \begin{bmatrix}
		1 & 0 & 0 \\
		0 & 0 & 1 \\
		0 & 1 & 0
	\end{bmatrix}
\]
A questo punto per ricaviamo $A$ in questo modo
\[ A = E_1^{-1} P_2^{-1} U \]
L'inversa di una matrice di permutazione coincide con la trasposta.
\[ P^{-1} = P^T \]

Consideriamo ora la generica matrice $A$ e supponiamo che sia invertibile. Se la matrice è invertibile allora
sulla prima colonna c'è almeno un elemento non nullo.
\[
	A = \begin{bmatrix}
		a_{1,1}^{(0)} & \dots & a_{1,n}^{(0)} \\
		\vdots        &       & \vdots        \\
		a_{n,1}^{(0)} & \dots & a_{n,n}^{(0)}
	\end{bmatrix}
\]
Se ci troviamo nell'ipotesi che $A$ sia invertibile l'algoritmo, all'$i$-esimo passo procede in questo modo:
\begin{enumerate}
	\item Per motivi di stabilità andiamo a scegliere l'elemento di modulo massimo della $i$-esima colonna.
	      Tale elemento è detto \textbf{pivot} e si trova sulla riga $k$ con $k$ tale che
	      \[
		      | a_{k,i}^{(i-1)} | = \norm{
			      \begin{bmatrix}
				      a_{1,i}^{(i-1)} \\ \vdots \\ a_{n,i}^{(i-1)}
			      \end{bmatrix}}_\infty
	      \]
	      L'elemento di modulo massimo garantisce che gli elementi di $L$ saranno più piccoli possibile. Per
	      trovarlo si ha un costo di $O(n)$ operazioni (scansione del vettore).
	\item Scambiamo l'$i$-esima riga con la $k$-esima determinando una matrice $P_i$ che effettua tale scambio
	      \[ P_i \cdot A_{i-1} \]
	\item Procediamo con l'eliminazione di gauss determinando $E_i$ e ricavando la matrice $A_i$ in questo modo
	      \[ A_i = E_i \cdot P_i \cdot A_{i-1} \]
\end{enumerate}
Una volta eseguiti $n-1$ passi dell'algoritmo otteniamo $U$ calcolando
\[ U = E_{n-1} P_{n-1} \dots E_1 P_1 A_0 \]
che possiamo riscrivere come
\[ A = E_{n-1} P_{n-1} \dots E_1 P_1 U = L U \]
Dove $U$ è una triangolare superiore e $L$ non è triangolare inferiore per effetto del prodotto con le matrici
di permutazione.

Questa fattorizzazione è calcolabile per una qualsiasi matrice invertibile e ha ottime proprietà di stabilità
numerica.

Dato che i pivot sono scelti prendendo l'elemento di modulo massimo della colonna che abbiamo come
riferimento, questa tecnica è detta \textbf{eliminazione gaussiana con pivoting parziale} che è anche l'algoritmo
di riferimento che viene usato in MatLab tramite l'operatore \verb|\| quando vogliamo risolvere un sistema lineare
$Ax = b$ e calcoliamo il vettore soluzione tramite
\begin{center}
	\verb|x = A \ b|
\end{center}
Se invece andassimo a scegliere l'elemento di modulo massimo su tutta la matrice dovremmo permutare sia righe
che colonne, si parlerebbe di \textbf{eliminazione gaussiana con pivoting totale}, la quale ha stabilità ancora
migliore ma intacca la complessità, in quanto la ricerca dell'elemento di modulo massimo dell'intera matrice ha
costo $O(n^2)$.

Per il calcolo del determinante teniamo sempre a mente che, per il teorema di Binet, vale
\[ \det(A) = \det(L) \cdot \det(U) \]
Per calcolare $\det(L)$ teniamo a mente che il determinante di una matrice elementare di Gauss è 1, mentre il
determinante di una matrice di permutazione può essere $\pm 1$. Dobbiamo quindi calcolare il prodotto tra tutti
i $\pm 1$ e moltiplicarlo per $\det(U)$. Dato che $U$ è una matrice triangolare il determinante equivale al
prodotto di tutti gli elementi sulla diagonale.

Per la risoluzione del sistema lineare $Ax = b$ non è necessario calcolare fattorizzazione di $A$. Possiamo
applicare le matrici di eliminazione e permutazione ad $A$ e $b$ passo dopo passo, ottenendo dei sistemi
equivalenti. Questo ci fa risparmiare memoria evitando di mantenere tutte le matrici di eliminazione e
permutazione.
