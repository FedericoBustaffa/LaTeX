\section{Risoluzione di sistemi lineari}
Vediamo ora alcuni metodi per la risoluzione di sistemi lineari e andiamo a studiare il loro condizionamento
in macchina. Il problema che vogliamo provare a risolvere ha in input la matrice $A \in \R^{n \times n}$ e il
vettore $b \in \R^n$ dai quali vogliamo ricavare il vettore $x \in \R^n$ tale che $A x = b$.

\subsection{Matrici diagonali}
La prima cosa che vogliamo osservare è che per particolari classi della matrice $A$ il sistema è facilmente
risolubile. Prendiamo ad esempio la classe delle matrici \textbf{diagonali} ossia matrici con tutti gli elementi
fuori dalla diagonale principale nulli (per $i \neq j$ vale $a_{i,j} = 0$). Per esempio sono matrici diagonali
\[
	\begin{pmatrix}
		1 & 0 & 0 \\
		0 & 3 & 0 \\
		0 & 0 & 2
	\end{pmatrix} \quad
	\begin{pmatrix}
		2 & 0 & 0 \\
		0 & 5 & 0 \\
		0 & 0 & 0
	\end{pmatrix} \quad
	\begin{pmatrix}
		0 & 0 & 0 \\
		0 & 0 & 0 \\
		0 & 0 & 0
	\end{pmatrix}
\]
Se proviamo a calcolare il determinante della matrice $A$ possiamo vedere che questo equivale al prodotto degli
elementi sulla diagonale principale, così come possiamo dimostrare che gli autovalori sono esattamente gli
elementi sulla diagonale principale.

Segue dalle considerazioni appena fatte che una matrice diagonale è invertibile se e solo se ha tutti gli elementi
sulla diagonale principale diversi da 0. Per risolvere un sistema diagonale possiamo scrivere ogni equazione del
sistema nella forma
\[ a_{i,i} x_i = b_i \]
ottenendo facilmente che
\[ x_i = \frac{b_i}{a_{i,i}} \]
che è ben definito se tutti gli $a_{i,i}$ sono diversi da 0. Un semplice algoritmo che risolve un sistema diagonale
$n \times n$ è il seguente:
\begin{lstlisting}[language=pseudo, style=pseudo-style]
for i = 1 to n
	x[i] = b[i] / a[i][i]
\end{lstlisting}
Dunque il costo per risoluzione di un sistema diagonale è di $O(n)$ operazioni.

\subsection{Matrici triangolari}
Prendiamo ora in esame il caso delle matrici triangolari.

\begin{definition}
	Una matrice si dice \textbf{triangolare} se ha tutti elementi nulli sopra la diagonale principale e, in
	tal caso, si dirà \textbf{inferiore} (per $i < j$ vale $a_{i,j} = 0$). Se invece ha tutti elementi nulli
	sotto la diagonale principale si dirà \textbf{superiore} (per $i > j$ vale $a_{i,j} = 0$).
\end{definition}

Non facciamo nessuna assunzione sugli altri elementi: la matrice nulla è sia triangolare superiore che inferiore.
Esempi di matrici triangolari inferiori e superiori sono
\[
	\begin{pmatrix}
		1 & 0 & 0 \\
		2 & 4 & 0 \\
		3 & 1 & 9
	\end{pmatrix} \quad
	\begin{pmatrix}
		1 & 3 & 5 \\
		0 & 2 & 7 \\
		0 & 0 & 4
	\end{pmatrix}
\]
Il determinante di una matrice triangolare, se calcolato ad esempio con lo sviluppo di Laplace secondo la prima
colonna, risulta ancora una volta equivalente al prodotto degli elementi sulla diagonale principale.

Il discorso fatto per gli autovalori di matrici diagonali vale anche qui, dato che la matrice $A - \lambda I$ è
ancora una matrice triangolare, abbiamo anche in questo caso che gli autovalori di una matrice triangolare sono
esattamente gli elementi sulla diagonale principale.

Prendiamo un generico sistema di equazioni $A x = b$ con $A$ triangolare superiore. Otterremo qualcosa di questo
tipo
\[
	\begin{cases}
		a_{1,1} x_1 + a_{1,2} x_2 + \dots + a_{1,n} x_n = b_1 \\
		a_{2,2} x_2 + \dots + a_{2,n} x_n = b_2               \\
		\dots                                                 \\
		a_{n,n} x_n = b_n
	\end{cases}
\]
Per risolvere il sistema si fa ricorso al metodo di \textbf{sostituzione all'indietro}: si ricava $x_n$
dall'ultima equazione, lo si sostituisce nella penultima trovando $x_{n-1}$ e così via fino a ricavare tutti gli
$x_i$.

Supponiamo di aver già ricavato $x_{k+1}, \dots, x_n$ e di voler determinare $x_k$. Dalla $k$-esima equazione si
ottiene
\[
	a_{k,k} x_k = \sum_{j=k+1}^n a_{k,j} x_j = b_k \quad \to \quad
	x_k = \frac{b_k - \displaystyle\sum_{j=k+1}^n a_{k,j} x_j}{a_{k,k}}
\]
In MatLab la \emph{sostituzione all'indietro} sarebbe così implementata:
\begin{lstlisting}[language=matlab]
function[x] = solve_tri(a, b)
	n = length(b);
	x = zeros(n, 1);
	x(n) = b(n) / a(n, n);
	for k = n-1 : -1 : 1
		s = 0;
		for j = k+1 : n
			s = s+ a(k, j) * x(j);
		end
		x(k) = (b(k) - s) / a(k, k);
	end
end
\end{lstlisting}
Il costo computazionale di questo algoritmo è di $O(n^2)$ moltiplicazioni:
\[ \sum_{k=1}^n k = \frac{n (n + 1)}{2} \]
Se $A \in \R^{n \times n}$ si può pensare, per la risoluzione del sistema lineare $A x = b$, di \emph{ridurre}
progressivamente $A$ in forma triangolare mediante una sequenza di trasformazioni del tipo
\begin{align*}
	A_0 = & A, \quad A_k \to A_{k+1}, \quad 0 \leq k \leq n - 2; \\
	b_0 = & b, \quad b_k \to b_{k+1}, \quad 0 \leq k \leq n - 2
\end{align*}

\subsection{Fattorizzazione LU}
Introduciamo quindi il concetto di \textbf{fattorizzazione}, ossia scrivere una matrice come prodotto di più
matrici.

Vogliamo quindi rappresentare la matrice $A \in \R^{n \times n}$ come prodotto di due matrici $L$ ed $U$, la
prima triangolare inferiore con elementi uguali a 1 sulla diagonale pricipale e la seconda triangolare superiore.
\[
	A = L \cdot U = \begin{pmatrix}
		1       & 0      & \dots     & 0      \\
		l_{2,1} & \ddots & \ddots    & \vdots \\
		\vdots  & \ddots & \ddots    & 0      \\
		l_{n,1} & \dots  & l_{n,n-1} & 1
	\end{pmatrix} \cdot
	\begin{pmatrix}
		u_{1,1} & \dots  & \dots  & u_{1,n} \\
		0       & \ddots &        & \vdots  \\
		\vdots  & \ddots & \ddots & \vdots  \\
		0       & \dots  & 0      & u_{n,n}
	\end{pmatrix}
\]

Se siamo in grado di fare questo allora diciamo che $A$ è \textbf{fattorizzabile} nella forma $LU$
(fattorizzazione di Gauss).

\begin{observation}
	Se $A$ può essere scritta in questa forma allora, per il teorema di Binet, possiamo dire che
	\[ \det(A) = \det(L) \cdot \det(U) \]
	Dato che $L$ ha tutti 1 sulla diagonale principale $\det(L) = 1$ quindi vale che
	\[ \det(A) = \det(U) \neq 0 \]
	Ne deduciamo quindi che, se tale fattorizzazione esiste, allora $L$ e $U$ sono invertibili. Vale quindi che
	\[ A x = b \quad \Leftrightarrow \quad LU x = b \]
	Imponendo $U x = y$ possiamo scrivere
	\[
		\begin{cases}
			L y = b \\
			U x = y
		\end{cases}
	\]
	Il sistema lineare è dunque risolvibile tramite la risoluzione di due sistemi triangolari.
\end{observation}

\begin{observation}
	In questo modo abbiamo anche un metodo veloce per il calcolo del determinante di $A$ in quanto questo equivale
	al determinante di $U$. E dato che $U$ è triangolare il suo determinante è equivalente al prodotto degli
	elementi sulla diagonale principale.
\end{observation}

Il problema è che, in generale, una decomposizione di questo tipo \textbf{non esiste} e se esiste
\textbf{non è univocamente} determinata.

\begin{example}
	Prendiamo la matrice
	\[
		A  = \begin{pmatrix}
			0 & 1 \\
			1 & 0
		\end{pmatrix}
	\]
	e proviamo a cercare una fattorizzazione per tale matrice in questo modo
	\[
		\begin{pmatrix}
			0 & 1 \\
			1 & 0
		\end{pmatrix} =
		\begin{pmatrix}
			1 & 0 \\
			x & 1
		\end{pmatrix} \cdot
		\begin{pmatrix}
			u_1 & u_2 \\
			0   & u_3
		\end{pmatrix}
	\]
	Vogliamo provare a trovare $x$, $u_1$, $u_2$ e $u_3$ che soddisfano questa relazione. Se facciamo prima riga
	per prima colonna notiamo subito che $u_1$ deve essere uguale a 0, ma a questo punto, se proviamo a fare
	seconda riga per prima colonna otteniamo come risultato 0 per qualunque valore di $x$, quando invece dovremmo
	ottenere come risultato 1.
\end{example}

In generale, possiamo dire che una fattorizzazione di quel tipo non esiste neanche se la matrice è invertibile.

\begin{example}
	Proviamo ora a trovare una fattorizzazione per la matrice nulla
	\[
		\begin{pmatrix}
			0 & 0 \\
			0 & 0
		\end{pmatrix} =
		\begin{pmatrix}
			1 & 0 \\
			x & 1
		\end{pmatrix} \cdot
		\begin{pmatrix}
			0 & 0 \\
			0 & 0
		\end{pmatrix}
	\]
	Come possiamo vedere, la fattorizzazione in questo caso esiste ma non è univocamente determinata. Questo
	perché, per qualsiasi valore di $x$, otteniamo una fattorizzazione $LU$.
\end{example}

Prima di proseguire introduciamo una notazione. Sia $A \in \R^{n \times n}$ la matrice ad elementi $(a_{i,j})$
chiamiamo $A_k = (a_{i,j}) \in \R^{k \times k}$ \textbf{sottomatrici principali di testa} di $A$. Si tratta delle
matrici formate dalle prime $k$ righe e prime $k$ colonne di $A$.

\begin{theorem}
	Sia $A \in \R^{n \times n}$, se $\det(A_k) \neq 0$ per $1 \leq k \leq n-1$ allora esiste ed è unica la
	fattorizzazione $LU$ di $A$.
	\begin{proof}
		Dimostriamo per induzione su $n$. Per $n = 1$ allora $n-1 = 0$ dunque il teorema ci dice che, presa
		una matrice $1 \times 1$ esiste ed è unica la sua fattorizzazione $LU$
		\[ (a) = (1) \cdot (a) \]
		Assumiamo che il teorema sia vero per una qualsiasi matrice di ordine fino a $n-1$ e consideriamo una
		matrice $A$ di ordine $n$ che scriviamo in questo modo
		\[
			A = \begin{pmatrix}
				A_{n-1} & z       \\
				w^T     & a_{n,n}
			\end{pmatrix} = L \cdot U =
			\begin{pmatrix}
				L_{n-1} & 0 \\
				x^T     & 1
			\end{pmatrix} \cdot
			\begin{pmatrix}
				U_{n-1} & y       \\
				0       & u_{n,n}
			\end{pmatrix}
		\]
		Vogliamo quindi trovare due matrici $L$ e $U$ tali che $L \cdot U = A$. Per riuscire a farlo cerchiamo
		gli elementi dei vari partizionamenti con i quali abbiamo organizzato le matrici tali che $L \cdot U = A$.

		Tenendo conto delle dimensioni è possibile fare il prodotto righe per colonne anche per interi blocchi di
		matrice. Otteniamo quindi
		\[
			\begin{cases}
				L_{n-1} \cdot U_{n-1} = A_{n-1} \\
				L_{n-1} \cdot y = z             \\
				x^T \cdot U_{n-1} = w^T         \\
				x^T \cdot y + u_{n,n} = a_{n,n}
			\end{cases}
		\]
		La prima equazione ci dice che $L_{n-1} \cdot U_{n-1}$ è una fattorizzazione della matrice $A_{n-1}$.
		Dato che $A_{n-1}$ è una matrice di ordine $n-1$ e le sue sottomatrici principali di testa fino all'ordine
		$n-2$ sono invertibili per l'ipotesi del teorema e per ipotesi induttiva il teorema è vero per tutte le
		matrici fino all'ordine $n-1$. Quindi tale fattorizzazione esiste ed è unica.

		Per trovare $y$ dobbiamo risolvere un sistema lineare e vale
		\[ y = L_{n-1}^{-1} \cdot z \]
		e sappiamo che $L_{n-1}$ è invertibile perché si tratta di una matrice triangolare con tutti 1 sulla
		diagonale principale.

		Anche la terza equazione è un sistema lineare e, dato che $U_{n-1}$ è invertibile per ipotesi del teorema
		la quale ci garantisce l'invertibilità fino all'ordine $n-2$ ma anche $A_{n-1}$ è invertibile e quindi,
		per il teorema di Binet vale che
		\[ \det(A_{n-1}) = \det(U_{n-1}) \]
		Possiamo quindi scrivere
		\[ x^T = w^T \cdot U_{n-1} \]
		Una volta ricavati $x^T$ e $y$ troviamo $u_{n,n}$ in modo univoco calcolando
		\[ u_{n,n} = a_{n,n} - x^T \cdot y \]
		Il sistema è quindi univocamente risolubile e quindi la fattorizzazione esiste ed è unica.
	\end{proof}
	Quando le ipotesi del teorema sono violate la fattorizzazione non esiste o non è univocamente determinata.
\end{theorem}

La dimostrazione del teorema appena enunciato ci dice che se siamo interessati a calcolare la fattorizzazione
$LU$ di una matrice di ordine $n$, è possibile farlo calcolando la fattorizzazione $LU$ della sua sottomatrice
di ordine $n-1$ per poi aggiornarla con tale costruzione.

Supponendo di avere la fattorizzazione di ordine $n-1$, il costo per trovare al fattorizzazione di ordine $n$
è dato dal costo per trovare la fattorizzazione di ordine $n-1$ sommato al costo dell'\emph{aggiornamento}.

Il costo dell'aggiornamento è pari al costo della risoluzione di due sistemi triangolari di ordine $n-1$ ossia
\[ c (n - 1)^2 \]
Il costo complessivo dell'algoritmo per la fattorizzazione è dunque
\[ F(n) = F(n-1) + c (n-1)^2 = c \cdot \sum_{i=1}^{n} (n - i)^2 \]
Il costo che ne ricaviamo è di $O(n^3)$ operazioni. Il problema di questo metodo è che, implementato in macchina,
riporta problemi di stabilità.

L'algoritmo può essere comunque interessanti in alcuni casi, ad esempio per alcune classi di matrici particolari.

\begin{example}
	Prendiamo la matrice $A \in \R^{n \times n}$ così definita
	\[
		A = \begin{pmatrix}
			1      & 0      & \dots  & 0      & x_1    \\
			0      & \ddots & \ddots & \vdots & \vdots \\
			\vdots & \ddots & \ddots & 0      & \vdots \\
			0      & \dots  & 0      & 1      & \vdots \\
			x_1    & \dots  & \dots  & \dots  & x_n
		\end{pmatrix}
	\]
	detta \textbf{matrice a freccia}. Il problema che ci poniamo è trovare per quali $x_i$ con $1 \leq i \leq n$
	la matrice ammette fattorizzazione $LU$ e in caso affermativo, trovare la fattorizzazione $LU$ per tali
	valori.

	\`E immediato notare che le sottomatrici principali di testa di $A$ sono tutte matrici identità e dunque vale
	\[ A_k = I_k \]
	quindi sono tutte invertibili. Per il teorema esiste ed è unica la sua fattorizzazione.

	Per trovare tale fattorizzazione dobbiamo trovare la fattorizzazione $LU$ della matrice di ordine $n-1$. Per
	farlo scriviamo
	\[
		A = \begin{pmatrix}
			L_{n-1} & 0 \\
			x^T     & 1
		\end{pmatrix} \cdot
		\begin{pmatrix}
			U_{n-1} & y       \\
			0       & u_{n,n}
		\end{pmatrix}
	\]
	Come abbiamo detto, abbiamo che
	\[ A_{n-1} = I_{n-1} \]
	L'identità di ordine $n-1$ si fattorizza come l'identità per se stessa. Possiamo quindi scrivere
	\[
		A = \begin{pmatrix}
			I_{n-1} & 0 \\
			x^T     & 1
		\end{pmatrix} \cdot
		\begin{pmatrix}
			I_{n-1} & y       \\
			0       & u_{n,n}
		\end{pmatrix}
	\]
	A questo punto ricaviamo che
	\begin{align*}
		y =       & \begin{pmatrix} x_1 \\ \vdots \\ x_{n-1} \end{pmatrix} \\
		x^T =     & \begin{pmatrix} x_1 & \dots & x_{n-1} \end{pmatrix}    \\
		u_{n,n} = & x_n - \sum_{i=1}^{n-1} x_i^2
	\end{align*}
	Quando $u_{n,n} \neq 0$ allora la matrice è invertibile, ossia quando
	\[ x_n - \sum_{i=1}^{n-1} x_i^2 \neq 0 \]
	Risolvere i sistemi lineari per $L$ e $U$ appena trovate con l'eliminazione gaussiana ha un costo di $O(n)$
	operazioni.
\end{example}

\begin{definition}
	La matrice $A$ si dice \textbf{tridiagonale} se, per $|i - j| > 1$, l'elemento $a_{i,j}$ è uguale a 0.
\end{definition}

\begin{definition}
	Sia $A$ una matrice di dimensione $n \times n$, $A$ si dice \textbf{predominante diagonale} se il valore
	assoluto di ogni elemento diagonale è maggiore alla somma dei valori assoluti degli elementi sulla stessa
	riga escluso l'elemento diagonale.
	\[ |a_{i,i}| > \sum_{j=1, j \neq i}^{n} |a_{i,j}| \quad i = 1 \dots n \]
	In questo caso la matrice si dice \emph{predominante diagonale per righe}. Se
	l'operazione viene fatta effettuando la somma degli elementi di ciascuna colonna si parlerà di
	\emph{predominanza diagonale per colonne}.
\end{definition}

\begin{proposition}
	Se una matrice è predominante diagonale allora è invertibile.
\end{proposition}

\begin{proposition}
	Se una matrice è predominante diagonale allora anche le sue sottomatrici principali di testa sono predominanti
	diagonali.
\end{proposition}

\begin{proposition}
	Se una matrice è predominante diagonale esiste ed è unica la sua fattorizzazione $LU$ in quanto le sue
	sottomatrici principali di testa sono anch'esse predominanti diagonali e quindi invertibili.
\end{proposition}

\begin{example}
	Prendiamo la seguente matrice \emph{tridiagonale} $A \in \R^{n \times n}$
	\[
		A = \begin{pmatrix}
			3      & -1     & 0      & \dots  & 0      \\
			-1     & \ddots & \ddots & \ddots & \vdots \\
			0      & \ddots & \ddots & \ddots & 0      \\
			\vdots & \ddots & \ddots & \ddots & -1     \\
			0      & \dots  & 0      & -1     & 3
		\end{pmatrix}
	\]
	Se volessimo applicare il teorema dobbiamo andare a verificare l'invertibilità di tutte le sottomatrici fino
	all'ordine $n-1$. Le sottomatrici sono tutte \emph{predominanti diagonali} in quanto, se $A$ è predominante
	diagonale anche le sue sottomatrici sono predominanti diagonali e dunque invertibili.
\end{example}
