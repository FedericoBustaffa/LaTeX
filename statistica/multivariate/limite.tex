\section{Teoremi limite}
I \textbf{teoremi limite} entrano in gioco quando si vuole, in un certo senso, passare al lato
più empirico della statistica, ecco che vengono introdotti la \textbf{legge dei grandi numeri} e
il \textbf{teorema del limite centrale}.

Prendiamo per esempio il lancio di una moneta equilibrata. Se si effettuano 1000 lanci ci
aspettiamo di ottenere all'incirca 500 teste se la moneta è equilibrata. La legge dei grandi numeri
cerca di formalizzare questo risultato, mentre il teorema del limite centrale cerca di quantificare
le oscillazioni del numero di teste attorno a 500. Iniziamo però a dare qualche definizione.

\begin{definition}
	Data una famiglia di variabili aleatorie $X_1, X_2, \dots, X_n$ (possibilmente infinita), le
	variabili $X_i$ si dicono indipendenti e identicamente distribuite (i.i.d) se sono indipendenti
	e hanno la stessa distribuzione (cioè $P_{X_i}$ non dipende da $i$).

	Nel caso di famiglia infinita, diciamo che le $X_i$ sono indipendenti se $\forall n \in \N$ le
	variabili $X_1, X_2, \dots, X_n$ sono indipendenti.
\end{definition}

Dire che la variabile $X_i$ sono i.i.d equivale a dire che le $X_i$ hanno la stessa funzione di
ripartizione
\[ P(X_i \leq t) = F_{X_i} (t) = F(t) \]
per ogni $i$ e inoltre significa che sono indipendenti
\[ P(X_1 \leq t_1, \dots, X_n \leq t_n) = F_{X_1} (t_1) \cdot ... \cdot F_{X_n} (t_n) \]
per ogni $t_1, \dots, t_n \in \R$. Il tipico esempio di variabili aleatorie i.i.d è dato dalle
ripetizioni di un esperimento (non perforza con esisto successo o insuccesso).

Consideriamo $n$ (o anche infinite) ripetizioni di un esperimento nelle stesse condizioni. Sia
$X$ la variabile aleatoria che descrive un carattere dell'esperimento (ad esempio l'esito del
lancio di un dado). Per $i \in \N^+$, sia $X_i$ il valore del carattere dell'esito dell'$i$-esimo
esperimento. Allora le $X_i$ sono i.i.d.

Come caso particolare consideriamo l'estrazione di un campione da una popolazione reale.
Consideriamo quindi una popolazione e sia $X$ la variabile aleatoria che rappresenta un carattere
degli individui. Supponiamo ora di estrarre casualmente $n$ individui, in modo indipendente l'uno
dall'altro, e chiamiamo $X_i$ il carattere dell'$i$-esimo individuo estratto. Allora gli $X_i$
sono i.i.d con la stessa distribuzione di $X$.

Introduciamo ora la notazione per la media aritmetica di $n$ variabili aleatorie $X_1, \dots, X_n$
i.i.d
\[ \overline{X} = \overline{X_n} = \frac{X_1 + X_2 + \dots + X_n}{n} \]
Osserviamo che $\overline{X}$ è essa stessa una variabile aleatoria in quanto combinazione di
variabili aleatorie. Osserviamo anche che se $X_1, \dots, X_n$ rappresenta un campione,
$\overline{X}$ è la media campionaria del campione.

\begin{example}
	Consideriamo il caso di $n$ ripetizioni di un esperimento con esito successo o insuccesso.
	Chiamiamo $A$ il successo e sia $p = P(A)$ e sia $X_i$ la variabile aleatoria di Bernoulli
	associata all'$i$-esima ripetizione
	\[
		X_i = \begin{cases}
			1 & \text{se accade $A$ all'$i$-esima ripetizione} \\
			0 & \text{altrimenti}
		\end{cases}
	\]
	Allora le $X_i$ sono i.i.d indipendenti e con distribuzione $B(p)$, quindi $X=X_1+\dots+X_n$ è
	una variabile aleatoria $B(n,p)$ e rappresenta la \textbf{frequenza assoluta} del successo
	(di $A$) mentre
	\[ \overline{X} = \frac{X}{n} \]
	rappresenta la frequenza relativa del successo.
\end{example}

\subsection{Legge dei grandi numeri}
Uno degli obbiettivi fondamentali della legge dei grandi numeri è studiare il valore di
$\overline{X_n}$ per campioni grandi ($n$ grande). Questo significa calcolare in qualche modo il
limite, ma trattandosi di variabili aleatorie dobbiamo definire meglio cosa si intenda per limite.
Per la legge dei grandi numeri, ci serve la \textbf{convergenza in probabilità}.

\begin{definition}
	Una successione $Y_1, \dots, Y_n, \dots$ di variabili aleatorie definite sullo stesso spazio
	diciamo che \textbf{converge in probabilità} a una variabile aleatoria $Y$ se
	\[ P(|Y_n - Y| > \varepsilon) \to 0 \]
	per ogni $\varepsilon > 0$. In alternativa possiamo vedere la cosa in questo modo
	\[ \lim_{n \to +\infty} P(|Y_n - Y| > \varepsilon) = 0 \]
	Stiamo quindi dicendo che per $n$ grande $Y_n$ è vicina a $Y$ con probabilità alta.
\end{definition}

\begin{theorem}[Legge debole dei grandi numeri]
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie i.i.d dotate di momento
	secondo e sia $\mu = \E[X_i]$, allora $\overline{X_n}$ converge in probabilità a $\mu$, cioè
	\[ P(|\overline{X_n} - \mu| > \varepsilon) \to 0 \]
	per $n \to +\infty$ e per ogni $\varepsilon > 0$.
	\begin{proof}
		L'idea alla base della dimostrazione è che si può dimostrare che
		\begin{align*}
			\E[\overline{X_n}] = & \E\left[ \frac{X_1 + \dots + X_n}{n} \right] \\
			=                    & \frac{1}{n} \sum_{i=1}^n \E[X_i] = \mu
		\end{align*}
		Vogliamo anche calcolare la varianza di $\overline{X}_n$
		\begin{align*}
			\Var(\overline{X_n}) = & \Var \left( \frac{1}{n} \cdot \sum_{i=1}^n X_i \right)   \\
			=                      & \frac{1}{n^2} \cdot \Var \left( \sum_{i=1}^n X_i \right) \\
			=                      & \frac{1}{n^2} \cdot \sum_{i=1}^n \Var(X_i)               \\
			=                      & \frac{1}{n^2} \cdot n \sigma^2 = \frac{\sigma^2}{n}
		\end{align*}
		Per la disuguaglianza di Chebyshev abbiamo che
		\[
			P(|\overline{X_n} - \E[\overline{X_n}]| >
			\varepsilon) \leq \frac{\Var(\overline{X_n})}{\varepsilon^2} =
			\frac{1}{\varepsilon^2} \cdot \frac{\sigma^2}{n} \to 0
		\]
		per $n \to \infty$.
	\end{proof}
\end{theorem}

In altre parole il teorema dice che la media campionaria dei primi $n$ termini tende, per $n$
grande, al valore atteso. Valore atteso che è la media sulla popolazione nel caso di estrazione.

\begin{example}
	Lanciamo un dado 1000 volte e vogliamo fornire un valore approssimato per il numero di 5 nei
	1000 lanci. In questo caso le $X_i \sim B(1/6)$ indipendenti e abbiamo che
	\[ \overline{X_n} = \frac{1}{n} \cdot \sum_{i=1}^{n} X_i \]
	è la frequenza relativa di 5. Per la LGN $\overline{X_n} \to 1/6$ e quindi se definiamo
	\[ Y_n = \sum_{i=1}^{n} X_i \]
	la frequenza assoluta di 5 abbiamo che che $Y_n \to n \cdot 1/6$. Per $n=1000$ abbiamo che
	\[ Y_{1000} \approx 1000 \cdot \frac{1}{6} \approx 167 \]
\end{example}

Introduciamo ora la \textbf{varianza campionaria} come la misura di quanto i campioni si
discostano dalla media e definita come
\[ S^2 = S_n^2 = \sum_{i=1}^n \left( X_i - \overline{X_n} \right)^2 \]
Se $(X_1, \dots, X_n)$ rappresenta un campione, allora $S^2$ è la varianza campionaria di tale
campione.

\begin{proposition}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie dotate di momento quarto
	finito e sia $\sigma^2$ la loro varianza, allora $S_n^2$ converge in probabilità a $\sigma^2$,
	cioè, per ogni $\varepsilon > 0$ vale
	\[ P(|S_n^2 - \sigma^2| > \varepsilon) \to 0 \]
	per $n \to \infty$.
\end{proposition}

\subsection{Teorema del limite centrale}
Come anticipato, questo strumento matematico ci fornisce una misura di quanto ci discostiamo dal
valore atteso della media.

Quello che ci dice il teorema del limite centrale ci dice che le fluttuazioni intorno alla media
hanno un comportamento, per $n$ grande, universale e gaussiano a prescindere dall'esperimento in
esame.

\begin{definition}
	Sia $Y_1, \dots, Y_n, \dots$ una successione di variabili aleatorie, $Y$ una variabile
	aleatoria e siano $F_n$ ed $F$ le funzioni di ripartizione rispettivamente di $Y_n$ e $Y$.
	Supponendo che $F$ sia continua diciamo che $(Y_n)_n$ \textbf{converge in legge} (o in
	distribuzione) a $Y$ se
	\[ \lim_{n \to \infty} F_n (t) = F(t) \]
	per ogni $t \in \R$. Notiamo che questa condizione di convergenza dipende solo dalle leggi di
	$Y_n$ e di $Y$.
\end{definition}

\begin{theorem}[Teorema del limite centrale]\label{th: tlc}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie i.i.d dotate di momento
	secondo finito, allora $\mu = \E[X_i]$ e $\sigma^2 = \Var(X_i) < \infty$, allora
	\[ \left( \sqrt{n} \cdot \frac{\overline{X_n} - \mu}{\sigma} \right)_n \]
	converge in legge a $Z \sim N(0,1)$. Questo vuol dire che per ogni $a, b$ tali che
	$-\infty \leq a < b \leq +\infty$
	\begin{align*}
		\lim_{n \to \infty} P\left( a \leq \sqrt{n} \frac{\overline{X_n} -
		\mu}{\sigma} \leq b \right) = & P(a \leq Z \leq b)                                      \\
		=                             & \int_{a}^{b} \frac{1}{\sqrt{2 \pi}} \cdot e^{-x^2/2} dx \\
		=                             & \Phi(b) - \Phi(a)
	\end{align*}
\end{theorem}

\begin{observation}
	Osserviamo che
	\[
		\sqrt{n} \cdot \frac{\overline{X_n} - \mu}{\sigma} =
		\frac{X_1 + \dots + X_n - n \cdot \mu}{\sigma \cdot \sqrt{n}}
	\]
	Vale anche che
	\[
		\E \left[ \frac{\overline{X_n} - \mu}{\sigma / \sqrt{n}} \right] = 0 \quad
		\Var \left( \frac{\overline{X_n} - \mu}{\sigma / \sqrt{n}} \right) = 1
	\]
\end{observation}

\begin{observation}
	La legge di $X_i$ può essere qualunque a patto che abbia momento secondo.
\end{observation}

\begin{example}
	Consideriamo $n$ ripetizioni di un esperimento di Bernoulli. Abbiamo che
	\[ Y_n = Y_1 + \dots + Y_n \]
	è la frequenza assoluta del successo nelle prime $n$ ripetizioni ed è anche una binomiale
	$B(n,p)$. Il teorema del limite centrale ci dice che
	\[ \frac{Y_n - n \cdot p}{\sqrt{n \cdot p \cdot (1-p)}} \]
	è approssimativamente una $N(0,1)$. Questo risultato prende il nome di
	\textbf{approssimazione della binomiale} per $n$ grande.
\end{example}

\begin{example}
	Consideriamo 1000 lanci di moneta e ci chiediamo
	\begin{itemize}
		\item Qual è la probabilità di avere almeno 480 teste.
		\item Quanto deve valere $k$ tale che, con probabilità del 95\%, escano almeno $k$ teste su
		      1000.
	\end{itemize}
	Sia $X$ il numero di teste su 1000 lanci $\sim B(1000, 1/2)$. Per poter applicare il TLC
	dobbiamo verificare che
	\[ n \cdot p \cdot (1-p) = 1000 \cdot \frac{1}{2} \cdot \frac{1}{2} = 250 \geq 20 \]
	Quindi
	\begin{align*}
		P(X \geq 480) = & P \left( \frac{X - 1000 \cdot \frac{1}{2}}
		{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}} \geq
		\frac{480 - 1000 \cdot \frac{1}{2}}{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}} \right)
	\end{align*}
	A questo punto, calcolando il membro a destra della seconda disequazione otteniamo
	\[
		\frac{480 - 1000 \cdot \frac{1}{2}}{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}}
		\approx -1.26
	\]
	Se adesso introduciamo $Z \sim N(0,1)$, possiamo dire, per il TLC, che
	\begin{align*}
		P \left( \frac{X - 1000 \cdot \frac{1}{2}}
		{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}}
		\geq -1.26 \right) \approx & P(Z \geq -1.26)          \\
		=                          & 1 - P(Z < -1.26)         \\
		=                          & 1 - \Phi (-1.26)         \\
		=                          & \Phi(1.26) \approx 0.896
	\end{align*}
	Vogliamo ora trovare $k$ tale che $P(X \geq k) = 0.95$ che equivale a
	\begin{align*}
		P(X \geq k) = & P \left( \frac{X - 1000 \cdot \frac{1}{2}}
		{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}} \geq \frac{k - 1000 \cdot \frac{1}{2}}
		{\sqrt{1000 \cdot \frac{1}{2} \cdot \frac{1}{2}}} \right)  \\
		=             & P \left( \frac{X - 500}{\sqrt{250}} \geq
		\frac{k - 500}{\sqrt{250}} \right)
	\end{align*}
	Sempre per il TLC abbiamo che
	\[
		P \left( \frac{X - 500}{\sqrt{250}} \geq
		\frac{k - 500}{\sqrt{250}} \right) \approx
		P\left( Z \geq \frac{k - 500}{\sqrt{250}} \right) = 0.95
	\]
	Come possiamo vedere il problema è quello di trovare un quantile, ossia un valore per
	\[ \frac{k - 500}{\sqrt{250}} \]
	tale che
	\[ P \left( Z < \frac{k - 500}{\sqrt{250}} \right) = 0.05 \]
	Scriviamo quindi che
	\begin{gather*}
		1 - \Phi \left( \frac{k - 500}{\sqrt{250}} \right) = 0.95 \\
		\Leftrightarrow \\
		\Phi \left( \frac{k - 500}{\sqrt{250}} \right) = 0.05 \\
		\Leftrightarrow \\
		1 - \Phi \left( \frac{500 - k}{\sqrt{250}} \right) = 0.05 \\
		\Leftrightarrow \\
		\Phi \left( \frac{500 - k}{\sqrt{250}} \right) = 0.95
	\end{gather*}
	A questo punto, facendo ricorso alle tavole, si ottiene
	\begin{gather*}
		\frac{500 - k}{\sqrt{250}} \approx 1.64 \\
		\Leftrightarrow \\
		k \approx 473.91
	\end{gather*}
	Questo ci dice che con probabilità del 95\% otteniamo almeno 473 teste.
\end{example}

\begin{proposition}
	Sia $X_1, \dots, X_n, \dots$ una successione di variabili aleatorie i.i.d con momento secondo
	finito, $\mu = \E[X_i]$ e $\sigma^2 = \Var(X_i)$ supponendo $\sigma^2 > 0$, allora
	\[ \sqrt{n} \cdot \frac{\overline{X_n}  - \mu}{S_n} \]
	tende in legge a $Z \sim N(0,1)$ per $n \to \infty$. Possiamo quindi dire che per ogni $a,b$
	tali che $-\infty < a < b < +\infty$ vale
	\[
		P \left( a \leq \sqrt{n} \cdot \frac{\overline{X_n}  - \mu}{S_n} \leq b \right) \to
		P(a \leq Z \leq b)
	\]
	dove
	\[ S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X_n})^2 \]
\end{proposition}