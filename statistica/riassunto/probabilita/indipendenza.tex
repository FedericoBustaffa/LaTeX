\section{Indipendenza}
Il concetto di \textbf{indipendenza} nasce dalla necessità di esprimere in modo rigoroso che la
probabilità di un evento $A$ non cambia sapendo che accade $B$ e viceversa. Per $A$ e $B$ non
trascurabili vale quindi
\[ P(A) = P(A | B) \quad \Leftrightarrow \quad P(A) \cdot P(B) = P(A \cap B) \]
Lo stesso vale per $P(B)$.

\begin{definition}
	Due eventi $A$ e $B$ sono \textbf{indipendenti} se
	\[ P(A \cap B) = P(A) \cdot P(B) \]
\end{definition}

Si può dimostrare per esercizio che, se $A$ e $B$ sono indipendenti allora lo sono anche
\begin{itemize}
	\item $A^c$ e $B$
	\item $A$ e $B^c$
	\item $A^c$ e $B^c$
\end{itemize}
L'indipendenza è dunque stabile per la complementazione. Si può inoltre dimostrare che
\begin{itemize}
	\item Se $P(A) \in \{ 0, 1 \}$ allora $A$ è indipendente da ogni altro evento.
	\item Se $A \cap B = \emptyset$ allora $A$ e $B$ non sono indipendenti, a meno che $P(A)$ o
	      $P(B)$ siano 0.
\end{itemize}

\begin{example}
	Si vuole estrarre una carta da un mazzo di 40 carte napoletane
	\[ \Omega = \{ \text{tutte le carte} \} \]
	e ci chiediamo se i due eventi
	\[ A = \{ \text{asso} \} \quad B = \{ \text{denari} \} \]
	sono indipendenti. La probabilità di estrarre un asso equivale a
	\[ P(A) = \frac{4}{40} = \frac{1}{10} \]
	La probabilità di estrarre una carta di denari è invece
	\[ P(B) = \frac{10}{40} = \frac{1}{4} \]
	La probabilità di estrarre l'asso di denari equivale a
	\[ P(A \cap B) = \frac{1}{40} = \frac{1}{10} \cdot \frac{1}{4} = P(A) P(B) \]
	Quindi gli eventi sono indipendenti.
\end{example}

\subsection{Indipendenza per 3 o più eventi}
Siano $A$, $B$ e $C$ degli eventi. Per riuscire a dire che sono indipendenti c'è bisogno che
\begin{itemize}
	\item Siano a due a due indipendenti
	      \begin{align*}
		      P(A \cap B) = & P(A) \cdot P(B) \\
		      P(A \cap C) = & P(A) \cdot P(C) \\
		      P(B \cap C) = & P(B) \cdot P(C)
	      \end{align*}
	\item La probabilità dell'intersezione di tutti e tre si spezzi come il prodotto delle
	      probabilità dei tre eventi.
	      \[ P(A \cap B \cap C) = P(A) \cdot P(B) \cdot P(C) \]
\end{itemize}

\begin{example}
	Consideriamo lo spazio campionario $\Omega=\{ 1,2,3,4 \}$ e i relativi sottoinsiemi
	\[ A=\{ 1, 2 \} \quad B=\{ 1, 3 \} \quad C=\{ 2, 3 \} \]
	con $P$ uniforme su $\Omega$. La probabilità che i singoli eventi si verifichino è di
	\[ P(A) = P(B) = P(C) = \frac{2}{4} = \frac{1}{2} \]
	La probabilità dell'intersezione degli eventi presi a due a due è di
	\[ P(A \cap B) = P(A \cap C) = P(B \cap C) = \frac{1}{4} \]
	Dunque la prima condizione per l'indipendenza è soddisfatta. Andiamo a calcolare ora quanto
	vale l'intersezione dei tre eventi. Consideriamo l'intersezione in questo modo
	\[ A \cap (B \cap C) \]
	Dunque possiamo calcolare la probabilità dell'intersezione in questo modo
	\[ P(A \cap (B \cap C)) = P(A | (B \cap C)) = 0 \neq P(A) \]
	Dunque sapere che accade $B \cap C$ cambia la probabilità che accada $A$.
\end{example}

\begin{definition}
	Dati $A_1, A_2, \dots, A_n$ eventi, questi si dicono \textbf{indipendenti} se $\forall k$
	intero con $1 \leq k \leq n$ e $\forall \; 1 \leq i_1 < \dots < i_k \leq n$ vale
	\[ P(A_{i_1} \cap \dots \cap A_{i_k}) = P(A_{i_1}) \cdot \dots \cdot P(A_{i_k}) \]
\end{definition}

In altre parole la probabilità dell'intersezione degli eventi si deve poter spezzare come
prodotto delle probabilità dei singoli eventi.

\begin{observation}
	Se $A_1, A_2, \dots, A_n$ sono indipendenti allora sono indipendenti a due a due.
\end{observation}

\subsubsection{Prove ripetute}
Un caso importante di eventi indipendenti è il caso delle \textbf{prove ripetute} in cui si ripete
un esperimento $n$ volte nelle medesime condizioni. Possiamo dire che per gli eventi riferiti a
ripetizioni distinte o a gruppi disgiunti di ripetizione sono indipendenti.

Un sottocaso importante delle prove ripetute è lo schema di Bernoulli per $n$ prove ripetute in
cui ciascuna prova ha esito \emph{successo} o \emph{insuccesso}. In questo specifico caso è
possibile scrivere lo spazio campionario
\[ \Omega = \{ a_1, a_2, \dots, a_n | \forall i \; a_i \in \{ 0, 1 \} \} = \{ 0, 1 \}^n \]
In cui associamo tipicamente a 1 il successo e a 0 l'insuccesso. La probabilità associata ad una
sequenza equivale a
\[ P(\{ a_1, \dots, a_n \}) = p^{\# \{i | a_i = 1\}} \cdot (1 - p)^{\# \{i | a_i = 0\}}  \]
dove $p$ è la probabilità di successo della singola prova.

\begin{observation}
	Il concetto di indipendenza non è legato al concetto di causalità.
\end{observation}
