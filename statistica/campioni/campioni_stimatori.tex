\part{Statistica inferenziale}

\chapter{Campioni di variabili aleatorie}
Lo scopo dell’\textbf{inferenza statistica} è l'analisi di un campione per ricavare informazioni
su un carattere di una intera popolazione. Ad esempio, un sondaggio sulle intenzioni di voto
raccoglie le risposte di un certo numero di intervistati per ricostruire gli orientamenti
elettorali dell’intera popolazione, in questo caso l’intero corpo elettorale.

L’assunzione di base della statistica inferenziale è che si possa descrivere la misura del carattere
desiderato su un individuo scelto casualmente con una variabile aleatoria, di cui vogliamo
determiare la legge, o informazioni su di essa.

\section{Campioni statistici e stimatori}
Il carattere che vogliamo studiare viene rappresentato con una variabile aleatoria
$X$, la cui distribuzione $P_X$ è secondo i casi parzialmente o del tutto sconosciuta. In altre
parole ripetiamo l'esperimento aleatorio $n$ volte e, per ciascuna ripetizione, misuriamo il
carattere in esame. In altre parole assumiamo di avere a disposizione $n$ variabili aleatorie
$X_1, \dots, X_n$, indipendenti e aventi la stessa legge di X, che rappresentano il
\textbf{campione} estratto dalla popolazione.

\begin{definition}\label{def: campione statistico}
	Una famiglia finita $X_1, \dots, X_n$ di variabili aleatorie i.i.d si chiama
	\textbf{campione statistico} di \textbf{taglia} $n$ che ha come legge, la legge $P_X$ di $X$.
\end{definition}

\begin{observation}
	Le $X_i$ sono variabili aleatorie che esistono solo nel modello matematico. Con la notazione
	$x_i$ indichiamo invece l'\textbf{esito} dell'$i$-esima misurazione.
	\[ x_i = X_i(\omega) \]
	per un certo $\omega$ estratto dalla popolazione.
\end{observation}

Sia $(X_1, \dots, X_n)$ un campione i.i.d di $X$ di legge non nota $P_X$. Supponiamo che la
distribuzione $P_X$ è parzialmente specificata. Appartiene cioè ad una famiglia di probabilità
dipendenti da un parametro $\theta$ o da più parametri $\theta_1, \dots, \theta_n$, non noti.

\begin{example}
	L'altezza di una popolazione ha una distribuzione gaussiana di cui non conosciamo né media né
	varianza (i parametri sono media e varianza).
\end{example}

\label{def: statistica campionaria}
L'obbiettivo della stima parametrica è ricostruire il parametro o i parametri incogniti, a partire
dalle osservazioni, cioè in funzione del campione. Una funzione $g(X_1, \dots, X_n)$ di un campione
statistico è chiamata \textbf{statistica campionaria} (per esempio la media campionaria).

\label{def: stimatore}
Uno \textbf{stimatore} di un parametro $\theta$ della distribuzione è una statistica campionaria,
cioè una funzione $g(X_1, \dots, X_n)$ del campione, atta a stimare $\theta$.

\begin{example}
	La media campionaria $\overline{X_n}$ è lo stimatore abituale del valore atteso $\E[X]$. La
	varianza campionaria $S_n^2$ è lo stimatore abituale della varianza $\Var(X)$.
\end{example}

\begin{observation}
	Poiché uno stimatore è una funzione del campione, anch'esso è una variabile aleatoria. In
	particolare, realizzazione diverse del campione $X_1, \dots, X_n$, portano a valori diversi
	dello stimatore.
\end{observation}

Si pone quindi il problema di come scegliere un buon stimatore e di come valutarne la \emph{bontà}.
Per fare questa valutazione vedremo tre criteri.

\begin{definition}\label{def: stimatore corretto}
	Dato $\theta$ parametro della distribuzione e dato $X_1, \dots, X_n$ un campione della
	distribuzione, una statistica $g(X_1, \dots, X_n)$ si dice \textbf{stimatore corretto} o
	\textbf{non distorto} del parametro $\theta$ se
	\[ \E[g(X_1, \dots, X_n)] = \theta \]
	cioè la media dello stimatore è proprio il parametro $\theta$.
\end{definition}

\begin{proposition}
	Media campionaria e varianza campionaria sono stimatori corretti, rispettivamente di valore
	atteso e varianza. Se $X_1, \dots, X_n$ è un campione i.i.d con momento secondo finito, allora
	\[ \E[X_n] = \E[X_1] \quad \quad \E[S_n^2] = \Var(X_1) \]
\end{proposition}

\begin{definition}\label{def: stimatore consistente}
	Dato un parametro $\theta$ della distribuzione e un campione $X_1, \dots, X_n, \dots$ di
	infinite variabili aleatorie i.i.d, la successione di statistiche $g_n(X_1, \dots, X_n)$ si
	dice uno stimatore \textbf{consistente} di $\theta$ se, per $n \to +\infty$, allora
	$g_n(X_1, \dots, X_n)$ tende a $\theta$ in probabilità, cioè
	\[ \lim_{n \to +\infty} P(|g_n(X_1, \dots, X_n) - \theta| > \varepsilon) = 0 \]
	per ogni $\varepsilon > 0$. Cioè, quando la taglia del campione diventa molto grande,
	$g_n(X_1, \dots, X_n)$ si avvicina con alta probabilità al valore $\theta$.
\end{definition}

La media campionaria e la varianza campionaria sono stimatori consistenti rispettivamente di valore
atteso e varianza. La media campionaria lo è per la legge dei grandi numeri, la varianza lo è per
un suo corollario.

\begin{definition}\label{def: stimatore efficiente}
	Dato un parametro $\theta$ e un campione $X_1, \dots, X_n$ e dati due stimatori corretti
	$g(X_1, \dots, X_n)$ e $h(X_1, \dots, X_m)$, diciamo che $g(X_1, \dots, X_n)$ è più
	\textbf{efficiente} di $h(X_1, \dots, X_m)$ se
	\[ \Var(g(X_1, \dots, X_n)) \leq \Var(h(X_1, \dots, X_m)) \]
\end{definition}

Vogliamo cioè che la dispersione di $g(X_1, \dots, X_n)$ attorno a
$\E[g(X_1, \dots, X_n)] = \theta$ sia minore della dispersione di $h(X_1, \dots, X_m)$ attorno a
$\E[h(X_1, \dots, X_m)] = \theta$.