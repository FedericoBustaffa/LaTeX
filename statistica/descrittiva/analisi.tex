\chapter{Analisi di dati numerici}
Procediamo con alcune definizioni fondamentali per l'analisi statistica dei dati.

\begin{definition} \label{vettore}
	Definiamo \textbf{vettore di dati}, un insieme di valori, potenzialmente tutti diversi.
	\[ x = (x_1, x_2, \dots, x_n) \]
\end{definition}

\begin{definition}
	Definiamo la \textbf{media empirica} o il \textbf{valore medio} come la media aritmetica dei valori in un certo
	vettore di dati.
	\[ \overline{x} = \frac{x_1 + x_2 + \dots + x_n}{n} \]
\end{definition}

\section{Varianza}
La \textbf{varianza} è il valore che definisce la \emph{variabilità} di un campione o meglio è una misura di
\emph{dispersione} dei dati. Andremo a vedere due tipi di varianza: campionaria ed empirica.

\begin{definition}
	Sia $x$ un vettore di dati, definiamo la \textbf{varianza campionaria} come
	\[ \Var(x) = \sum_{i=1}^n \frac{(x_i - \overline{x})^2}{n - 1} \]
	La varianza campionaria si applica meglio su campioni di dati.
\end{definition}

\begin{definition}
	Sia $x$ un vettore di dati, definiamo la \textbf{varianza empirica} come
	\[ \Var_e(x) = \sum_{i=1}^n \frac{(x_i - \overline{x})^2}{n} \]
	La varianza empirica si applica meglio su intere popolazioni di dati.
\end{definition}

\begin{observation}
	Se andiamo ad analizzare entrambe le formule possiamo facilmente accorgerci che la varianza, essendo una somma di
	quadrati, ha valore 0, quando questi sono tutti 0. Perché questo avvenga, tutti i valori in $x$ devono essere
	uguali fra di loro.

	Ecco che possiamo dire che un valore basso di varianza corrisponde ad un vettore di dati con valori molto simili
	fra loro. Al contrario, un alto valore di varianza corrisponde ad un vettore di dati con valori molto diversi fra
	loro.

	Presi due vettori di dati differenti, uno con valori molto simili fra loro e uno con valori molto diversi. La
	media potrebbe essere molto simile se non uguale ma la varianza per i due vettori sarà differente.
\end{observation}

\begin{definition}
	Definiamo anche un'uguaglianza di base che, per il momento non andremo a spiegare, ma che in seguito ci tornerà
	utile:
	\[ \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n x_i^2 - n \overline{x}^2 \]
	\begin{proof}
		Sviluppando il quadrato possiamo facilmente convincerci che l'uguaglianza sia vera:
		\[
			\begin{array}{ l l l l }
				  & \displaystyle\sum_{i=1}^n (x_i - \overline{x})^2                                     & = &
				\displaystyle\sum_{i=1}^n (x_i^2 - 2x_i \overline{x} + \overline{x}^2)                         \\
				\\
				= & \displaystyle\sum_{i=1}^n x_i^2 - 2 \overline{x} \sum_{i=1}^n x_i + n \overline{x}^2 &
				= & \displaystyle\sum_{i=1}^n x_i^2 - 2 n \overline{x}^2 + n \overline{x}^2                    \\
				\\
				= & \displaystyle\sum_{i=1}^n x_i^2 - n \overline{x}^2
			\end{array}
		\]
	\end{proof}
\end{definition}

\begin{definition}
	Definiamo lo \textbf{scarto quadratico medio} come la radice quadrata della varianza campionaria:
	\[ \sigma (x) = \sqrt{\Var (x)} \]
\end{definition}

\begin{definition}
	Definiamo la \textbf{deviazione standard} come la radice quadrata della varianza empirica:
	\[ \sigma_e (x) = \sqrt{\Var_e (x)} \]
\end{definition}

Come già detto in precendenza, la varianza è nulla se e solo se tutti i valori nel vettore $x$ sono uguali tra di
loro, vale quindi
\[ \Var(x) = 0 \Leftrightarrow x_1 = x_2 = \dots = x_n \]
Abbiamo anche già specificato che il valore della varianza indica il grado di \emph{dispersione} dei dati, per
fissare meglio il concetto analizziamo questa espressione:
\[ \# \{ x_i : |x_i - \overline{x}| > d  \} \leq \frac{\sum_{i=1}^n (x_i - \overline{x})}{d^2} \]
Da qui possiamo dedurre che
\[ \frac{\# \{ x_i : |x_i - \overline{x}| > d \}}{n} \leq \frac{\Var_e(x)}{d^2} \]
La disequazione appena descritta è detta \textbf{disuguaglianza di Chebyshev}, anche se, come vedremo più avanti,
si tratta di un caso particolare della vera disuguaglianza di Chebyshev. Essa indica la percentuale di dati che
differiscono da $\overline{x}$ per almeno un fattore $d$.

\begin{observation}
	Questo ci dice che più la varianza è piccola, più diminuisce la percentuale di dati che differiscono dalla
	media per almeno un fattore $d$.
\end{observation}

\section{Concentrazione dei dati}
\begin{definition}
	Definiamo ora una \textbf{regola empirica sulla concentrazione dei dati}: supponendo che l'istogramma sia
	normale abbiamo che
	\begin{itemize}
		\item Circa il 68\% dei dati appartiene a
		      \[ [ \overline{x} - \sigma(x),\; \overline{x} + \sigma(x) ] \]
		\item Circa il 95\% dei dati appartiene a
		      \[ [ \overline{x} - 2 \sigma(x),\; \overline{x} + 2 \sigma(x) ] \]
		\item Circa il 99.7\% dei dati appartiene a
		      \[ [ \overline{x} - 3 \sigma(x),\; \overline{x} + 3 \sigma(x) ] \]
	\end{itemize}
	Questi intervalli si allargano tanto più velocemente quanto più grande è lo scarto quadratico medio (o la
	varianza).
\end{definition}

\begin{definition}
	Definiamo la \textbf{funzione di ripartizione empirica} come
	\[ F_e(t) = \frac{\{ x_i : x_i \leq t \}}{n} \]
	Quello che la funzione fa in pratica è prendere la percentuale dei dati che vengono prima di $t$.
\end{definition}

Supponendo di avere un certo insieme di dati, il procedimento per calcolare la funzione appena definita è il
seguente:
\begin{enumerate}
	\item Disporre i dati in ordine crescente:
	\item Si parte da 0 e si compie un \emph{salto} di ampiezza $1 / n$ sull'asse $y$ in corrispondenza di uno
	      dei dati. Nel caso in cui ci siano $m$ dati coincidenti, il salto in quel punto sarà di $m / n$.
\end{enumerate}

\begin{example}
	Supponiamo di avere il seguente insieme di $n = 5$ dati:
	\[ x = (1.2, \; -0.7, \; 3.4, \; 1.6, \; 2.1) \]
	Ordiniamoli in ordine crescente:
	\[ x = (-0.7, \; 1.2, \; 1.6, \; 2.1, \; 3.4) \]
	Dato che $n = 5$ abbiamo un salto verticale di $1/5 = 0.2$ in corrispondenza di ognuno dei dati.
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
					axis lines = center,
					width = 12cm,
					height = 6cm,
					font = \small,
					ymin=-0.2, ymax=1.2,
					xmin=-1.3, xmax=4,
					xtick={-0.7, 1.2, 1.6, 2.1, 3.4},
					ytick=\empty
				]
				\addplot [thick, red, domain={-2 : -0.7}] {0};
				\draw [thick, red] {(-0.7, 0) -- (-0.7, 0.2)};
				\addplot [thick, red, domain={-0.7 : 1.2}] {0.2};
				\draw [thick, red] {(1.2, 0.2) -- (1.2, 0.4)};
				\addplot [thick, red, domain={1.2 : 1.6}] {0.4};
				\draw [thick, red] {(1.6, 0.4) -- (1.6, 0.6)};
				\addplot [thick, red, domain={1.6 : 2.1}] {0.6};
				\draw [thick, red] {(2.1, 0.6) -- (2.1, 0.8)};
				\addplot [thick, red, domain={2.1 : 3.4}] {0.8};
				\draw [thick, red] {(3.4, 0.8) -- (3.4, 1)};
				\addplot [thick, red, domain={3.4 : 4}] {1};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	Prendiamo per esempio $t = 1.8$, otteniamo che
	\[ F_e(1.8) = 3/5 = 0.6 \]
	ossia il 60\% dei dati.
\end{example}

\subsection{Percentili e quantili}
Definiamo ora \textbf{percentili} e \textbf{quantili}, i primi usati più nel linguaggio comune, gli altri usati
più spesso nel linguaggio statistico.

\begin{definition}
	Sia $k$ un numero (non necessariamente intero) compreso tra 0 e 100, diciamo che l'intero $t$ è il $k$-esimo
	\textbf{percentile}, se
	\begin{itemize}
		\item Almeno $k / 100$ dati sono inferiori o uguali di $t$.
		\item Almeno $1 - (k / 100)$ dati sono superiori o uguali a $t$.
	\end{itemize}
	Intuitivamente si tratta del più piccolo numero che supera il $k \%$ dei dati.
\end{definition}

\begin{definition}
	Sia $k$ un numero compreso tra 0 e 100, diciamo che l'intero $\beta = k / 100$ è un
	$\beta$-\textbf{quantile}.
\end{definition}

\begin{definition}
	Definiamo anche i \textbf{quartili} ($i/4$-quantili) come segue:
	\begin{itemize}
		\item Il \textbf{primo quartile} equivale allo $0.25$-quantile.
		\item La \textbf{mediana} o \textbf{secondo quartile} equivale allo $0.50$-quantile.
		\item Il \textbf{terzo quartile} equivale allo $0.75$-quantile.
	\end{itemize}
\end{definition}

\section{Dati multipli}
Fino ad ora abbiamo sempre preso in considerazione vettori di dati monodimensionali, ma supponiamo di dover
lavorare con vettori contenenti $n$-uple di valori, per esempio
\[ (x, y) = ((x_1, \; y_1), \; (x_2, \; y_2), \; \dots, \; (x_n, \; y_n)) \]
Ecco che dobbiamo introdurre degli strumenti necessari a lavorare anche con questi vettori.

\subsection{Covarianza}
La \textbf{covarianza} è la misura di \emph{quanto varia} un insieme.

\begin{definition}
	Siano $x$  e $y$ due vettori di dati, definiamo la \textbf{covarianza campionaria} come
	\[ \Cov(x, y) = \frac{1}{n - 1} \sum_{i=1}^n (x_i - \overline{x}) (y_i - \overline{y}) \]
\end{definition}

\begin{definition}
	Siano $x$  e $y$ due vettori di dati, definiamo la \textbf{covarianza empirica} come
	\[
		\Cov_e(x, y) = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x}) (y_i - \overline{y}) =
		\frac{1}{n} \left( \sum_{i=1}^n x_i \cdot y_i \right) - \overline{x} \cdot \overline{y}
	\]
\end{definition}

\begin{definition}
	Supponiamo $\sigma (x) \neq 0$ e $\sigma (y) \neq 0$, allora chiamiamo \textbf{coefficiente di correlazione}
	tra $x$ e $y$ il numero
	\[
		r(x, y) = \frac{\Cov (x, y)}{\sigma (x) \cdot \sigma (y)} =
		\frac{\displaystyle\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}
		{\sqrt{\displaystyle\sum_{i=1}^n (x_i - \overline{x})^2} \cdot
			\sqrt{\displaystyle\sum_{i=1}^n (y_i - \overline{y})^2}}
	\]
	Sia che si scelga la covarianza campionaria sia che si scelga quella empirica il risultato non cambia.
\end{definition}

\begin{definition}
	La \textbf{disuguaglianza di Schwartz} è definita come segue
	\[
		\sum_{i=1}^n |(x_i - \overline{x}) (y_i - \overline{y})| \leq
		\sqrt{\sum_{i=1}^n (x_i - \overline{x})^2} \cdot \sqrt{\sum_{i=1}^n (y_i - \overline{y})^2}
	\]
	e di conseguenza vale \[ |r(x, y)| \leq 1 \]
\end{definition}

Intuitivamente il \emph{coefficiente di correlazione} misura il \emph{legame lineare} che c'è tra i dati $x$ e $y$
ma per comprendere meglio il concetto dobbiamo introdurre la \textbf{retta di regressione}.

\subsection{Rette di regressione}
Al fine di calcolare una \emph{retta di regressione} dobbiamo calcolare questo termine
\[ \inf_{a,b \in \mathbb{R}^2} \sum_{i=1}^n (y_i - a - b x_i)^2 \]
vogliamo cioè approssimare nel modo migliore i dati $y_i$ con una combinazione lineare affine $a + b x_i$.

\begin{theorem}
	Il minimo al variare di $(a, b) \in \mathbb{R}^2$ della quantità
	\[ \sum_{i=1}^n (y_i - a - b x_i)^2 \]
	si ottiene con
	\[ b^* = \frac{\Cov(x, y)}{\Var(x)} \quad \wedge \quad a^* = -b^* \overline{x} + \overline{y} \]
	e vale l'uguaglianza
	\[
		\min_{a, b \in \mathbb{R}^2} \sum_{i=1}^n (y_i - a - b x_i)^2 =
		\sum_{i=1}^n (y_i - \overline{y})^2 (1 - r(x, y)^2)
	\]
	La retta $y = a^* + b^* x$ è chiamata \textbf{retta di regressione}.
	\begin{proof}
		Quel che vogliamo fare è minimizzare la funzione
		\[ Q(a, b) = \sum_{i=1}^n (y_i - a - b x_i)^2 \]
		al variare di $a$ e $b$. Come è possibile notare dal calcolo del limite
		\[ \lim_{|a|, |b| \to +\infty} Q(a, b) = +\infty \]
		si tratta di una funzione a valori positivi che tende a $+\infty$ quando $a$ e $b$
		tendono a $+\infty$.

		Per trovare il minimo di una funzione a due variabili si annullano le derivate parziali rispetto a
		$a$ e $b$
		\[ \frac{\partial Q}{\partial a} = 0 \quad \frac{\partial Q}{\partial b} = 0 \]
		Dobbiamo quindi andare a risolvere un sistema a due equazioni di questo tipo
		\[
			\begin{cases}
				\sum_{i=1}^n y_i - n a - b \sum_{i=1}^n x_i                      & = 0 \\
				\sum_{i=1}^n x_i y_i - a \sum_{i=1}^n x_i - b \sum_{i=1}^n x_i^2 & = 0
			\end{cases}
		\]
		Dividendo per $n$ entrambe le equazioni si arriva a
		\[
			\begin{cases}
				a + b \overline{x}                              & = \overline{y}                   \\
				a \overline{x} + b \sum_{i=1}^n \frac{x_i^2}{n} & = \sum_{i=1}^n \frac{x_i y_i}{n}
			\end{cases}
		\]
		Risolvendo il sistema si ricavano $a^*$ e $b^*$ come segue
		\[
			\begin{cases}
				b^* = \displaystyle\frac{\sum_i \frac{x_i y_i}{n} - \overline{x} \cdot \overline{y}}
				{\sum_i \frac{x_i^2}{n} - \overline{x}^2} =
				\frac{\Cov_e(x, y)}{\Var_e(x)}         \\
				a^* = -b^* \overline{x} + \overline{y} \\
			\end{cases}
		\]
		Se si calcola $Q(a^*, b^*)$ si ricava che
		\[ Q(a^*, b^*) = \sum_{i=1}^n (y_i - \overline{y})^2 (1 - r(x, y)^2) \]
	\end{proof}
\end{theorem}

\begin{observation}
	Il \textbf{valore assoluto} del coefficiente di correlazione ci fornisce quindi delle informazioni
	interessanti sui dati:
	\begin{itemize}
		\item $|r(x, y)| = 1$ se e solo se i dati sono tutti sulla stessa retta.
		\item $|r(x, y)| \approx 1$ se i dati sono molto allineati.
		\item $|r(x, y)| \approx 0$ se i dati sono dispersi.
	\end{itemize}
	Possiamo anche notare che il \textbf{segno} del coefficiente di correlazione ci fornisce un'altra
	informazione interessante:
	\begin{itemize}
		\item Se $r(x, y) < 0$ allora la retta decresce al crescere di $x$.
		\item Se $r(x, y) > 0$ allora la retta cresce al crescere di $x$.
	\end{itemize}
\end{observation}

\begin{example}
	Proviamo ora a calcolare la retta di regressione su un insieme di dati contenuto. Supponiamo di avere 5
	maschi e 5 femmine della stessa età e, per ognuno di essi, sappiamo altezza e peso.
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
					title={Rette di regressione},
					xmin=145, xmax=195,
					ymin=35, ymax=100,
					xlabel={Altezza},
					ylabel={Peso},
					width=9cm,
					grid=both,
					grid style=dashed,
					legend pos=north west
				]
				\addplot [blue, only marks] coordinates{
						(170, 60)
						(175, 70)
						(180, 78)
						(185, 87)
						(190, 96)
					};

				\addplot [blue, domain=158:190, dashed] {1.78 * x - 242.2};

				\addplot [red, only marks] coordinates{
						(150, 40)
						(155, 47)
						(160, 50)
						(170, 55)
						(180, 65)
					};

				\addplot [red, domain=148:190, dashed] {0.76 * x - 73.38};
			\end{axis}
		\end{tikzpicture}
	\end{center}
	Come possiamo notare abbiamo due rette che passano molto vicine ai dati, ma hanno coefficiente angolare
	differente.

	Sia per maschi che per femmine possiamo dire che, più un ragazzo è alto, più è pesante. Il fatto che però
	le due rette crescano in modo differente ci dice che le ragazze hanno un rapporto peso-altezza più basso dei
	maschi.
\end{example}
