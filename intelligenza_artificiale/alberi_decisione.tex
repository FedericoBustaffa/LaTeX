\chapter{Alberi di decisione}
Con gli \textbf{alberi di decisione} andiamo a trattare problemi di classificazione ma con modelli pi\`u flessibili
che ci permettono di fare approssimazioni in una spazio delle ipotesi discreto.

Con questo approccio abbandoniamo anche le regole congiuntive, che, come avevamo visto, rendono il modello abbastanza
rigido.

Gli alberi di decisione rappresentano una disgiunzione di congiunzioni di vincoli sui valori degli attributi. In
pratica un albero ha vari \textbf{cammini} o \textbf{path}. Il nodo di un cammino \`e composto da un attributo, al
quale viene assegnato un certo valore. Ogni cammino termina un valore \verb|true| o \verb|false|.

L'albero \`e una congiunzione di tutte le congiunzioni (o cammini) che portano a un valore \verb|true|.

\section{Algoritmo ID3}
\`E un algoritmo di tipo \emph{greedy} che fa una ricerca nello spazio delle ipotesi usando l'euristica del
\textbf{maggior guadagno di informazione} o \textbf{information gain} e procede in questo modo:
\begin{enumerate}
	\item Scegliamo l'attributo con il maggior information gain.
	\item Valutiamo tutti i possibili valori dell'attributo generando, per ognuno di essi, un nodo successore.
	\item Ripetiamo il procedimento per ogni nodo successore generato finch\'e tutti gli esempi sono classficati
	      correttamente oppure finch\'e non ci sono pi\`u attributi rimasti.
\end{enumerate}
In questo modo abbiamo generato un albero di decisione, il quale conterr\`a vari cammini, ognuno dei quali terminer\`a
con un valore \verb|true| o \verb|false|.

I cammini potrebbero non essere tutti della stessa lunghezza. Mettiamo caso che per un certo valore di un certo
attributo, qualsiasi siano i valori degli altri attributi l'esempio \`e sempre positivo. In questo caso l'algoritmo
classificher\`a ogni esempio contenente quell'attributo con quel certo valore come positivo senza creare ulteriori
successori.

\subsection{Entropia}
La scelta del miglior attributo viene fatta in base al valore di \textbf{entropia}. L'entropia, in questo caso, indica
l'\emph{impurit\`a} di un insieme di esempi.

Essa dipende dalla distribuzione di una variabile casuale ($p$). In particolare se $S$ \`e il nostro insieme di esempi,
$p_+$ indica la proporzione di esempi positivi in $S$ mentre $p_-$ indica la proporzione di esempi negativi in $S$.

Il valore di entropia, relativa ad un certo insieme di esempi $S$, \`e dato dalla seguente formula:
\[ Entropy(S) = -p_+ \log_2{(p_+)} - p_- \log_2{(p_-)} \]
Ci\`o che vogliamo fare adesso, \`e assegnare un valore ad un attributo e misurare il valore di entropia di $S$
togliendo da $S$ tutti gli esempi in cui a quel determinato attributo \`e assegnato quel determinato valore. Andremo
quindi a calcolare
\[ Entropy(S_{A(v)}) \]
dove $S_{A(v)}$ indica l'insieme di tutti gli esempi in cui l'attributo $A$ assume come valore $v$.

L'information gain di un certo attributo $A$ non \`e altro che la differenza tra il valore di entropia di $S$ e la
somma dei valori di entropia di $S_{A(v)}$ per ogni possibile $v$ relativo ad $A$.

In realt\`a la somma \`e pesata sul rapporto tra la cardinalit\`a dell'insieme $S_{A(v)}$ e quella di $S$, infatti
la formula per ottenere l'information gain relativo all'attributo $A$ \`e definita in questo modo:
\[ Gain(S, A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_{A(v)}|}{|S|} Entropy(S_{A(v)}) \]
Un sottoinsieme $S_{A(v)}$ ben partizionato, avr\`a una bassa entropia, render\`a quindi pi\`u alto l'information gain.
Il nostro obbiettivo \`e trovare l'attributo che massimizza l'information gain.

Andremo quindi a preferire attributi che generano sottoinsiemi a bassa entropia, poich\'e massimizzano l'information
gain e riescono a partizionare meglio l'insieme di esempi.

\subsection{Gain Ratio}
L'information gain cos\`i definito ha per\`o un problema, ossia, quello di favorire attributi che hanno molti possibili
valori e che compaiono in pochi esempi.

In questo modo si ottengono sottoinsiemi perfettamente partizionati (o quasi) ma che per\`o non ci forniscono
un'informazione interessante nel complesso.

Introduciamo quindi un metodo per evitare la formazione di sottoinsiemi \emph{piccoli}, ossia, il \textbf{gain ratio}. Il
gain ratio si ottiene tramite il calcolo di un altro valore: lo \textbf{split information} definito come segue:
\[ SplitInformation(S, A) = -\sum_{i=1}^c \frac{|S_i|}{|S|} \log_2 \left(\frac{|S_i|}{|S|}\right) \]
dove $S_i$ indica il l'insieme ottenuto partizionando sul valore $i$ di $A$ e dove $c$ \`e il numero di possibili valori
che pu\`o assumere $A$.

Lo split information indica il valore di entropia di $S$ rispetto ai valori di $A$. Pi\`u \emph{uniformemente} i dati sono
dispersi, pi\`u alto \`e il suo valore.

Ora possiamo definire il gain ratio in questo modo:
\[ GainRatio(S, A) = \frac{Gain(S, A)}{SplitInformation(S, A)} \]
ottenendo cos\`i uno strumento che penalizza gli attributi che ottengono un alto gain tramite la frammentazione di tanti
insieme piccoli.

\subsubsection{Ulteriori aggiustamenti}
Anche con questo metodo si possono avere dei problemi. Infatti nel caso in cui ci sia un valore di un attributo costante
per tutti gli esempi,lo split information va a 0 (o comunque tenderebbe a 0), facendo cos\`i salire troppo il gain ratio.

Se il gain ratio sale troppo finiremo per privilegiare un attributo che in realt\`a non ci sta dicendo nulla poich\'e
\`e uguale per tutti gli esempi.

Per mitigare questo problema si procede in questo modo:
\begin{enumerate}
	\item Si calcola il gain per tutti gli attributi.
	\item Per gli attributi il cui gain supera un certo valore medio, si calcola il relativo gain ratio.
\end{enumerate}

\subsection{Analisi ID3}
L'algoritmo ID3 fa un Hill-Climbing in uno spazio delle ipotesi \emph{discreto}, dall'ipotesi pi\`u semplice a quella
pi\`u complessa. Inoltre, rispetto a Candidate Elimination, abbiamo che
\begin{itemize}
	\item Lo spazio delle ipotesi \`e \textbf{completo}.
	\item La ricerca \textbf{mantiene} una sola ipotesi consistente.
	\item Non c'\`e backtracking, dunque \textbf{non \`e ottimo}.
	\item Usa \textbf{tutti} gli esempi disponibili.
	\item Pu\`o avere \textbf{terminazione anticipata}.
\end{itemize}

\subsubsection{Bias induttivi per ID3}
Per gli alberi di decisione abbiamo due bias induttivi:
\begin{enumerate}
	\item Si preferiscono alberi \textbf{corti}.
	\item Si preferiscono alberi che mettono attributi con maggior information gain \textbf{vicino alla radice}.
\end{enumerate}
In questo caso abbiamo un limite sulla strategia di ricerca, ossia un \textbf{bias di ricerca}. Se invece avessimo avuto
restrizioni sullo spazio delle ipotesi avremmo avuto un \textbf{bias di linguaggio}.

In generale, l'uno non esclude l'altro, anzi, una combinazione di questi due bias fornisce modelli \textbf{flessibili},
che poi sono quelli che pi\`u si prestano a fare un buon fitting dei dati.

\section{Overfitting in alberi di decisione}
Analogamente al caso dei modelli lineari, costruire un albero di decisione che si adatta troppo agli esempi di training
pu\`o portare al problema dell'\textbf{overfitting}.

Quando trattiamo l'overfitting dobbiamo considerare due tipi di errore:
\begin{itemize}
	\item L'\textbf{errore empirico}, ossia, l'errore sul training set.
	      \[ error_D(h) \]
	\item L'\textbf{errore vero}, ossia, l'errore sull'intera distribuzione dei dati. Intendiamo quindi sia l'errore sui
	      dati di training che quello sui dati non ancora analizzati.
	      \[ error_X(h) \]
\end{itemize}

\begin{definition}
	Un'ipotesi $h$ fa \textbf{overfitting} sui dati di training se esiste un'ipotesi $h'$ tale che
	\begin{itemize}
		\item $error_D(h) < error_D(h')$
		\item $error_X(h') < error_X(h)$
	\end{itemize}
	ovvero se $h'$ si comporta meglio sui dati non ancora analizzati e peggio sul training set.
\end{definition}

\subsection{Validation Set}
Per mitigare il problema di overfitting negli alberi di decisione si possono adottare due strategie:
\begin{itemize}
	\item Si termina prima di raggiungere la perfezione sui dati di training.
	\item Si ammette overfitting sul training set e poi si fa un'operazione di \textbf{pruning} sull'albero.
\end{itemize}
In particolare si fa ricorso ad un nuovo strumento che andremo ad approfondire pi\`u avanti, ossia, il
\textbf{validation set}.

Quello che andiamo a fare \`e dividere in due parti il training set. Una parte avr\`a ancora la funzione di training,
l'altra diventer\`a il nostro validation set che ci andr\`a a guidare nella fase di pruning oppure ci forninar\`a
informazioni utili su quando fermarci (a seconda della strategia scelta).

\subsection{Pruning}
Andiamo ora a vedere come procedere nell'operazione di \textbf{pruning}.
\begin{enumerate}
	\item Ogni nodo \`e candidato per una possibile potatura.
	\item Fare pruning consiste nel rimuovere un sottoalbero radicato in un certo nodo. Il nodo a quel punto diventa una
	      foglia e gli viene assegnata la classe pi\`u comune tra gli esempi che gli erano stati assegnati.
	\item Un nodo \`e rimosso solo se non peggiora sul validation set.
	\item I nodi sono potati iterativamente: si cerca un nodo la cui rimozione porta ad un miglioramento sul validation
	      set
	\item Il pruning termina quando non c'\`e pi\`u alcun nodo, la cui rimozione porta ad un miglioramento sul validation
	      set
\end{enumerate}