\chapter{Alberi di decisione}
Con gli \textbf{alberi di decisione} andiamo a trattare problemi di classificazione ma con modelli pi\`u flessibili
che ci permettono di fare approssimazioni in una spazio delle ipotesi discreto.

Con questo approccio abbandoniamo anche le regole congiuntive, che, come avevamo visto, rendono il modello abbastanza
rigido.

Gli alberi di decisione rappresentano una disgiunzione di congiunzioni di vincoli sui valori degli attributi. In
pratica un albero ha vari \textbf{cammini} o \textbf{path}. Il nodo di un cammino \`e composto da un attributo, al
quale viene assegnato un certo valore. Ogni cammino termina un valore \verb|true| o \verb|false|.

L'albero \`e una congiunzione di tutte le congiunzioni (o cammini) che portano a un valore \verb|true|.

\section{Algoritmo ID3}
\`E un algoritmo di tipo \emph{greedy} che fa una ricerca nello spazio delle ipotesi usando l'euristica del
\textbf{maggior guadagno di informazione} o \textbf{information gain} e procede in questo modo:
\begin{enumerate}
	\item Scegliamo l'attributo con il maggior information gain.
	\item Valutiamo tutti i possibili valori dell'attributo generando, per ognuno di essi, un nodo successore.
	\item Ripetiamo il procedimento per ogni nodo successore generato finch\'e tutti gli esempi sono classficati
	      correttamente oppure finch\'e non ci sono pi\`u attributi rimasti.
\end{enumerate}
In questo modo abbiamo generato un albero di decisione, il quale conterr\`a vari cammini, ognuno dei quali terminer\`a
con un valore \verb|true| o \verb|false|.

I cammini potrebbero non essere tutti della stessa lunghezza. Mettiamo caso che per un certo valore di un certo
attributo, qualsiasi siano i valori degli altri attributi l'esempio \`e sempre positivo. In questo caso l'algoritmo
classificher\`a ogni esempio contenente quell'attributo con quel certo valore come positivo senza creare ulteriori
successori.

\subsection{Entropia}
La scelta del miglior attributo viene fatta in base al valore di \textbf{entropia}. L'entropia, in questo caso, indica
l'\emph{impurit\`a} di un insieme di esempi.

Essa dipende dalla distribuzione di una variabile casuale ($p$). In particolare se $S$ \`e il nostro insieme di esempi,
$p_+$ indica la proporzione di esempi positivi in $S$ mentre $p_-$ indica la proporzione di esempi negativi in $S$.

Il valore di entropia, relativa ad un certo insieme di esempi $S$, \`e dato dalla seguente formula:
\[ Entropy(S) = -p_+ \log_2{(p_+)} - p_- \log_2{(p_-)} \]
Ci\`o che vogliamo fare adesso, \`e assegnare un valore ad un attributo e misurare il valore di entropia di $S$
togliendo da $S$ tutti gli esempi in cui a quel determinato attributo \`e assegnato quel determinato valore. Andremo
quindi a calcolare
\[ Entropy(S_{A(v)}) \]
dove $S_{A(v)}$ indica l'insieme di tutti gli esempi in cui l'attributo $A$ assume come valore $v$.

L'information gain di un certo attributo $A$ non \`e altro che la differenza tra il valore di entropia di $S$ e la
somma dei valori di entropia di $S_{A(v)}$ per ogni possibile $v$ relativo ad $A$.

In realt\`a la somma \`e pesata sul rapporto tra la cardinalit\`a dell'insieme $S_{A(v)}$ e quella di $S$, infatti
la formula per ottenere l'information gain relativo all'attributo $A$ \`e definita in questo modo:
\[ Gain(S, A) = Entropy(S) - \sum_{v \in Values(A)} \frac{|S_{A(v)}|}{|S|} Entropy(S_{A(v)}) \]
Un sottoinsieme $S_{A(v)}$ ben partizionato, avr\`a una bassa entropia, render\`a quindi pi\`u alto l'information gain.
Il nostro obbiettivo \`e trovare l'attributo che massimizza l'information gain.

Andremo quindi a preferire attributi che generano sottoinsiemi a bassa entropia, poich\'e massimizzano l'information
gain e riescono a partizionare meglio l'insieme di esempi.

\subsection{Gain Ratio}
L'information gain cos\`i definito ha per\`o un problema, ossia, quello di favorire attributi che hanno molti possibili
valori e che compaiono in pochi esempi.

In questo modo si ottengono sottoinsiemi perfettamente partizionati (o quasi) ma che per\`o non ci forniscono
un'informazione interessante nel complesso.

Introduciamo quindi un metodo per evitare la formazione di sottoinsiemi \emph{piccoli}, ossia, il \textbf{gain ratio}. Il
gain ratio si ottiene tramite il calcolo di un altro valore: lo \textbf{split information} definito come segue:
\[ SplitInformation(S, A) = -\sum_{i=1}^c \frac{|S_i|}{|S|} \log_2 \left(\frac{|S_i|}{|S|}\right) \]
dove $S_i$ indica il l'insieme ottenuto partizionando sul valore $i$ di $A$ e dove $c$ \`e il numero di possibili valori
che pu\`o assumere $A$.

Lo split information indica il valore di entropia di $S$ rispetto ai valori di $A$. Pi\`u \emph{uniformemente} i dati sono
dispersi, pi\`u alto \`e il suo valore.

Ora possiamo definire il gain ratio in questo modo:
\[ GainRatio(S, A) = \frac{Gain(S, A)}{SplitInformation(S, A)} \]
ottenendo cos\`i uno strumento che penalizza gli attributi che ottengono un alto gain tramite la frammentazione di tanti
insieme piccoli.

\subsubsection{Ulteriori aggiustamenti}
Anche con questo metodo si possono avere dei problemi. Infatti nel caso in cui ci sia un valore di un attributo costante
per tutti gli esempi,lo split information va a 0 (o comunque tenderebbe a 0), facendo cos\`i salire troppo il gain ratio.

Se il gain ratio sale troppo finiremo per privilegiare un attributo che in realt\`a non ci sta dicendo nulla poich\'e
\`e uguale per tutti gli esempi.

Per mitigare questo problema si procede in questo modo:
\begin{enumerate}
	\item Si calcola il gain per tutti gli attributi.
	\item Per gli attributi il cui gain supera un certo valore medio, si calcola il relativo gain ratio.
\end{enumerate}

\subsection{Analisi ID3}
L'algoritmo ID3 fa un Hill-Climbing in uno spazio delle ipotesi \emph{discreto}, dall'ipotesi pi\`u semplice a quella
pi\`u complessa. Inoltre, rispetto a Candidate Elimination, abbiamo che
\begin{itemize}
	\item Lo spazio delle ipotesi \`e \textbf{completo}.
	\item La ricerca \textbf{mantiene} una sola ipotesi consistente.
	\item Non c'\`e backtracking, dunque \textbf{non \`e ottimo}.
	\item Usa \textbf{tutti} gli esempi disponibili.
	\item Pu\`o avere \textbf{terminazione anticipata}.
\end{itemize}

\subsubsection{Bias induttivi}
Per gli alberi di decisione abbiamo due bias induttivi:
\begin{enumerate}
	\item Si preferiscono alberi \textbf{corti}.
	\item Si preferiscono alberi che mettono attributi con maggior information gain \textbf{vicino alla radice}.
\end{enumerate}
In questo caso abbiamo un limite sulla strategia di ricerca e non sullo spazio delle ipotesi. Abbiamo quindi un
\textbf{bias di ricerca}. I \textbf{bias di linguaggio} sono invece restrizioni sullo spazio di ricerca.

Una combinazione di questi due bias fornisce modelli \textbf{flessibili}, che in generale, sono quelli che pi\`u si
prestano a fare un buon fitting dei dati.