\chapter{Modelli lineari}
I modelli lineari sono molto utili per risolvere \textbf{problemi di regressione}, che, come abbiamo gi\`a anticipato
hanno una funzione obbiettivo che restituisce un valore target di tipo \textbf{numerico reale}.

Il nostro obbiettivo \`e quello di riuscire ad approssimare una funzione a valori reali e continui che potrebbe restituire
anche dati imprecisi o \textbf{rumorosi}. Un tipico esempio di coppia presente nel training set sar\`a del tipo:
\[ (x, f(x) + rumore) \]

\section{Regressione lineare univariata}
Per capire meglio i modelli lineari facciamo riferimento ad un sottoinsieme della classe di problemi descritta sopra, ovvero
la \textbf{regressione lineare univariata}, in cui si considerano modelli composti da funzioni lineari, che hanno una sola
variabile $x$ in input e una sola variabile $y$ in output.

Per questa classe di problemi si assume un modello espresso come
\[ h(x) = w_1 x + w_0 \]
dove i $w_i$ sono coefficienti reali o parametri liberi detti \textbf{pesi}.

\subsection{Struttura del problema}
Quello che vogliamo fare \`e un'operazione di \textbf{fitting}, ossia tracciare una retta che passi il pi\`u vicino
possibile ad ognuno dei punti presenti nel training set. Nello specifico, vogliamo costruire un algoritmo di apprendimento
che riesca a stimare la $y$ corrispondente ad altri valori $x$ ancora non osservati, trovando $w_1$ e $w_0$ e
\emph{minimizzando} l'errore o perdita di precisione.

\subsubsection{Funzione di errore}
Il valore dell'errore accumulato su dati del training set \`e definito dalla seguente funzione:
\[ loss(h_w) = E(w) = \sum_{p=1}^l (y_p - h_w(x_p))^2 = \sum_{p=1}^l (y_p - (w_1 x_p + w_0))^2 \]
dove $l$ \`e il numero di esempi presenti nel training set.

In pratica non si fa altro che misurare la differenza tra il valore target del training set e il valore che restituisce
una certa approssimazione $h$.

Il valore trovato viene elevato al quadrato per due motivi:
\begin{enumerate}
	\item L'idea \`e che la funzione $E$ sommi tutti gli errori sui singoli dati ma il valore potrebbe essere sia positivo
	      che negativo e questo potrebbe portare ad un'errata diminuzione dell'errore o addirittura ad un annullamento.
	      Per evitare questo eleviamo l'errore al quadrato in modo da avere sempre un valore positivo.
	\item Una volta letto il punto precedente ci si potrebbe chidere perch\'e non usare il valore assoluto. Il motivo \`e
	      che vogliamo trovare il minimo della funzione $E$ e per farlo occorre calcolarne la derivata. La derivata di un
	      polinomio elevato al quadrato \`e semplicemente pi\`u comoda e pi\`u semplice da calcolare rispetto a quella
	      di un valore assoluto.
\end{enumerate}

\subsection{Metodo di risoluzione analitico - LMS}
La tecnica che vedremo \`e chiamata \emph{Least Mean Square} il cui obbiettivo \`e quello di trovare un valore dei
coefficienti $w$ che minimizzi l'errore quadratico.

Per fare questo abbiamo bisogno di uno strumento gi\`a introdotto in precedenza: il \textbf{gradiente}.

L'idea \`e molto semplice ma anche efficace. Dato che nel training set sono forniti i valori $x$ e $y$, i valori incogniti
sono $w_1$ e $w_0$. Per trovarli
\begin{enumerate}
	\item Calcoliamo la funzione $E$ per tutti gli esempi presenti nel training set.
	\item Calcoliamo la derivata parziale della funzione rispetto a $w_0$ e $w_1$, ossia calcoliamo il gradiente.
	\item Eguagliamo il gradiente a 0.
	      \[
		      \begin{pmatrix}
			      \displaystyle\frac{ \partial E(w) }{ \partial w_0 }, &
			      \displaystyle\frac{ \partial E(w) }{ \partial w_1 }
		      \end{pmatrix} = 0
	      \]
	\item Risolto il sistema ottengo $w_1$ e $w_0$ tali che la distanza tra i dati e la retta sia minima per tutti i dati.
\end{enumerate}

Agendo in maniera analitica abbiamo semplicemente trovato il punto stazionario corrispondente al minimo globale della
funzione $E$.

Adesso abbiamo una funzione $h_w$ con dei $w$ fissati con cui posso provare a calcolare nuovi $y$ relativi a nuovi
input $x$ non presenti nel training set.

Chiariamo che i nuovi valori $h_w(x)$ calcolati saranno approssimazioni del reale valore relativo a $x$, tranne rari casi
in cui il reale valore $y$ relativo a $x$ si trovi esattamente sulla retta trovata.

\subsection{Metodo algoritmico}
Adesso vogliamo per\`o vogliamo agire in maniera algoritmica e cerca la funzione giusta con un metodo simile a quello
visto nel capitolo \ref{chapter: ricerca_locale} quando parlavamo di ricerca locale in spazi continui.

Avevamo definito una regola che ci permetteva di muoverci in spazi continui seguendo le indicazioni del gradiente in questo
modo:
\[ x_{\text{new}} = x \pm \eta \nabla f(x) \]
Adesso dobbiamo semplicemente riadattarla come segue:
\[ w_{\text{new}} = w - \eta \nabla E(w) \]
Sapendo che
\[ -\nabla E(w) = \Delta w \]
possiamo definire adesso la \textbf{regola delta} riscrivendo la funzione in questo modo:
\[ w_{\text{new}} = w + \eta \Delta w \]
Notiamo che nella ricerca locale usavamo $+\nabla f(x)$ per cercare massimi locali e $-\nabla f(x)$ per cercare minimi
locali. In questo caso siamo interessati solo ai minimi locali dunque nella formula comparir\`a sempre
$-\nabla E(w) = +\Delta w$.

Ricordiamo inoltre che $\eta$ rappresenta la dimensione del passo ($0 < \eta < 1$), che, in ambito Machine Learning,
rappresenta il \textbf{learning rate}.

Su questa base possiamo costruire un algoritmo di ricerca nello spazio delle ipotesi che corregge l'errore man mano che
si elaborano nuovi esempi del training set. Nello specifico le correzioni avvengono in questo modo:
\begin{itemize}
	\item Se $y = h(x)$ l'errore \`e nullo quindi non faccio niente.
	\item Se $h(x) > y$ significa che il valore atteso si trova sopra la nostra retta. Le correzioni che possiamo fare
	      sono quindi:
	      \begin{itemize}
		      \item Trasliamo la retta in basso decrementando $w_0$: caso $\Delta w_0 < 0$.
		      \item Se $x > 0$ ruotiamo la retta in senso orario decrementando $w_1$: caso $\Delta w_1 < 0$.
		      \item Altrimenti se $x < 0$ ruotiamo la retta in senso antiorario incrementando $w_1$: caso $\Delta w_1 > 0$.
	      \end{itemize}
	\item Se $h(x) < y$ mi muovo in maniera speculare al punto precedente.
\end{itemize}
Per velocizzare un po' i calcoli possiamo subito derivare le formule generali in questo modo
\begin{gather*}
	\Delta w_0 = -\frac{\partial E(w)}{\partial w_0} = 2 \cdot \sum_{p=1}^l (y_p - h_w(x)) \\
	\\
	\Delta w_1 = -\frac{\partial E(w)}{\partial w_1} = 2 \cdot \sum_{p=1}^l (y_p - h_w(x)) \cdot x_p
\end{gather*}

\subsubsection{Algoritmo}
Ora possiamo definire l'algoritmo come segue
\begin{enumerate}
	\item Iniziamo con dei valori di $w$ iniziali presi a caso e possibilmente piccoli.
	\item Fissiamo $\eta$ tra 0 e 1.
	\item \label{enum: delta w} Calcoliamo $\Delta w$ per ogni $w_i$.
	\item Calcoliamo $w_\text{new}$ per ogni $w_i$.
	\item Torniamo al punto \ref{enum: delta w} finch\'e non si ha una convergenza (non si migliora pi\`u) o
	      $E(w)$ non \`e abbastanza piccolo.
\end{enumerate}

L'algoritmo pu\`o essere implementato con due politiche differenti:
\begin{itemize}
	\item \textbf{Batch}: si calcola il $\Delta w$ su tutto l'insieme dei dati di training (\textbf{epoca}) in un solo
	      colpo svolgendo interamente la sommatoria. Solo alla fine si andr\`a ad applicare la regola delta per aggiornare
	      i pesi.

	      Questo approccio garantisce maggiore precisione e linearit\`a nel raggiungere la soluzione.
	\item \textbf{Online}: in questo caso non si svolge la sommatoria ma si calcola il $\Delta w$ per ogni esempio nel
	      training set e si va subito ad applicare la regola delta aggiornando i pesi ogni volta.

	      Questa politica pu\`o rendere l'algoritmo pi\`u veloce ma, generalmente, lo rende anche pi\`u instabile. necessita
	      quindi di un \textbf{learning rate} pi\`u basso rispetto alla politica batch.
\end{itemize}

\section{Input multidimensionale}
Abbiamo visto il caso base con una sola variabile in input ma nella realt\`a i problemi affrontati possono avere in input
molte variabili. Dobbiamo quindi trovare una regola pi\`u generale che ci aiuti a trattare problemi con pi\`u variabili in
input.

D'ora in avanti considereremo membri del training set (\textbf{pattern}) multidimensionali che hanno per l'appunto pi\`u
variabili di input e un valore target di output. Indicheremo quindi con $x_{p, i}$ l'$i$-esima variabile del $p$-esimo
pattern e $X$ sar\`a una matrice $l \times n$ dove $l$ \`e il numero di pattern e $n$ \`e il numero di variabili per ogni
pattern.

Stavolta la funzione da approssimare sar\`a nella forma
\[ h(x_1, \dots, x_n) = w_0 + w_1 x_1 + \dots + w_n x_n = w_0 + \sum_{i=1}^n w_i x_i \]
e non rappresenter\`a pi\`u una retta come nella regressione linerare univariata ma rappresenter\`a degli
\textbf{iperpiani}.

Per rendere pi\`u compatta la notazione useremo $x^T = [1, x_1, \dots, x_n]$ per indicare l'insieme delle $n$ variabili
in input relative a un generico pattern e $w = [w_0, w_1, \dots, w_n]$ per indicare i pesi. Ora possiamo riscrivere la
formula in maniera pi\`u compatta in questo modo:
\[ x^T w = \sum_{i=0}^n w_i x_i \]
Adesso possiamo facilmente definire la funzione $E$ per input multidimensionali come segue:
\[ E(w) = \sum_{p=1}^l (y_p - x^T_p w)^2 = \| y - X w \|^2 \]
Di conseguenza possiamo definire anche $\Delta w$ in questo modo:
\[
	\Delta w_i = -\frac{\partial E(w)}{\partial w_i} = 2 \sum_{p=1}^l (y_p - h_w(x_p)) \cdot x_{p, i} =
	\sum_{p=1}^l (y_p - x^T_p w) \cdot x_{p, i}
\]
Una volta definita l'equazione per input multidimensionale possiamo riutilizzare l'algoritmo visto in precedenza per la
regressione lineare univariata.

Possiamo inoltre costruire un algoritmo che non tiene un learning rate fisso ma lo decrementa col passare del tempo.
Questo fa s\`i che all'inizio la correzione dell'errore sia pi\`u grossolana evitandoci di procedere troppo lentamente,
in seguito per\`o, col decrescere dell'errore, anche le correzioni che dovremo fare saranno sempre pi\`u piccole. Un
learning rate pi\`u basso ci aiuta dunque ad essere pi\`u precisi man mano che ci si avvicina alla soluzione.

\section{Limitazioni}
Un grosso limite dei modelli lineari \`e il problema dell'\textbf{underfitting}. Questo si traduce in una scarsa
capacit\`a di approssimazione della funzione obbiettivo. Nel caso della regressione lineare univariata ad esempio posso
tracciare la miglior retta possibile ma potrei avere comunque un errore molto alto alcuni esempi di training.

Per la trattazione di problemi pi\`u complessi \`e necessario muoverci verso modelli \textbf{non lineari}, che vedremo nel
prossimo capitolo.

\section{Relazioni non lineari}
Per approssimare meglio casi di regressione \`e molto conveniente passare a relazioni di tipo \textbf{non lineare}.
Fino ad ora abbiamo trattato modelli lineari espressi dalla formula
\[ h_w(x) = \sum_{i = 0}^n w_i x_i \]
con le variabili e coefficienti di grado 1. Adesso vogliamo passare a modelli pi\`u flessibili con una maggior
capacit\`a di approssimazione. Per farlo possiamo passare ad un polinomio di questo tipo
\[ h_w(x) = \sum_{i = 0}^n w_i x^i \]
Quella appena scritta \`e detta \textbf{regressione polinomiale} ma il modello resta lineare. Questo perch\'e ci\`o che
rende lineare un modello \`e la linearit\`a nei pesi $w$ e non nelle variabili $x$.

Precisiamo anche che la formula appena scritta vale solo per \textbf{regressioni polinomiali univariate}. Infatti se
notiamo, la variabile in ingresso \`e una sola ma ha grado $n$.

Questo \`e molto comodo e possiamo usare ancora l'algoritmo LMS per trovare una buona approssimazione della funzione
dato che il modello rimane lineare.

\section{Espansione lineare della base}
Quel che vogliamo fare ora \`e trovare modelli lineari ancora pi\`u flessibili e funzionanti per pi\`u variabili in
ingresso. Ecco che viene introdotta l'\textbf{espansione lineare di base}, la cui formula, ancora pi\`u generale della
precedente  \`e definita in questo modo:
\[ h_w(x) = \sum_{k = 0}^K w_k \phi_k(x) \]
dove $x$ equivale al pattern $x = [x_1, x_2, \dots, x_n]$ e dove $\phi$ \`e una funzione da
$\mathbb{R}^n$ in $\mathbb{R}$

Il modello appena rappresentato continua ad essere linerare nei coefficienti $w$ e anche in $\phi$ ma non lo \`e pi\`u
in $x$. Come gi\`a spiegato in precedenza, questo fa s\`i che il modello sia ancora lineare, consentendoci cos\`i di
utilizzare ancora l'algoritmo LMS per riuscire a trovare una buona approssimazione.

Quello che abbiamo introdotto ora ci consente un maggior grado di libert\`a nell'approssimazione della funzione ma ha
di contro il fatto che il modello potrebbe diventare \textbf{troppo complesso}. Dove per \emph{complessit\`a} intendiamo
il grado di libert\`a del modello. Un modello \`e tanto pi\`u espressivo, quanto pi\`u \`e complesso.

\subsection{Complessit\`a dei modelli}
Ora che abbiamo piena libert\`a di espressione per definire modelli flessibili a piacere, andiamo in contro a nuovi
problemi. Uno di questi \`e il problema della scelta per la $\phi$. La scelta di $\phi$ \`e fondamentale per costruire
un buon modello e in base a questa scelta andiamo incontro ad altri due problemi: l'\textbf{underfitting} e
l'\textbf{overfitting}.

Il primo esprime la scarsa capacit\`a di approssimazione del modello, il quale ammette molto rumore sui dati di test.

Il secondo, invece, denota una capacit\`a di approssimazione del modello altissima sui dati di test. Talmente alta da
non ammettere rumore su di essi ($E(w) = 0$ sul training set). Ma per riuscire ad essere perfetto sui dati di test crea
una funzione che non riesce ad approssimare in maniera consistente la funzione obbiettivo.

\subsubsection{Individuare underfitting e overfitting}
Mentre per l'individuazione di un problema di underfitting baster\`a calcolare $E(w)$ per tutti i dati di training,
la quale restituir\`a un alto valore di errore, individuare un problema di overfitting sar\`a pi\`u difficile.
Questo perch\'e l'errore sui dati di training calcolato da $E(w)$ sar\`a nullo.

\section{Regolarizzazione}
La \textbf{regolarizzazione} \`e un approccio che permette di usare un modello complesso e in seguito
\emph{regolarizzarlo} per rendere migliore la sua capacit\`a di approssimazione mitigando il pi\`u possibile il
problema di overfitting.

In generale \`e meglio usare un modello pi\`u flessibile e regolarizzarlo in seguito, piuttosto che usare un modello
troppo rigido che non pu\`o poi pi\`u essere migliorato.

Nello specifico andremo a parlare di \textbf{regressione Ridge} o \textbf{regolarizzazione di Tikhonov}. In questo caso
sarebbe pi\`u appropriato chiamare la funzione $E$ di errore $loss$ poich\'e nel problema di overfitting l'errore \`e
nullo o comunque minimo sui dati di training. Qui ci interessa sapere quanta precisione perdiamo nell'approssimare la
funzione obbiettivo e dunque la funzione $loss$ sar\`a definita in questo modo:
\[ E(w) = \sum_{p = 1}^l (y_p - h_w(x_p))^2 + \lambda \| w \|^2 \]
dove $\lambda$ \`e detto coefficiente di regolarizzazione e dove
\[ \| w \|^2 = \sum_{i} w_i^2 \]
L'effetto \`e quello di ridurre i pesi $w$ ottenendo cos\`i una nuova regola delta:
\[ w_{\text{new}} = w + \eta \Delta w - 2 \lambda w \]
Va specificato che tanto pi\`u $\lambda$ \`e alto tanto pi\`u il modello si semplifica. Dunque anche la scelta di un
$\lambda$ corretto determina quanto il modello venga regolarizzato.

Se scelgo un alto $\lambda$ rischio il problema dell'underfitting, se lo scelgo troppo basso rimango nel problema di
overfitting che stavo cercando di risolvere con la regolarizzazione.

Questo metodo ci permette di regolarizzare anche modelli di complessit\`a non nota. Strumento quindi molto utile dato
che non \`e sempre possibile sapere quanto il nostro modello sia complesso.

\section{Problemi di classificazione}
Fino ad ora abbiamo utilizzato modelli lineari solo per trattare problemi di regressione. In realt\`a i modelli lineari
sono molto utili anche per affrontare problemi di classificazione.

Se prima il modelli lineare restituiva un valore numerico reale adesso si limiter\`a restituire solo due valori numerici:
uno per indicare \verb|false| (in genere $0$ o $-1$) e uno per indicare \verb|true| (in genere $1$).

Per farlo assumiamo una zona dell'iperpiano $wx$ negativa e una positiva e la usiamo per classificare un punto. Se il
punto si trova nella zona dell'iperpiano negativa il valore di ritorno sar\`a \verb|false|, se si trova in quella
positiva sar\`a \verb|true|.

La notazione non cambia, se non per il fatto che gli unici valori target nel training set saranno $-1/0$ e $1$.

\begin{definition}
	Chiamo \textbf{decision boundary} il luogo geometrico dei punti tali che $x^T w = 0$. Al di fuori di esso ci sono
	le zone assunte negative e positive dell'iperpiano.
\end{definition}

La funzione che useremo per i valori di ritorno sar\`a
\[
	h(x) = \begin{cases}
		1 & \text{se } x^T w \geq 0 \\
		0 & \text{altrimenti}
	\end{cases}
\]
nel caso considerassimo $0$ e $1$ come valori target.
\[ h(x) = \text{sign}(x^T w) \]
nel caso considerassimo $-1$ e $1$ come valori target.

I valori che classifichiamo come positivi vengono separati da quelli classificati negativi da un iperpiano.

L'obbiettivo per questa classe di problemi \`e quello di trovare i $w$ tali che, il relativo iperpiano generato,
separari al meglio gli esempi negativi da quelli positivi. Muovendo l'iperpiano si andr\`a a muovere anche il decision
boundary.

\subsection*{Analogia con regressione lineare univariata}
Per fissare meglio il concetto, facciamo un'analogia con il problema di regressione lineare univariata. Il training
set, come anticipato, contiene coppie di questo tipo:
\[ (x, y) \]
dove $x$ \`e l'unica variabile in input e $y$ \`e il valore target che assume valore $-1/0$, se negativo oppure $1$ se
positivo.

Noi vogliamo trovare una retta che divida al meglio i punti positivi da quelli negativi e che dunque riesca a fare una
buona approssimazione di classificazione.

Non facciamo altro che applicare l'algoritmo LMS.

Per trovare il decision boundary dovr\`o quindi
\[ f_w(x) = w_1 x + w_0 \]
ma stavolta la funzione che consideriamo per i valori target sar\`a
\[ h(x) = \text{sign}(f_w(x)) \]
Dunque per valori di $f_w(x)$ negativi $h(x)$ ritorner\`a \verb|false|, viceversa per valori positivi, $h(x)$
ritorner\`a \verb|true|.

Il nostro obbiettivo \`e approssimare $f_w(x)$ per riuscire a fare previsioni sui valori di verit\`a per nuovi dati
$x$ in input non ancora trattati.
