\chapter{Modelli lineari}
I modelli lineari sono molto utili per risolvere \textbf{problemi di regressione}, che, come abbiamo gi\`a anticipato
hanno una funzione obbiettivo che restituisce un valore target di tipo \textbf{numerico}.

Il nostro obbiettivo \`e quello di riuscire ad approssimare una funzione a valori reali e continui che potrebbe restituire
anche dati imprecisi o \textbf{rumorosi}. Un tipico esempio di coppia presente nel training set sar\`a del tipo:
\[ (x, f(x) + rumore) \]

\section{Regressione lineare univariata}
Per utilizzare modelli lineari dobbiamo fare riferimento ad un sottoinsieme della classe di problemi descritta sopra, ovvero
la \textbf{regressione lineare univariata}, in cui si considerano modelli composti da funzioni lineari, che hanno una sola
variabile $x$ in input e una sola variabile $y$ in output.

Per questa classe di problemi si assume un modello espresso come
\[ h(x) = w_1 x + w_0 + r \]
dove i $w_i$ sono coefficienti reali o parametri liberi detti \textbf{pesi} e $r$ sia il rumore ossia l'errore che
commettiamo nello stimare la funzione obbiettivo

\subsection{Struttura del problema}
Quello che vogliamo fare \`e un'operazione di \textbf{fitting}, ossia tracciare una retta che passi il pi\`u vicino
possibile ad ognuno dei dati presenti nel training set. Nello specifico, vogliamo costruire un algoritmo di apprendimento
che riesca a stimare la $y$ corrispondente ad altri valori $x$ ancora non osservati, trovando $w_1$ e $w_0$ e
\emph{minimizzando} l'errore o perdita di precisione.

\subsubsection{Errore}
Come valore d'errore considereremo il quadrato della differenza tra il valore target del training set e l'output della
funzione stimata dall'algoritmo che vedremo pi\`u avanti. Di seguito la funzione d'errore:
\[ loss(h_w) = E(w) = \sum_{p=1}^l (y_p - h_w(x_p))^2 = \sum_{p=1}^l (y_p - (w_1 x_p + w_0))^2 \]
I motivi dell'elevamento al quadrato sono due. Il primo ha a che fare con la correttezza dell'errore calcolato: noi
vogliamo valutare l'errore che la nostra funzione sta commettendo e minimizzarlo. I valori della differenza di cui
abbiamo parlato poco fa potrebbero per\`o avere segno discorde e quindi annullarsi a vicenda o riducendo l'errore una
volta sommati. L'errore effettivo continuerebbe tuttavia a crescere o, nella migliore delle ipotesi rimarrebbe invariato.

Il secondo motivo \`e relativo alla comodit\`a di calcolo. Elevare al quadrato la differenza significa non avere mai valori
negativi. Ma perch\'e non utilizziamo il valore assoluto ?

Come vedremo pi\`u avanti, dovremo calcolare delle derivate e la derivata di un polinomio di secondo grado \`e pi\`u
semplice e pi\`u comoda da calcolare rispetto alla derivata di un valore assoluto.

\subsection{Metodo di risoluzione - LMS}
La tecnica che vedremo \`e chiamata \emph{Least Mean Square} il cui obbiettivo \`e quello di trovare un valore dei
coefficienti $w$ che minimizzi l'errore quadratico.

Per fare questo abbiamo bisogno di uno strumento gi\`a introdotto in precedenza: il \textbf{gradiente}.

L'idea \`e molto semplice ma anche efficace. Dato che nel training set sono forniti i valori $x$ e $y$, i valori incogniti
sono $w_1$ e $w_0$. Per trovarli
\begin{enumerate}
	\item Calcoliamo la funzione $loss$ per tutti gli esempi presenti nel training set.
	\item Calcoliamo la derivata parziale della funzione rispetto a $w_0$ e $w_1$, ossia calcoliamo il gradiente.
	\item Eguagliamo il gradiente a 0.
	      \[
		      \begin{pmatrix}
			      \displaystyle\frac{ \partial E(w) }{ \partial w_0 }, &
			      \displaystyle\frac{ \partial E(w) }{ \partial w_1 },
		      \end{pmatrix} = 0
	      \]
	\item Risolto il sistema ottengo $w_1$ e $w_0$ tali che la distanza tra i dati e la retta sia minima per tutti i dati.
\end{enumerate}
Agendo in maniera analitica abbiamo semplicemente trovato il punto stazionario corrispondente al minimo globale della
funzione $loss$.

Adesso abbiamo una funzione $h_w$ con dei $w$ fissati con cui posso provare a calcolare nuovi $y$ relativi a nuovi
input $x$ non presenti nel training set.

\subsubsection{Metodo algoritmico}
Adesso vogliamo per\`o vogliamo agire in maniera algoritmica e cerca la funzione giusta con un metodo simile a quello
visto nel capitolo \ref{chapter: ricerca_locale} quando parlavamo di ricerca locale in spazi continui.

Avevamo definito una regola che ci permetteva di muoverci in spazi continui seguendo le indicazioni del gradiente in questo
modo:
\[ x_{\text{new}} = x - \eta \nabla f(x) \]
Adesso dobbiamo semplicemente riadattarla come segue:
\[ w_{\text{new}} = w - \eta \nabla E(w) \]
Sapendo che
\[ -\nabla E(w) = \Delta w \]
posso riscrivere la funzione in questo modo:
\[ w_{\text{new}} = w + \eta \Delta w \]
Ricordiamo che $\eta$ rappresenta la dimensione del passo ($0 < \eta < 1$). Nell'ambito del machine learning rappresenta il cosidetto
\textbf{learning rate}.

Ora possiamo correggere la nostra stima man mano che assumiamo dati in questo modo:
\begin{itemize}
	\item Se $y = h(x)$ l'errore \`e nullo quindi non faccio niente.
	\item Se $h(x) > y$ posso compiere due operazioni:
	      \begin{itemize}
		      \item Traslo la retta in basso decrementando $w_0$: caso $\Delta w_0 < 0$.
		      \item Se $x > 0$ ruoto la retta decrementando $w_1$: caso $\Delta w_1 < 0$.
		      \item Altrimenti ruoto la retta incrementando $w_1$: caso $\Delta w_1 > 0$.
	      \end{itemize}
	\item Se $h(x) < y$ mi muovo in maniera speculare al punto precedente.
\end{itemize}

\subsubsection{Metodo di risoluzione online}
Prima definito quella che viene chiamata \textbf{politica batch} dell'algoritmo. In sostanza si prendono tutti i membri
presenti nel training set (\textbf{epoca}), si calcola la funzione $loss$ finch\'e non si esauriscono ed infine si calcola
il gradiente per aggiornare i $w$.

Esiste anche un'altro modo di procedere. In pratica, quello che succede \`e che si calcola la funzione $loss$ per un singolo
membro del training set e subito si aggiornano i $w$. Si ripete il procedimento per tutti i membri del training set.

Questo metodo fa subire al gradiente variazioni pi\`u imprevedibili e significative. Potrebbe essere pi\`u veloce ma
necessita di un $\eta$ pi\`u piccolo.
