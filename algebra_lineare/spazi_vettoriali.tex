\chapter{Spazi Vettoriali}

\section{Definizione di spazio vettoriale}
Per fornire la definizione di spazio vettoriale si ha bisogno di un insieme non vuoto $V$ e di un campo
$\K$, dove sia possibile definire le operazioni di \textbf{somma vettoriale} e
\textbf{prodotto per scalare}.

\begin{definition}
	Uno \textbf{spazio vettoriale su un campo} $\K$ è un insieme $V$ su cui sono definite la somma
	fra due elementi di $V$ (il cui risultato è ancora un elemento di V, si dice quindi che $V$ è chiuso
	per la somma), e il prodotto di un elemento di $\K$ per un elemento di $V$ (il cui risultato è
	sempre un elemento di $V$, si dice quindi che V è chiuso per il prodotto con elementi di $\K$)
	che verificano le seguenti \textbf{proprietà}:
	\begin{enumerate}
		\item \textbf{Associatività della somma}: $\forall u, v, w \in V$ vale
		      \[ (u + v) + w = u + (v + w) \]
		\item \textbf{Commutatività della somma}: $\forall v, w \in V$ vale
		      \[ v + w = w + v \]
		\item \textbf{Elemento neutro per la somma}: $\exists O \in V$ tale che $\forall v \in V$ vale
		      \[ v + O = v \]
		\item \textbf{Inverso per la somma}: $\forall v \in V$, $\exists w \in V$ tale che
		      \[ v + w = O \]
		\item \textbf{Distributività del prodotto per uno scalare}: $\forall \lambda, \mu \in \K$ e
		      $\forall v, w \in V$ vale
		      \[ \lambda(v + w) = \lambda v + \lambda w \]
		      e anche
		      \[ (\lambda + \mu)v = \lambda v + \mu v \]
		\item \textbf{Associatività del prodotto per uno scalare}: $\forall \lambda, \mu \in \K$ e
		      $\forall v \in V$ vale
		      \[ (\lambda \mu)v = \lambda(\mu v) \]
		\item \textbf{Invariante moltiplicativo}: $\forall v \in V$ vale
		      \[ 1v = v \]
	\end{enumerate}
\end{definition}

\begin{observation}
	L'elemento neutro della somma $O$ e lo $0$, elemento neutro di $\K$ sono due cose ben distinte, il primo è
	un vettore, il secondo è uno scalare.
\end{observation}

\begin{example}
	Ogni campo $\K$ è uno spazio vettoriale su $\K$ stesso con le operazioni di somma vettoriale e prodotto per
	scalare che sono definite identiche alle operazioni di somma e prodotto sul campo. In particolare $\R$ è uno
	spazio vettoriale su $\R$, così come $\Q$ è uno spazio vettoriale su $\Q$.
\end{example}

\begin{example}
	$\R^2 = \{(a, b) \mid a,b \in \R\}$ è uno spazio vettoriale su $\R$ con le operazioni di somma vettoriale e
	prodotto scalare definite come segue:
	\begin{align*}
		(a,b) + (c,d) =  & (a + c, b + d)         \\
		\lambda (a, b) = & (\lambda a, \lambda b)
	\end{align*}
\end{example}

\begin{example}
	Anche l'insieme dei polinomi $\K[x]$, con la somma tra polinomi e il prodotto tra polinomi e costanti di
	$\K$ definiti come segue:
	\begin{itemize}
		\item Il polinomio somma di $p(x)$ e $q(x)$ è quello il cui coefficiente di grado $n$ è la somma dei
		      coefficienti di grado $n$ dei polinomi $p(x)$ e $q(x)$.
		\item Il polinomio prodotto di $k \in \K$ e $p(x)$ è il polinomio che ha come coefficiente di
		      grado $n$ $k$ volte il coefficiente di grado $n$ di $p(x)$.
	\end{itemize}
	è uno spazio vettoriale su $\K$.
\end{example}

\section{Sottospazi vettoriali}

\begin{definition}
	Un \textbf{sottospazio vettoriale} $W$ di $V$ è un sottoinsieme di $V$ che (rispetto alle operazioni $+$
	e $\cdot$ che rendono $V$ uno spazio vettoriale su $\K$) è uno spazio vettoriale su $\K$.
\end{definition}

\begin{example}
	Dato uno spazio vettoriale $V$ su un campo $\K$, $V$ e l'insieme ${O}$ sono sempre sottospazi di $V$.
\end{example}

\begin{definition}
	Chiamiamo \textbf{sottospazio proprio} di $V$ un qualsiasi sottospazio vettoriale di $V$ che sia diverso da
	$V$ e dal sottospazio ${O}$.
\end{definition}

\begin{proposition}
	Dato uno spazio vettoriale $V$ su $\K$ e $W \subseteq V$, $W$ è sottospazio vettoriale di $V$
	(rispetto alle operazioni $+$ e $\cdot$ che rendono $V$ uno spazio vettoriale su $\K$) se e solo
	se:
	\begin{enumerate}
		\item Il vettore $O$ appartiene a $W$.
		\item $\forall u, v \in W$ vale $u + v \in W$.
		\item $\forall k \in \K$ e $\forall u \in W$ vale $ku \in W$.
	\end{enumerate}
\end{proposition}

\begin{example}
	Consideriamo lo spazio vettoriale $\R^2$ su $\R$ e proviamo vedere se l'insieme
	$X = \{\forall x,y \in \R \mid x^2 + y^2 = 1\}$ è un sottospazio vettoriale di $\R^2$.
	L'insieme in questione è l'insieme di punti di una circonferenza. Subito notiamo che il vettore $(0, 0)$
	non appartiene all'insieme dunque possiamo subito concludere che $X$ non è un sottospazio di
	$\R^2$.
\end{example}

\begin{observation}
	Tutte le rette passanti per l'origine sono gli unici sottospazi vettoriali di $\R^2$. Tutti gli altri
	sottoinsiemi non sono chiusi per somma e prodotto.
\end{observation}

\begin{example}
	Consideriamo il sottoinsieme $L$ di $\K[x]$ che contiene tutti e soli i polinomi che hanno radice
	$1$, ovvero:
	\[ L = \{p(x) \in \K[x] \mid p(1) = 0\} \]
	Verifichiamo che $L$ è sottospazio vettoriale di $\K[x]$.
	\begin{itemize}
		\item Il polinomio $0$, che è il vettore $O$ di $\K[x]$, appartiene a $L$, infatti ha $1$
		      come radice (addirittura ogni elemento di $\K$ è una radice di $0$).
		\item Se $p(x), q(x) \in L$ allora $(p + q)(x)$ appartiene a $L$, infatti:
		      \[ (p + q)(1) = p(1) + q(1) = 0 + 0 = 0 \]
		\item Se $p(x) \in L$ e $k \in \K$ allora $k \cdot p(x) \in L$, infatti:
		      \[ (k \cdot p)(1) = k \cdot p(1) = k \cdot 0 = 0 \]
	\end{itemize}
\end{example}

\section{Intersezione e somma di sottospazi vettoriali}
Dati due sottospazi vettoriali $U$ e $W$ di uno spazio vettoriale $V$, la somma è il più piccolo sottospazio
vettoriale di $V$ che contenga sia $U$ che $W$ mentre l'intersezione è il più grande sottospazio vettoriale
di $V$ contenuto sia in $U$ che in $W$.

\begin{proposition}
	Sia $V$ uno spazio vettoriale su un campo $\K$, $U$ e $W$ due sottospazi di $V$, allora $U \cap W$
	è un sottospazio vettoriale di $V$.
	\begin{proof}
		Ci interessa verificare che $U \cap W$ verifichi le proprietà della definizione:
		\begin{enumerate}
			\item $O \in U \cap W$, infatti essendo $U$ e $W$ due sottospazi, certamente $O \in U$ e $O \in W$.
			\item Siano $v_1, v_2 \in U \cap W$ allora:
			      \[
				      \begin{cases}
					      v_1 + v_2 \in U \\
					      v_1 + v_2 \in W
				      \end{cases}
				      \Rightarrow{v_1 + v_2 \in U \cap W}
			      \]
			\item Sia $v \in U \cap W$ allora $\forall \lambda \in \K$ si ha:
			      \[
				      \begin{cases}
					      \lambda v \in U \\
					      \lambda v \in W
				      \end{cases}
				      \Rightarrow{\lambda v \in U \cap W}
			      \]
		\end{enumerate}
	\end{proof}
\end{proposition}

Per cercare il più piccolo sottospazio contenente sia $U$ che $W$ verrebbe da pensare all'unione insiemistica,
tuttavia, in generale non è vero che $U \cup W$ è un sottospazio vettoriale di $V$.

Dunque il più piccolo sottospazio vettoriale di $V$ che contiene sia $U$ che $W$ deve necessariamente (per
essere chiuso per la somma) contenere tutti gli elementi della forma $u + v$ dove $u \in U$ e $w \in W$.

\begin{example}
	Provare che se $V = \R^2$ e $U$ e $W$ sono due rette distinte passanti per $O$, allora $U \cup W$
	non è un sottospazio di $V$.

	Basta mostrare che, presi $u \in U$ e $w \in W$, entrambi diversi dall'origine, $v + w$ non appartiene
	all'unione $U \cup W$.
\end{example}

\begin{definition}
	Dati due sottospazi vettoriali $U$ e $W$ di uno spazio vettoriale $V$ su $\K$, chiamo \textbf{somma}
	di $U$ e $W$ l'insieme
	\[ U + W = \{u + w \mid u \in U, w \in W\} \]
\end{definition}

\begin{proposition}
	Dati due sottospazi vettoriali $U$ e $W$ di uno spazio vettoriale $V$ su $\K$, $U + W$ è un
	sottospazio vettoriale di $V$ (ed è il più piccolo contenente $U$ e $W$).
	\begin{proof}
		Partiamo col dire che $O \in U + W$, infatti $O$ appartiene sia ad $U$ che a $W$. Ora, dati
		$a \in \K$ e $x, y \in U + W$, per definizione di $U + W$ esistono $u_1, u_2 \in U$ e
		$w_1, w_2 \in W$ tali che
		\begin{align*}
			x = & u_1 + w_1 \\ y = & u_2 + w_2
		\end{align*}
		Dunque
		\[ x + y = (u_1 + w_1) + (u_2 + w_2) = (u_1 + u_2) + (w_1 + w_2) \in U + W \]
		e
		\[ ax = a(u_1 + w_1) = au_1 + aw_1 \in U + W \]
	\end{proof}
\end{proposition}

\section{Base di uno spazio vettoriale}
Sia $V$ uno spazio vettoriale su $\K$. Per definizione di $V$, se $v_1, v_2, ..., v_n$ sono $n$ vettori
di $V$, allora per qualsiasi scelta di $n$ elementi $k_1, k_2, ..., k_n$ di $\K$ il vettore:
\[ v = k_1 v_1 + ... + k_n v_n = \sum_{i=1}^n k_i v_i \]
appartiene a $V$, in quanto $V$ è chiuso per somma vettoriale e prodotto per scalare.

\begin{definition}
	Dato un insieme di vettori $\{v_1, v_2, ..., v_n\}$ di $V$, spazio vettoriale sul campo $\K$, il
	vettore:
	\[ v = k_1 \cdot v_1 + ... + k_n \cdot v_n \]
	con $\{k_1, k_2, ..., k_n\}$ scalari di $\K$, si dice una \textbf{combinazione lineare} dei vettori
	$\{v_1, v_2, ..., v_n\}$. I $k_i$ sono detti \textbf{coefficienti} della combinazione lineare.
\end{definition}

\begin{example}
	Consideriamo lo spazio vettoriale $\R^3$ su $\R$ e i seguenti due vettori:
	\[
		v_1 = \begin{pmatrix}
			3 \\ -1 \\ 3
		\end{pmatrix}
		\quad
		v_2 = \begin{pmatrix}
			1 \\ 0 \\ 2
		\end{pmatrix}
	\]
	Allora il vettore $v_3$ seguente:
	\[
		v_3 = \begin{pmatrix}
			5 \\ -1 \\ 7
		\end{pmatrix}
		= 1 \cdot \begin{pmatrix}
			3 \\ -1 \\ 3
		\end{pmatrix}
		+ 2 \cdot \begin{pmatrix}
			1 \\ 0 \\ 2
		\end{pmatrix}
	\]
	È una combinazione lineare dell'insieme dei vettori $\{v_1, v_2\}$ di coefficienti 1 e 2.
\end{example}

\begin{definition}
	Dati $\{v_1, v_2, ..., v_t\}$ vettori di $V$ su $\K$, si definisce \textbf{span} dei vettori
	$v_1, v_2, ..., v_t$ (e si indica con $Span(v_1, v_2, \dots, v_t)$) l'insieme di tutte le possibili
	combinazioni lineari dell'insieme di vettori $\{v_1, v_2, \dots, v_t\}$.
\end{definition}

\begin{definition}
	Un insieme di vettori $\{v_1, v_2, \dots, v_t\}$ di $V$ per cui $V = Span(v_1, v_2, \dots, v_t)$, ovvero
	$\forall v \in V$, esistono degli scalari $a_1, a_2, \dots, a_t$ tali che
	\[ a_1 v_1 + a_2 v_2 + \dots + a_t v_t = v \]
	si dice un \textbf{insieme di generatori} di $V$. In tal caso si dice anche che i vettori
	$v_1, v_2, \dots, v_t$ \textbf{generano} $V$.
\end{definition}

L'esistenza di un sistema finito di generatori per uno spazio vettoriale $V$ su un campo $\K$ è un
fatto molto importante, dato che si riduce la descrizione di uno spazio vettoriale con cardinalità infinita,
ad una lista di numero finito di vettori di $V$.

Dato un sistema di generatori $\{v_1, ..., v_t\}$ di $V$ sappiamo dunque che ogni $v \in V$ si può scrivere,
con opportuni coefficienti $\{k_1, ..., k_t\}$, come:
\[ v = \sum_{i=1}^t k_i v_i \]
In generale, tale scrittura non è unica, ovvero non ci permette di identificare univocamente ogni vettore di
$v \in V$.

\begin{example}
	Si verifica che i vettori
	\[
		\begin{pmatrix}
			1 \\ 2 \\ 3
		\end{pmatrix} \quad
		\begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} \quad
		\begin{pmatrix}
			0 \\ 0 \\ 1
		\end{pmatrix} \quad
		\begin{pmatrix}
			2 \\ 2 \\ 4
		\end{pmatrix}
	\]
	generano $\R^3$. Si possono facilmente trovare due distinte combinazioni lineari di tali vettori
	che esprimono il vettore
	\[ \begin{pmatrix} 2 \\ 2 \\ 5 \end{pmatrix} \]
	Per esempio:
	\[
		\begin{pmatrix}
			1 \\ 2 \\ 3
		\end{pmatrix} +
		\begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} +
		\begin{pmatrix}
			0 \\ 0 \\ 1
		\end{pmatrix} =
		\begin{pmatrix}
			2 \\ 2 \\ 5
		\end{pmatrix} =
		\begin{pmatrix}
			2 \\ 2 \\ 4
		\end{pmatrix} +
		\begin{pmatrix}
			0 \\ 0 \\ 1
		\end{pmatrix}
	\]
\end{example}

\begin{definition}
	Si dice che un insieme finito di vettori $\{v_1, v_2, ..., v_r\}$ è un \textbf{insieme di vettori
		linearmente indipendenti} se l'unico modo di scrivere il vettore $O$ come combinazione lineare di questi
	vettori è con tutti i coefficienti nulli, ossia se
	\[ a_1 v_1 + a_2 v_2 + ... + a_r v_r = O \quad \Leftrightarrow \quad a_1 = a_2 = ... = a_r = 0 \]
	Si può dire anche che i vettori sono \textbf{linearmente indipendenti}. Se invece i vettori
	$v_1, v_2, ..., v_r$ non sono linearmente indipendenti si dice che sono \textbf{linearmente dipendenti}.
\end{definition}

\begin{proposition}
	Un insieme $A = \{v_1, ..., v_n\}$ di vettori di uno spazio vettoriale $V$ su $\K$ è un insieme
	di vettori linearmente indipendenti se e solo se nessun $v_i$, appartenente ad $A$, si può scrivere come
	combinazione lineare dell'insieme $B = A \backslash \{v_i\}$ (ovvero $v_i$ non appartiene a $Span(B)$).
\end{proposition}

\begin{definition}
	Sia $V$ uno spazio vettoriale su $\K$, un insieme di vettori $\{v_1, v_2, ..., v_n\} \in V$, che
	generano lo spazio $V$ e che sono linearmente indipendenti, si dice una \textbf{base} (finita) di $V$.
\end{definition}

\begin{observation}
	Nella definizione è specificato \emph{finita}. Non sempre uno spazio vettoriale ammette un numero finito di
	generatori, e di conseguenza nemmeno una base finita.
\end{observation}

Fissata la defizione di base siamo interessati a capire:
\begin{enumerate}
	\item Se la scelta di una base garantisce l'unicità di scrittura di un vettore in termini di combinazione
	      lineare degli elementi della base.
	\item Quando uno spazio vettoriale ammette una base finita, ed in particolare se il fatto che uno spazio
	      vettoriale $V$ abbia un insieme finito di generatori, garantisca che $V$ abbia una base finita o meno.
\end{enumerate}

\begin{proposition}
	Ogni vettore $v \in V$ si scrive \emph{in modo unico} come combinazione lineare degli elementi della base.
\end{proposition}

\begin{theorem}
	Sia $V$ uno spazio vettoriale su $\K$ diverso da $\{O\}$ e generato dall'insieme finito di vettori
	\emph{non nulli} $\{w_1, w_2, ..., w_s\}$. Allora è possibile estrarre da $\{w_1, w_2, ..., w_s\}$ un
	sottoinsieme $\{w_{i_1}, w_{i_2}, ..., w_{i_n}\}$ (con $n \leq s$) che è una base di $V$.
\end{theorem}

\begin{definition}
	Sia $V$  uno spazio vettoriale con basi di cardinalità $n$. Tale cardinalità $n$ è detta \textbf{dimensione}
	di $V$.
\end{definition}

\section{Applicazioni Lineari}
Le applicazioni lineari non sono altro che funzioni che mandano sottospazi in sottospazi.

\begin{example}
	Consideriamo la funzione $\textit{f} : \R^2 \to \R^2$ definita da
	\[
		f \begin{pmatrix}
			x \\ y
		\end{pmatrix}
		=
		\begin{pmatrix}
			x \\ x^2
		\end{pmatrix}
	\]
	La funzione $f$ manda il punto $(x, x)$, con la prima e seconda coordinata uguali, ovvero i punti della
	retta di equazione $x = y$, nella parabola di equazione $y = x^2$. Ma, come sappiamo, la retta $y = x$,
	passando dall'origine, è un sottospazio di $\R^2$, mentre la parabola non lo è. Si devono dunque
	considerare applicazioni con proprietà particolari.
\end{example}

\begin{definition}
	Siano $V$ e $W$ spazi vettoriali di dimensione finita sul campo $\K$. Un'applicazione $L$ da $V$
	in $W$ è detta \textbf{lineare} se soddisfa le seguenti proprietà:
	\begin{itemize}
		\item $\forall v_1, v_2 \in V$ vale
		      \[ L(v_1 + v_2) = L(v_1) + L(v_2) \]
		\item $\forall \lambda \in \K$ e $\forall v \in V$ vale
		      \[ L(\lambda v) = \lambda L(v) \]
	\end{itemize}
\end{definition}

\begin{observation}
	Soddisfare le due proprietà, da parte di un'applicazione lineare $L$, è equivalente a soddisfare la seguente
	proprietà: $\forall v_1, v_2 \in V$ e $\forall \lambda, \mu \in \K$ vale
	\[ L(\lambda v_1 + \mu v_2) = \lambda L(v_1) + \mu L(v_2) \]
\end{observation}

\begin{definition}
	Siano $V$ e $W$ spazi vettoriali su $\K$ e $L$ un'applicazione lineare da $V$ in $W$. Chiamo
	\textbf{immagine} di $L$, e la indico con $\Imm(L)$, il seguente sottoinsieme di $W$:
	\[ \Imm(L) = \{w \in W \mid \forall v \in V, \quad L(v) = w\} \]
\end{definition}

\begin{proposition}
	È dimostrabile che \[ \Imm(L) = Span(L(e_1), \dots, L(e_n)) \] dove $\{e_1, \dots, e_n\}$ è una base di $V$.
\end{proposition}

\begin{definition}
	Siano $V$ e $W$ spazi vettoriali su $\K$ e $L$ un'applicazione lineare da $V$ in $W$. Chiamo
	\textbf{nucleo} di $L$, e lo indico con $\Ker(L)$, il seguente sottoinsieme di $V$:
	\[ \Ker(L) = \{v \in V \mid L(v) = O\} \]
\end{definition}

Di seguito qualche proprietà utile:
\begin{enumerate}
	\item $\Ker(L)$ è un sottospazio vettoriale di $V$.
	\item $\Imm(L)$ è un sottospazio vettoriale di $W$.
	\item $L$ è iniettiva se e solo se $\Ker(L) = \{O\}$.
\end{enumerate}

\section{Matrici e vettori}

\begin{definition}
	Dati due interi positivi $m, n$, una \textbf{matrice} $m \times n$ a coefficienti in $\K$ è una
	griglia composta da $m$ righe e $n$ colonne in cui in ogni posizione c'è un elemento di $\K$:
	\[
		A = \begin{pmatrix}
			a_{11} & a_{12} & \dots & a_{1n} \\
			a_{21} & a_{22} & \dots & \dots  \\
			\dots  & \dots  & \dots & \dots  \\
			a_{m1} & \dots  & \dots & a_{mn}
		\end{pmatrix}
	\]
	Per indicare l'elemento che si trova nella riga $i$-esima dall'alto e nella colonna $j$-esima da sinistra
	viene indicato con $a_{ij}$. Spesso per indicare la matrice $A$ useremo la notazione $A = (a_{ij})$ e per
	ricordare le dimensioni della matrice scriveremo:
	\[
		A = (a_{ij})_{\substack{
					i = 1, 2, \dots, m \\
					j = 1, 2, \dots, n}}
	\]
\end{definition}

\begin{definition}
	Dati due interi positivi m, n, chiamiamo $\Mat{m \times n} (\K)$, l'insieme di tutte le matrici
	$m \times n$ a coefficienti in $\K$.
\end{definition}

\begin{definition}
	Sull'insieme $\Mat{m \times n} (\K)$ possiamo definire la \textbf{somma} e il
	\textbf{prodotto per scalare}. Date due matrici $A, B \in \Mat{m \times n} (\K)$ e dato uno scalare
	$k \in \K$, definiamo:
	\begin{itemize}
		\item La \textbf{matrice somma} $A + B = C$, il cui generico coefficiente nella $i$-esima riga e
		      $j$-esima colonna si ottiene sommando i coefficienti nella stessa posizione indicata da $(i, j)$
		      di $A$ e di $B$. Ovvero per ogni $i \leq m$ e per ogni $j \leq n$ ho che $c_{ij} = a_{ij} + b_{ij}$.
		\item La \textbf{matrice prodotto per scalare} $k \cdot A = D$, il cui generico coefficiente nella
		      $i$-esima riga e $j$-esima colonna si ottiene moltiplicando lo scalare $k$ per il coefficiente
		      di $A$ in posizione $(i, j)$. Ovvero per ogni $i \leq m$ e per ogni $j \leq n$ ho che
		      $d_{ij} = k \cdot a_{ij}$.
	\end{itemize}
\end{definition}

Esiste un'altra operazione, si tratta del \emph{prodotto righe per colonne}. Per definire tale prodotto è
importante l'ordine in cui si considerano le due matrici (quindi non vale la proprità commutativa). Inoltre
tale operazione è definita solo quando il numero di colonne di $A$ è uguale al numero di righe di $B$.

\begin{definition}
	Data una matrice $A = (a_{ij}) \in \Mat{m \times n} (\K)$ e una matrice
	$B = (b_{st}) \in \Mat{n \times k} (\K)$, il \textbf{prodotto riga per colonna} $AB$, è la matrice
	$C = (c_{rh}) \in \Mat{m \times k} (\K)$, i cui coefficienti, per ogni $r, h$, sono definiti come
	segue:
	\[ c_{rh} = a_{r1} b_{1h} + a_{r2} b_{2h} + \cdots + a_{rn} b_{nh} \]
\end{definition}

\begin{example}
	Consideriamo la matrice $A \in \Mat{2 \times 3}(\K)$:
	\[
		A = \begin{pmatrix}
			1 & 2 & 4 \\
			0 & 6 & 3
		\end{pmatrix}
	\]
	e la matrice $B \in \Mat{3 \times 3}(\K)$:
	\[
		B = \begin{pmatrix}
			2 & 2 & 2  \\
			5 & 6 & -8 \\
			0 & 1 & 0
		\end{pmatrix}
	\]
	La definizione ci dice che possiamo definire $C = AB$ e che $C$ è la matrice di $\Mat_{2 \times 3}(\K)$
	i cui coefficienti sono ottenuti come segue:
	\begin{align*}
		c_{11} = & 1 \cdot 2 + 2 \cdot 5 + 4 \cdot 0 =    & 12  \\
		c_{12} = & 1 \cdot 2 + 2 \cdot 6 + 4 \cdot 1 =    & 18  \\
		c_{13} = & 1 \cdot 2 + 2 \cdot (-8) + 4 \cdot 0 = & -14 \\
		c_{21} = & 0 \cdot 2 + 6 \cdot 5 + 3 \cdot 0 =    & 30  \\
		c_{22} = & 0 \cdot 2 + 6 \cdot 6 + 3 \cdot 1 =    & 39  \\
		c_{23} = & 0 \cdot 2 + 6 \cdot (-8) + 3 \cdot 0 = & -48
	\end{align*}
	E dunque si ha:
	\[
		AB = C = \begin{pmatrix}
			12 & 18 & -14 \\
			30 & 39 & -48
		\end{pmatrix}
	\]
\end{example}

\begin{definition}
	Data un'applicazione lineare $L$ da uno spazio vettoriale $V$ di dimensione $n$ ad uno spazio vettoriale
	$W$ di dimensione $m$, si dice \textbf{matrice associata} all'applicazione lineare $L$ nelle basi
	$\{e_1, e_2, \dots, e_n\}$ di $V$ e $\{\epsilon_1, \epsilon_2, \dots, \epsilon_m\}$ di $W$, la seguente
	matrice di $m$ righe ed $n$ colonne:
	\[
		[L]_{\substack{
					e_1, e_2, \dots, e_n\\
					\epsilon_1, \epsilon_2, \dots, \epsilon_m
				}} = \begin{pmatrix}
			a_{11} & a_{12} & \dots & a_{1n} \\
			a_{21} & a_{22} & \dots & \dots  \\
			\dots  & \dots  & \dots & \dots  \\
			a_{m1} & \dots  & \dots & a_{mn}
		\end{pmatrix}
	\]
	dove $\{e_1, e_2, \dots, e_n\}$ è la base di partenza e $\{\epsilon_1, \epsilon_2, \dots, \epsilon_m\}$ è
	la base di arrivo.
\end{definition}

Sarà tutto più chiaro con un esempio che vedremo tra poco. In ogni caso, per alleggerire la notazione, si
possono omettere le basi, tuttavia si deve ricordare che la matrice $[L]$ associata all'applicazione lineare
$L$, non dipende solo da $L$ stessa, ma anche dalle basi scelte per $V$ e $W$.

\begin{example}
	Consideriamo gli spazi vettoriali $\R^4$ con la sua base
	\[
		v_1 = \begin{pmatrix}
			1 \\ 1 \\ 0 \\ 0
		\end{pmatrix} \quad
		v_2 = \begin{pmatrix}
			0 \\ 1 \\ 1 \\ 0
		\end{pmatrix} \quad
		v_3 = \begin{pmatrix}
			0 \\ 0 \\ 1 \\ 1
		\end{pmatrix} \quad
		v_4 = \begin{pmatrix}
			0 \\ 0 \\ 0 \\ 1
		\end{pmatrix}
	\]
	e $\R^3$ con la sua base
	\[
		w_1 = \begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} \quad
		w_2 = \begin{pmatrix}
			1 \\ 1 \\ 1
		\end{pmatrix} \quad
		w_3 = \begin{pmatrix}
			0 \\ 0 \\ 2
		\end{pmatrix}
	\]
	Quel che vogliamo fare è scrivere la matrice associata all'applicazione lineare
	\[
		L \begin{pmatrix}
			x \\ y \\ z \\ w
		\end{pmatrix} = \begin{pmatrix}
			x + y + z \\
			y - z     \\
			x + w
		\end{pmatrix}
	\]
	Procediamo calcolando l'immagine di ognuna delle componenti della base di $\R^4$
	\begin{align*}
		L \begin{pmatrix}
			  1 \\ 1 \\ 0 \\ 0
		  \end{pmatrix} =
		\begin{pmatrix}
			2 \\ 1 \\ 1
		\end{pmatrix} \quad
		L \begin{pmatrix}
			  0 \\ 1 \\ 1 \\ 0
		  \end{pmatrix} =
		\begin{pmatrix}
			2 \\ 0 \\ 0
		\end{pmatrix} \\
		L \begin{pmatrix}
			  0 \\ 0 \\ 1 \\ 1
		  \end{pmatrix} =
		\begin{pmatrix}
			1 \\ -1 \\ 1
		\end{pmatrix} \quad
		L \begin{pmatrix}
			  0 \\ 0 \\ 0 \\ 1
		  \end{pmatrix} =
		\begin{pmatrix}
			0 \\ 0 \\ 1
		\end{pmatrix}
	\end{align*}
	Ora dobbiamo esprimere i risultati trovati come combinazioni lineari della base di
	$\R^3$.
	\begin{gather*}
		\begin{pmatrix}
			2 \\ 1 \\ 1
		\end{pmatrix} =
		a \begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} +
		b \begin{pmatrix}
			1 \\ 1 \\ 1
		\end{pmatrix} +
		c \begin{pmatrix}
			0 \\ 0 \\ 2
		\end{pmatrix}\\
		\\
		\begin{pmatrix}
			2 \\ 0 \\ 0
		\end{pmatrix} =
		a \begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} +
		b \begin{pmatrix}
			1 \\ 1 \\ 1
		\end{pmatrix} +
		c \begin{pmatrix}
			0 \\ 0 \\ 2
		\end{pmatrix}\\
		\\
		\begin{pmatrix}
			1 \\ -1 \\ 1
		\end{pmatrix} =
		a \begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} +
		b \begin{pmatrix}
			1 \\ 1 \\ 1
		\end{pmatrix} +
		c \begin{pmatrix}
			0 \\ 0 \\ 2
		\end{pmatrix}\\
		\\
		\begin{pmatrix}
			0 \\ 0 \\ 1
		\end{pmatrix} =
		a \begin{pmatrix}
			1 \\ 0 \\ 1
		\end{pmatrix} +
		b \begin{pmatrix}
			1 \\ 1 \\ 1
		\end{pmatrix} +
		c \begin{pmatrix}
			0 \\ 0 \\ 2
		\end{pmatrix}
	\end{gather*}
	Ottengo dunque quattro sistemi.
	\begin{gather*}
		\begin{cases}
			a + b      & = 2 \\
			b          & = 1 \\
			a + b + 2c & = 1
		\end{cases}
		\quad
		\begin{cases}
			a + b      & = 2 \\
			b          & = 0 \\
			a + b + 2c & = 0
		\end{cases} \\
		\\
		\begin{cases}
			a + b      & = 1  \\
			b          & = -1 \\
			a + b + 2c & = 1
		\end{cases}
		\quad
		\begin{cases}
			a + b      & = 0 \\
			b          & = 0 \\
			a + b + 2c & = 2
		\end{cases}
	\end{gather*}
	Se li risolvo ottengo
	\begin{gather*}
		\begin{cases}
			a & = 1  \\
			b & = 1  \\
			c & = -1
		\end{cases}
		\quad
		\begin{cases}
			a & = 2  \\
			b & = 0  \\
			c & = -1
		\end{cases} \\
		\\
		\begin{cases}
			a & = 2  \\
			b & = -1 \\
			c & = 0
		\end{cases}
		\quad
		\begin{cases}
			a & = 0 \\
			b & = 0 \\
			c & = 1
		\end{cases}
	\end{gather*}
	Ora non devo fare altro che prendere i vettori
	\[
		\begin{pmatrix}
			1 \\ 1 \\ -1
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			2 \\ 0 \\ -1
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			2 \\ -1 \\ 0
		\end{pmatrix}
		\quad
		\begin{pmatrix}
			0 \\ 0 \\ -1
		\end{pmatrix}
	\]
	e formare la matrice associata all'applicazione lineare $L$.
	\[
		[L]_{\substack{v_1, v_2, v_3, v_4 \\
					w_1, w_2, w_3}} =
		\begin{pmatrix}
			1  & 2  & 2  & 0  \\
			1  & 0  & -1 & 0  \\
			-1 & -1 & 0  & -1
		\end{pmatrix}
	\]
\end{example}

\begin{observation}
	Dati due spazi vettoriali $V$ e $W$, esiste una sola applicazione lineare da $V$ a $W$ la cui matrice
	associata è indipendente dalle basi scelte. Questa è l'\emph{applicazione nulla}
	$\mathcal{O} : V \rightarrow W$ che manda ogni $v \in V$ in $O \in W$. Qualunque siano le basi scelte, la
	matrice associata avrà tutti i coefficienti uguali a $0$.
\end{observation}

\begin{observation}
	Consideriamo l'applicazione \emph{identità} $I : V \rightarrow V$, che lascia fisso ogni elemento di
	$v: I(v) = v$, $\forall v \in V$, e fissiamo la base $\B$ di $V$. Si verifica che la matrice
	$[I] = (a_{ij})$, associata ad $I$ rispetto a $\B$, sia in arrivo che in partenza, è la matrice
	quadrata di formato $n \times n$ che ha tutti i coefficienti uguali a $0$ eccetto quelli sulla diagonale,
	che sono invece uguali a $1$.  Tale matrice è l'elemento neutro rispetto alla moltiplicazione riga per
	colonna in $\Mat_{n \times n}(\K)$.

	In seguito useremo solo il simbolo $I$ per indicare sia la matrice identità che	l'applicazione lineare
	$I$.
\end{observation}