\chapter{Prodotti scalari e spazi euclidei}
\section{Prodotto scalare}

\begin{defn}
	Sia $V$ uno spazio vettoriale sul campo $\mathbb{K}$, dove
	$\mathbb{K} = \mathbb{R}^n$ o $\mathbb{K} = \mathbb{C}^n$. Il
	\textbf{prodotto scalare} \`e una funzione che ad ogni coppia di vettori $u, v$
	appartenenti a $V$ associa lo scalare $\langle u, v \rangle \in \mathbb{K}$
	calcolato come segue:
	\[
		\langle u, v \rangle = u_1 v_1 + u_2 v_2 + ... + u_n v_n
	\]
	dove gli $u_i$ e $v_i$ sono le $i$-esime coordinate di $u$ e $v$.

	Il prodotto scalare gode inoltre delle seguenti propriet\`a:
	\begin{enumerate}
		\item $\langle au_1 + bu_2, v \rangle =
			      a \langle u_1, v \rangle + b \langle u_2, v \rangle$, per ogni
		      $u_1, u_2, v \in V$ e per ogni $a, b \in \mathbb{K}$.
		\item $\langle u, v \rangle = \overline{\langle v, u \rangle}$ per ogni
		      $u, v \in V$ (dove $\overline{\langle v, u \rangle}$ indica il numero
		      complesso coniugato di $\langle u, v \rangle$).
		\item per ogni $u \in V$ vale $\langle u, u \rangle \geq 0$ e
		      $\langle u, u \rangle = 0$ se e solo se $u = O$.
	\end{enumerate}
	Uno spazio vettoriale reale $V$ munito di un prodotto scalare si dice
	\textbf{spazio euclideo}.
\end{defn}

\begin{observation}
	Osserviamo che, per la seconda propriet\`a, il prodotto scalare
	$\langle u, u \rangle$ \`e sempre un numero reale, dunque ha senso la disuguaglianza
	che compare nella terza propriet\`a.
\end{observation}

\begin{defn}
	Per ogni vettore $u \in V$, scriveremo
	\begin{equation*}
		\| u \| = \sqrt{\langle u, u \rangle}
	\end{equation*}
	e diremo che $\| u \|$ \`e la \textbf{norma} di $u$.
\end{defn}

\begin{observation}
	Dalle propriet\`a (1) e (2) segue che
	\begin{equation*}
		\langle u, av_1 + bv_2 \rangle =
		\overline{a} \langle u, v_1 \rangle + \overline{b} \langle u, v_2 \rangle
	\end{equation*}
	Se il campo $\mathbb{K}$ \`e $\mathbb{R}$ tutti i simboli di coniugazione
	complessa possono essere ignorati.
\end{observation}

\begin{observation}
	Il prodotto scalare standard estende a tutti gli spazi $\mathbb{R}^n$ un concetto
	che in $\mathbb{R}^2$ e in $\mathbb{R}^3$ ci \`e gi\`a familiare. Si pu\`o
	facilmente verificare che in questi casi per esempio $\| u \|$ coincide con
	ci\`o che \`e chiamata lunghezza del vettore $u$ e che $\| u - v \|$ coincide
	con la distanza tra i vettori $u$ e $v$. Inoltre $\langle u, v \rangle = 0$ se
	e solo se $u$ e $v$ sono ortogonali fra loro. Potete anche gi\`a osservare in
	$\mathbb{R}^2$ che vale la seguente relazione:
	\begin{equation*}
		\langle u, v \rangle = \| u \| \| v \| \cos \theta
	\end{equation*}
	dove $\theta$ \`e l'angolo compreso fra i vettori $u$ e $v$.
\end{observation}

\begin{example}
	In $\mathbb{C}^n$, dati i vettori
	\begin{equation*}
		u = \begin{pmatrix}
			a_1 \\ \dots \\ a_n
		\end{pmatrix}, \quad
		v = \begin{pmatrix}
			b_1 \\ \dots \\ b_n
		\end{pmatrix}
	\end{equation*}
	scritti rispetto alla base standard, abbiamo il prodotto scalare
	\begin{equation*}
		\langle u, v \rangle = a_1 \overline{b_1} + \cdots + a_n \overline{b_n}
	\end{equation*}
\end{example}

\begin{example}
	In $\mathbb{R}^3$, dati i vettori
	\[
		u = \begin{pmatrix}
			2 \\ 1 \\ 1
		\end{pmatrix}, \quad
		v = \begin{pmatrix}
			-1 \\ 0 \\ 3
		\end{pmatrix}
	\]
	Abbiamo che il prodotto scalare $\langle u, v \rangle$ \`e dato da
	\[
		\langle u, v \rangle = 2 \cdot (-1) + 1 \cdot 0 + 1 \cdot 3 = 1
	\]
\end{example}

\section{Ortogonalit\`a}
Sia $V$ uno spazio vettoriale munito di un prodotto scalare. Estendiamo il
concetto di perpendicolarit\`a che ci \`e noto in $\mathbb{R}^2$ e
$\mathbb{R}^3$ dando la seguente definizione:

\begin{defn}
	Due vettori $u, v \in V$ sono detti \textbf{ortogonali} se
	\begin{equation*}
		\langle u, v \rangle = 0
	\end{equation*}
\end{defn}

Notiamo che se due vettori verificano $\langle u, v \rangle = 0$ vale anche che
$\langle v, u \rangle = \overline{\langle u, v \rangle} = 0$ dunque il concetto
di ortogonalit\`a non dipende dall'ordine con cui stiamo considerando i due
vettori. Inoltre dalla definizione si ricava subito che il vettore $O$ \`e
ortogonale a tutti i vettori di $V$:
\begin{equation*}
	\langle O, v \rangle = \langle 0O, v \rangle = 0\langle O, v \rangle = 0
\end{equation*}

\begin{defn}
	Sia $U$ un sottospazio di $V$. L'insieme dato dai vettori di $V$ che sono
	ortogonali ad ogni vettore di $U$ si indica con $U^{\perp}$:
	\begin{equation*}
		U^{\perp} = \{v \in V | \langle v, u \rangle = 0 \forall u \in U\}
	\end{equation*}
	Si verifica facilmente che $U^{\perp}$ \`e un sottospazio di $V$, chiamato
	il \textbf{sottospazio ortogonale} a $U$.
\end{defn}

\begin{defn}
	Un insieme $\{u_1, \dots, u_r\}$ di vettori si dice \textbf{ortogonale} se
	i suoi elementi sono a due a due ortogonali fra loro. L'insieme
	$\{u_1, \dots, u_r\}$ si dice \textbf{ortonormale} se \`e ortogonale e se
	per ogni $i$ vale $\| u_i \| = 1$.
\end{defn}

\begin{example}
	Nello spazio euclideo $\mathbb{R}^n$ con il prodotto scalare standard,
	i vettori della base standard costituiscono un insieme ortonormale.
\end{example}

\begin{theorem}\label{orto}
	Sia $V$ uno spazio vettoriale munito di un prodotto scalare. Un insieme
	ortonormale di vettori $\{u_1, \dots, u_r\}$ \`e linearmente indipendente
	e, per ogni $v \in V$, il vettore
	\begin{equation*}
		w = v - \langle v, u_1 \rangle u_1 - ... -\langle v, u_r \rangle u_r
	\end{equation*}
	\`e ortogonale ad ognuno degli $u_i$, dunque appartiene a
	$Span(u_1, \dots, u_r)^{\perp}$.
	\begin{proof}
		Supponiamo di avere una combinazione lineare dei vari $u_i$ uguale a $O$:
		\[
			a_1 u_1 + ... + a_r u_r = O
		\]
		Per dimostrare la lineare indipendenza dei vettori $u_1, ..., u_r$ dobbiamo
		dimostrare che per ogni $i$ vale che $a_i = 0$.

		Fissato un $i$, consideriamo il prodotto scalare di entrambi i membri
		dell'uguaglianza con $u_i$:
		\[
			\langle a_1 u_1 + \cdots + a_r u_r, u_i \rangle = \langle O, u_i \rangle
		\]
		ovvero
		\[
			a_1 \langle u_1, u_i \rangle + \dots + a_r \langle u_r, u_i \rangle = 0
		\]
		Vista la ortonormalit\`a dell'insieme $\{ u_1, \dots, u_r \}$ questo implica
		$a_i = 0$.

		Ripetendo questa considerazione per ogni $i = 1, \dots, r$ si dimostra che i vettori
		$u_1, \dots, u_r$ sono linearmente indipendenti.

		Per concludere possiamo verificare che $w$ sia ortogonale a $u_i$ per ogni $i$:
		\begin{gather*}
			\langle w, u_i \rangle = \langle v, u_i \rangle - \langle v, u_1 \rangle
			\langle u_1, u_i \rangle - ... - \langle v, u_r \rangle \langle u_r, u_i \rangle =\\
			= \langle v, u_i \rangle - \langle v, u_1 \rangle 0 - \cdots - \langle v, u_i \rangle 1
			- \cdots - \langle v, u_r \rangle 0 = 0
		\end{gather*}
	\end{proof}
\end{theorem}

Il teorema appena dimostrato sar\`a fondamentale per il
\emph{procedimento di ortogonalizzazione di Gram-Schmidt}, che permette di ottenere una base
ortonormale "modificando" una base data.

\begin{theorem}[Gram-Schmidt]
	Sia $V$ uno spazio vettoriale munito di un prodotto scalare, e sia
	$v_1, \dots, v_n$ una qualunque base di $V$. Allora esiste una base
	$u_1, \dots, u_n$ di $V$ che \`e ortonormale e tale che, per ogni
	$i = 1, \dots, n$, $u_i \in Span(v_1, \dots, v_n)$.
	\begin{proof}
		Per prima cosa poniamo $u_1 = \frac{v_1}{\| v_1 \|}$. L'insieme ${u_1}$ \`e ortonormale.
		Come secondo passo consideriamo il vettore
		\[
			v_2 - \langle v_2, u_1 \rangle u_1
		\]
		questo vettore \`e diverso da $O$ per lineare indipendenza e per il teorema \ref{orto},
		applicato riferendosi all'insieme ortonormale $\{ u_1 \}$, \`e ortogonale a $u_1$. Inoltre
		appartiene al sottospazio $Span(v_1, v_2)$.

		\`E dunque un buon candidato per essere il nostro $u_2$; l'unico problema \`e che non ha
		norma 1. Per rimediare basta moltiplicarlo per un opportuno scalare. Poniamo dunque
		\[
			u_2 = \frac{v_2 - \langle v_2, u_1 \rangle u_1}
			{\| v_2 - \langle v_2, u_1 \rangle u_1\|}
		\]
		Per le osservazioni precedenti, l'insieme $\{ u_1, u_2 \}$ \`e ortonormale.

		Volendo possiamo proseguire per induzione e compiere $n$ passi e definire $u_n$ come segue:
		\[
			u_n = \frac{v_n - \langle v_n, u_1 \rangle u_1 - \dots -
				\langle v_n, u_{n-1} \rangle u_{n-1}}
			{\| v_n - \langle v_n, u_1 \rangle u_1 - \dots - \langle v_n, u_{n-1} \rangle u_{n-1} \|}
		\]
	\end{proof}
\end{theorem}

Il teorema appena dimostrato afferma che ogni spazio $V$ munito di un prodotto
scalare ha una base ortonormale.

\begin{example}
	Applicare il procedimento di ortogonalizzazione alla seguente base di $\mathbb{R}^2$:
	\[
		v_1 = \begin{pmatrix} 2 \\ 2 \end{pmatrix}, \quad
		v_2 = \begin{pmatrix} 3 \\ -1 \end{pmatrix}
	\]
	Per trovare $u_1$ mi basta fare
	\[
		u_1 = \frac{v_1}{\| v_1 \|}
	\]
	Per prima cosa calcolo $\| v_1 \|$
	\[ \| v_1 \| = \sqrt{\langle v_1, v_1 \rangle} = \sqrt{8} = 2 \sqrt{2} \]
	Ottengo dunque che
	\[
		u_1 = \begin{pmatrix}
			2 \\ 2
		\end{pmatrix} \frac{1}{2 \sqrt{2}} =
		\begin{pmatrix}
			\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
		\end{pmatrix}
	\]
	Ci rimane da calcolare $u_2$
	\[
		u_2 = \frac{v_2 - \langle v_2, u_1 \rangle u_1}
		{\| v_2 - \langle v_2, u_1 \rangle u_1 \|}
	\]
	Calcoliamo $\langle v_2, u_1 \rangle$
	\[
		\langle v_2, u_1 \rangle = \frac{2}{\sqrt{2}}
	\]
	Possiamo proseguire calcolando
	\[
		\langle v_2, u_1 \rangle u_1 =
		\frac{2}{\sqrt{2}}
		\begin{pmatrix}
			\frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}}
		\end{pmatrix} =
		\begin{pmatrix}
			1 \\ 1
		\end{pmatrix}
	\]
	Calcoliamo infine
	\[
		v_2 - \langle v_2, u_1 \rangle u_1 =
		\begin{pmatrix} 3 \\ -1 \end{pmatrix} - \begin{pmatrix} 1 \\ 1 \end{pmatrix} =
		\begin{pmatrix} 2 \\ -2 \end{pmatrix}
	\]
	Calcoliamo la norma di tale vettore
	\[
		\| v_2 - \langle v_2, u_1 \rangle u_1 \| = \sqrt{ 2^2 + (-2)^2 } = \sqrt{8} = 2 \sqrt{2}
	\]
	Quindi, in conclusione
	\[
		u_2 = \begin{pmatrix} 2 \\ -2 \end{pmatrix} \frac{1}{2 \sqrt{2}} =
		\begin{pmatrix} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \end{pmatrix}
	\]
	La base ortonormale cercata \`e dunque
	\[
		\begin{pmatrix} \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} \end{pmatrix}, \quad
		\begin{pmatrix} \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \end{pmatrix}
	\]
\end{example}

\begin{example}
	Trovare una base ortonormale di $\mathbb{R}^3$ il cui primo vettore sia
	\[ \left( \frac{1}{2}, \quad \frac{2}{3}, \quad \frac{2}{3} \right) \]
\end{example}

\section{La disuguaglianza di Cauchy-Schwarz}
Dato un prodotto scalare $\langle , \rangle$ su $V$, abbiamo il seguente
teorema:

\begin{theorem}[Disuguaglianza di Cauchy-Schwarz]
	Per ogni coppia di vettori $u, v \in V$ vale
	\begin{equation*}
		|\langle u, v \rangle| \leq \| u \| \| v \|
	\end{equation*}
\end{theorem}

\begin{example}
	Nel caso in cui $V$ sia $\mathbb{R}^n$ col prodotto scalare standard, la
	disuguaglianza di Cauchy-Schwarz si traduce cos\`i. Dati
	\begin{equation*}
		u = \begin{pmatrix}
			a_1 \\ \dots \\ a_n
		\end{pmatrix},
		v = \begin{pmatrix}
			b_1, \dots, b_n
		\end{pmatrix}
	\end{equation*}
	vale
	\begin{equation*}
		|a_1 b_1 + \cdots + a_n b_n| \leq
		(a_1^2 + \cdots + a_n^2)(b_1^2 + \cdots + b_n^2)
	\end{equation*}
	che pu\`o essere pensata anche, svincolandosi dai vettori e dagli spazi
	vettoriali, come una disuguaglianza che riguarda due qualsiasi $n$-uple
	$a_1, \dots, a_n$ e $b_1, \dots, b_n$ di numeri reali.
\end{example}

\begin{example}
	Nel caso in cui $V$ sia lo spazio delle funzioni continue reali definite
	sull'intervallo $[0, 1]$ la disuguaglianza di Cauchy-Schwarz si traduce
	nella seguente importante disuguaglianza fra integrali:
	\begin{equation*}
		\left( \int_0^1 f(t)g(t)dt \right)^2 \leq
		\int_0^1 f(t)^2 dt \int_0^1 g(t)^2 dt
	\end{equation*}
\end{example}

\section{Sottospazi ortogonali}
Sia $V$, uno spazio vettoriale munito di prodotto scalare.

\begin{proposition}
	Sia $U$ un sottospazio vettoriale di $V$. Allora esiste una base
	ortonormale di $U$ che \`e un sottoinsieme di una base ortonormale di $V$.
\end{proposition}

\begin{theorem}
	Sia $U$ un sottospazio vettoriale di $V$. Allora $V$ si decompone come
	somma diretta di $U$ e di $U^\perp$:
	\begin{equation*}
		V = U \oplus U^\perp
	\end{equation*}
	In particolare vale che $dim(U^\perp) = dim(V) - dim(U)$.
\end{theorem}

\begin{observation}
	Consideriamo $V = \mathbb{R}^n$ munito del prodotto scalare standard. Dato
	un sottospazio $W$, con base $w_1, \dots, w_r$, se pensiamo a come si
	scrive il prodotto scalare ci rendiamo conto che i vettori $W^\perp$ sono
	esattamente le soluzioni del sistema
	\begin{equation*}
		\begin{pmatrix}
			a_1   & \dots & a_n   \\
			b_1   & \dots & b_n   \\
			\dots & \dots & \dots \\
			\dots & \dots & \dots
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\ x_2 \\ \dots \\ \dots \\ x_n
		\end{pmatrix} =
		\begin{pmatrix}
			0 \\ 0 \\ \dots \\ \dots \\ 0
		\end{pmatrix}
	\end{equation*}
\end{observation}