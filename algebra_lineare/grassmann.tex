\chapter{La formula di Grassmann}
\section{La formula di Grassmann}

Dati due sottospazi vettoriali $A$ e $B$ in $\R^3$ di dimensione 2,
di che dimensione può essere la loro intersezione ?

Possono intersecarsi lungo una retta: in tal caso si nota che il sottospazio
generato dai vettori di $A \cup B$, ossia $A + B$, è tutto $\R^3$.

Oppure vale $A = B$: allora la loro intersezione è uguale ad $A$ (e a $B$) e
ha dimensione 2, e anche il sottospazio $A + B$ coincide con $A$.

In entrambi i casi, la somma delle dimensioni di $A \cap B$ e di $A + B$ è
sempre uguale a 4.

E se in $\R^4$ consideriamo un piano $C$ e un sottospazio $D$ di
dimensione 3?
Possono darsi tre casi per l'intersezione: $C \cap D = \{O\}$,
$\dim(C \cap D) = 1$, $C \cap D = C$.

Qualunque sia il caso si verifica sempre che
\[
	\dim(C \cap D) + \dim(C + D) = 5 = \dim(C) + \dim(D)
\]

In generale vale la formula
\[
	\dim(A \cap B) + \dim(A + B) = \dim(A) + \dim(B)
\]

Dati due spazi vettoriali $V$ e $W$ sul campo $\K$, sul loro prodotto
cartesiano $V \times W$, c'è una struttura naturale di spazio vettoriale, dove
la somma è definita da:
\[
	(v, w) + (v_1, w_1) = (v + v_1, w + w_1)
\]
e il prodotto per scalare da:
\[
	\lambda(v, w) = (\lambda v, \lambda w)
\]
Si verifica che, se $\{v_1, \dots, v_n\}$ è una base di $V$
e $\{w_1, \dots, w_m\}$ è una base di $W$, allora
$\{(v_1, O), \dots (v_n, O), (O, w_1), \dots, (O, w_m)\}$ è una base di
$V \times W$, che dunque ha dimensione $n + m = (\dim(V)) + (\dim(W))$.

\begin{theorem}[Grassmann]
	Dati due sottospazi $A, B$ di uno spazio vettoriale $V$ sul campo
	$\K$, vale
	\[
		\dim(A) + \dim(B) = \dim(A \cap B) + \dim(A + B)
	\]
	\begin{proof}
		Consideriamo l'applicazione
		\[
			\Phi : A \times B \to V
		\]
		definita da
		\[ \Phi((a, b)) = a - b \]
		Cosa sappiamo dire del nucleo di $\Phi$ ? Per definizione
		\[
			\Ker(\Phi) = \{(a, b) \in A \times B \mid a - b = O\}
		\]
		dunque
		\[
			\Ker(\Phi) = \{(a, b) \in A \times B \mid a = b\}
		\]
		che equivale a scrivere:
		\[
			\Ker(\Phi) = \{(z, z) \in A \times B \mid z \in A \cap B\}
		\]
		Si nota subito che la applicazione lineare
		\[ \theta : A \cap B \to \Ker(\Phi) \]
		è iniettiva e surgettiva, dunque è un isomorfismo. Allora il suo dominio e
		il suo codominio hanno la stessa dimensione, ovvero
		\[
			\dim(\Ker(\Phi)) = \dim(A \cap B)
		\]
		Cosa sappiamo dire invece dell'immagine di $\Phi$ ? Per definizione
		\[
			\Imm(\Phi) = \{a - b \mid a \in A, b \in B\}
		\]
		Visto che $B$, come ogni spazio vettoriale, se contiene un elemento
		$b$ contiene anche il suo opposto $-b$, possiamo scrivere la seguente
		uguaglianza fra insiemi:
		\[
			\{ a - b \mid a \in A, b \in B \} =
			\{ a + b \in V \mid a \in A, b \in B \} =
			A + B
		\]
		Dunque
		\[
			\Imm(\Phi) = A + B
		\]
		Sappiamo che:
		\[
			\dim(A \times B) = \dim(\Ker(\Phi)) + \dim(\Imm(\Phi))
		\]
		Questa formula, viste le osservazioni fatte fin qui, si traduce come:
		\[
			\dim(A) + \dim(B) = \dim(A \cap B) + \dim(A + B)
		\]
	\end{proof}
\end{theorem}

\section{Calcolo dell'intersezione di due sottospazi}
Consideriamo due sottospazi, $U$ e $W$, di $V$. Se entrambi sono presentati
come l'insieme delle soluzioni di un sistema è facile calcolare $U \cap W$:
basta calcolare le soluzioni del sistema 'doppio', ottenuto considerando tutte
le equazioni dei due sistemi.

Per esempio se $U$ e $W$ in $\R^4$ sono dati rispettivamente dalle
soluzioni dei sistemi $S_U$:
\[
	\begin{cases}
		3x + 2y + 4w = 0 \\
		2x + y + z + w = 0
	\end{cases}
\]
e $S_W$:
\[
	\begin{cases}
		x + 2y + z + w = 0 \\
		x + z + w = 0
	\end{cases}
\]
allora $U \cap W$ è dato dalle soluzioni del sistema:
\[
	\begin{cases}
		3x + 2y + 4w = 0   \\
		2x + y + z + w = 0 \\
		x + 2y + z + w = 0 \\
		x + z + w = 0
	\end{cases}
\]

\begin{observation}
	Visto che $U$ ha dimensione 2, un sistema le cui soluzioni coincidono con
	l'insieme $U$ deve avere almeno 3 equazioni.
\end{observation}

Come calcolare però $U \cap W$ se i due sottospazi sono presentati come span
di certi vettori ? Consideriamo per esempio $U$ e $W$ in $\R^5$
definiti così:
\begin{gather*}
	U = <\begin{pmatrix}
		1 \\ 2 \\ 3 \\ -1 \\ 2
	\end{pmatrix},
	\begin{pmatrix}
		2 \\ 4 \\ 7 \\ 2 \\ -1
	\end{pmatrix}> \\
	W = <\begin{pmatrix}
		1 \\ 2 \\ 0 \\ -2 \\ -1
	\end{pmatrix},
	\begin{pmatrix}
		0 \\ 1 \\ 1 \\ -1 \\ -1
	\end{pmatrix},
	\begin{pmatrix}
		0 \\ 1 \\ -3 \\ -6 \\ 1
	\end{pmatrix}>
\end{gather*}

Un metodo per calcolare $U \cap W$ è quello di esprimere $U$ e $W$ come
soluzioni di un sistema lineare. Cominciamo da $U$.

Per prima cosa si scrive la matrice:
\[
	\begin{pmatrix}
		1  & 2  & x_1 \\
		2  & 4  & x_2 \\
		3  & 7  & x_3 \\
		-1 & 2  & x_4 \\
		2  & -1 & x_5
	\end{pmatrix}
\]
Ora riduciamo la matrice (senza incognite) a scalini per righe
\[
	\begin{pmatrix}
		1 & 2 & x_1                 \\
		0 & 1 & x_3 - 3x_1          \\
		0 & 0 & 2x_1 - x_2          \\
		0 & 0 & 13x_1 - 4x_3 + x_4  \\
		0 & 0 & -17x_1 + 5x_3 + x_5
	\end{pmatrix}
\]
Tale matrice ha rango 2 se e solo se i coefficienti $x_1, x_2, x_3, x_4, x_5$ soddisfano
il sistema
\[
	\begin{cases}
		2x_1 - x_2          & = 0 \\
		13x_1 - 4x_3 + x_4  & = 0 \\
		-17x_1 + 5x_3 + x_5 & = 0
	\end{cases}
\]

Ora dobbiamo fare la stessa cosa con $W$. Scriviamo quindi la matrice
\[
	\begin{pmatrix}
		1  & 0  & 0  & x_1 \\
		2  & 1  & 1  & x_2 \\
		0  & 1  & -3 & x_3 \\
		-2 & -1 & -6 & x_4 \\
		-1 & -1 & 1  & x_5
	\end{pmatrix}
\]
e riduciamola a scalini per righe
\[
	\begin{pmatrix}
		1 & 0 & 0 & x_1                       \\
		0 & 1 & 1 & x_2 - 2x_1                \\
		0 & 0 & 2 & x_5 - x_1 + x_2           \\
		0 & 0 & 0 & x_3 + x_2 + 2x_5          \\
		0 & 0 & 0 & 2x_4 + 7x_2 + 5x_5 - 5x_1
	\end{pmatrix}
\]
La matrice ha rango 3 se e solo se i coefficienti $x_1, x_2, x_3, x_4, x_5$ soddisfano il
sistema
\[
	\begin{cases}
		x_3 + x_2 + 2x_5          & = 0 \\
		2x_4 + 7x_2 + 5x_5 - 5x_1 & = 0 \\
	\end{cases}
\]

Uniamo i due sistemi ottenuti e otteniamo
\[
	\begin{cases}
		2x_1 - x_2                & = 0 \\
		13x_1 - 4x_3 + x_4        & = 0 \\
		17x_1 - 5x_3 - x_5        & = 0 \\
		x_2 + x_3 + 2x_5          & = 0 \\
		5x_1 - 7x_2 - 2x_4 - 5x_5 & = 0
	\end{cases}
\]
Se risolviamo questo sistema otteniamo una base di $U \cap W$. Per verificare che i calcoli
siano corretti basta vedere se la dimensione risulta uguale a quella prevista dalla formula
di Grassmann.

\section{Somma diretta di sottospazi}
Si dice che due sottospazi $U$ e $W$ di uno spazio vettoriale $V$ sono in
\textbf{somma diretta} se vale che \[ U \cap W = \{O\} \] In questo caso,
come sappiamo dalla formula di Grassmann, la dimensione di $U + W$ è "la
massima possibile", ovvero \[ \dim(U) + \dim(W) \] Vale anche il
viceversa, ossia due sottospazi sono in somma diretta se e solo se
\[ \dim(U + W) = \dim(U) + \dim(W) \] Quando siamo sicuri che $U + W$ è la somma di due
sottospazi che sono in somma diretta, al posto di $U + W$ possiamo scrivere:
\[
	U \oplus W
\]
In particolare, per avere una base di $U \oplus W$ basta fare l'unione di una
base di $U$ con una base di $W$.

\begin{observation}
	Attenzione: un sottospazio vettoriale $U$ di $V$ che non è uguale a $V$
	possiede in generale molti complementari. Per esempio, in $\R^3$
	un piano passante per l'origine ha per complementare una qualunque retta
	passante per l'origine e che non giace sul piano.
\end{observation}

In generale dati $k$ sottospazi $U_1, \dots, U_k$ di uno spazio vettoriale $V$,
si dice che tali sottospazi sono in somma diretta se, per ogni
$i = 1, \dots, k$, vale che l'intersezione di $U_i$ con la somma di tutti
gli altri è uguale a $\{O\}$, ovvero
\[
	U_i \cap (U_1 + \cdots + \hat{U_i} + \cdots + U_k) = \{O\}
\]
dove il simbolo $\hat{U_i}$ indica che nella somma si è saltato il termine
$U_i$.

In tal caso per indicare $U_1 + \cdots + U_k$ si può usare la notazione:
\[
	U_1 \oplus \cdots \oplus U_k
\]